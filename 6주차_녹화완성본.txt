이겸
안녕하세요, 여러분. 오늘은 비지도학습 기반 이상탐지에 대해 강의하겠습니다. 
현대의 디지털 환경에서는 수많은 데이터가 생성되고 있으며, 이 중에서 정상적이지 않은 패턴, 즉 이상 징후를 자동으로 탐지하는 것이 매우 중요합니다. 
금융 사기 탐지, 제조업의 설비 이상, 네트워크 보안, IT 운영 모니터링 등 다양한 분야에서 활용되고 있습니다. 
오늘은 이상탐지의 핵심 원리부터 실무 적용까지 체계적으로 살펴보겠습니다.

오늘 강의는 세 개의 주요 섹션으로 구성되어 있습니다. 
첫 번째로, 이상 징후 자동 탐지의 핵심 원리를 다루겠습니다. 
여기서는 비지도 학습이 무엇인지, 어떤 방식으로 이상을 탐지하는지 살펴봅니다. 
두 번째로, 정상 패턴을 학습하고 이상치를 식별하는 구체적인 방법론들을 알아보겠습니다. 
마지막으로, 실제 산업 현장에서 어떻게 적용되고 있는지 사례를 통해 확인해보겠습니다. 
우리의 목표는 개념을 이해하고, 적절한 모델을 선택하며, 실제 운영 환경에 적용할 수 있는 능력을 기르는 것입니다.

첫 번째 섹션인, '이상 징후 자동 탐지 원리'를 시작하겠습니다. 이 섹션에서는 세 가지 핵심 학습 목표를 설정했습니다. 
먼저 비지도 이상탐지 문제가 무엇인지 정확히 정의하고, 다음으로 주요 탐지 패러다임들을 살펴보겠습니다. 
밀도 기반, 거리 기반, 재구성 기반, 아이솔레이션 기반, 확률 기반 접근법이 있습니다. 
마지막으로 이상 점수를 실제 의사결정으로 전환하는 임계값 설정과 시간에 따른 데이터 변화에 대응하는 드리프트 관리 방법을 다루겠습니다.

이상탐지란 무엇일까요? 간단히 말해서, 정상적인 패턴에서 벗어난 드물고 특이한 관측치를 식별하는 것입니다. 여기서 중요한 점은 '비지도' 맥락이라는 것입니다. 
즉, 미리 라벨링된 정답 데이터 없이 데이터의 구조와 분포만을 이용해 이상도를 추정합니다. 
이는 현실에서 매우 유용한데, 실제로는 모든 이상 사례를 미리 알기 어렵기 때문입니다. 
이상탐지는 금융에서의 사기 탐지, 제조업에서의 설비 고장 예측, IT 보안, 의료 진단 등 광범위한 영역에서 활용되고 있습니다.

이상치는 크게 세 가지 유형으로 분류할 수 있습니다. 첫 번째는 점 이상치입니다. 
이는 개별 데이터 포인트 하나가 전체 분포에서 크게 벗어나는 경우입니다. 
예를 들어, 평소 월 10만원 사용하던 카드에서 갑자기 500만원이 결제되는 경우입니다.
두 번째는 집단 이상치로, 개별 포인트는 정상이지만 여러 데이터가 묶여서 비정상적인 패턴을 보이는경우입니다. 특정 IP 대역에서의 트래픽 급증이 이에 해당합니다. 
마지막은 문맥적 이상치로, 맥락이나 시간대를 고려했을 때 이상한 경우입니다. 
심야 시간대의 정상 수준 활동도 문맥상 이상할 수 있습니다.

밀도 기반 접근법의 핵심 아이디어는 매우 직관적입니다. 
데이터가 빽빽하게 모여 있는 곳은 정상적이고, 드문드문 떨어져 있는 희소한 영역일수록 이상일 가능성이 높다는 것입니다.
이를 시각화해보면 커널 밀도나 혼합 정규분포에서 낮은 확률값 p(x)를 갖는 구간이 이상 영역이 됩니다. 하지만 주의해야 할 점들이 있습니다. 
고차원에서는 차원의 저주 문제가 발생하고, 지역적인 밀도 변화에 취약할 수 있습니다. 
또한 데이터가 여러 군집으로 나뉘어 있을 때는 전역 밀도만으로는 판단이 어려울 수 있습니다.

거리 기반 방법은 각 데이터 포인트가 주변 이웃들로부터 얼마나 멀리 떨어져 있는지를 측정합니다. 
가장 간단한 방법은 k-최근접 이웃까지의 평균 거리를 이상도로 사용하는 것입니다. 
더 정교한 방법으로는 Local Outlier Factor가 있는데, 이는 국소적인 밀도 비율을 계산해서 지역적 희소성을 탐지합니다. 
이 방법들의 장점은 직관적이고 구현이 용이하다는 것입니다. 
하지만 특징들의 스케일에 민감하고, 대용량 데이터에서는 계산 부담이 크다는 한계가 있습니다.
 따라서 전처리에서 적절한 정규화가 중요합니다.

확률적 접근법은 데이터의 생성 과정을 통계 모델로 표현합니다. 
가우시안 혼합 모델(GMM)을 사용해 전체 데이터의 확률 분포 p(x)를 추정하고, 낮은 우도를 갖는 지점을 이상치로 판단합니다. 
Mahalanobis 거리는 데이터의 공분산 구조를 반영한 거리 측도로, 타원형 분포에서 효과적입니다.
 하지만 이 방법들은 강한 분포 가정에 의존합니다. 
실제 데이터가 봉우리가 여러개인 분포이거나 비정규 분포일 때, 그리고 고차원에서는 성능이 저하될 수 있습니다. 따라서 데이터의 분포 특성을 미리 분석하는 것이 중요합니다.

재구성 기반 방법의 핵심은 정상 데이터의 패턴을 학습해서 압축하고 다시 복원하는 것입니다. 대표적으로 오토인코더가 있습니다. 
정상 데이터로만 학습된 오토인코더는 정상 패턴은 잘 복원하지만, 학습하지 못한 패턴은 복원 오차가 큽니다. 이 재구성 오차를 이상으로 사용합니다. 
장점은 복잡한 비선형 패턴도 학습할 수 있다는 것입니다. 
하지만 과적합 문제가 있고, 이상 데이터가 학습에 누설되지 않도록 주의해야 합니다. 또한 적절한 잠재 공간의 차원을 선택하는 것도 중요한 과제입니다.

아이솔레이션 포레스트는 독특한 접근법을 사용합니다. 
무작위로 특징을 선택하고 임계값으로 데이터를 분할하는 과정을 반복해서, 각 포인트가 얼마나 빨리 고립되는지를 측정합니다. 
이상치는 정상 데이터보다 더 적은 분할로도 고립될 것이라는 아이디어입니다. 경로 길이가 짧을수록 이상도가 높습니다. 
이 방법의 큰 장점은 대규모 데이터에서도 효율적이고, 선형 시간 복잡도를 갖는다는 것입니다. 
주의할 점은 특징들의 값 범위가 크게 다를 때 스케일링이 필요하고, 범주형 변수를 다룰 때는 별도의 전략이 필요하다는 것입니다.

좋은 이상탐지를 위해서는 데이터 전처리와 특징 공학이 매우 중요합니다. 먼저 스케일링과 정규화를 통해 서로 다른 단위의 특징들을 동일한 척도로 맞춰야 합니다. 
로그 변환이나 Box-Cox 변환으로 치우친 분포를 정규화할 수 있습니다. 
시계열 데이터의 경우 주기성과 계절성을 분해해서 추세, 계절, 잔차 성분을 분리할 수 있습니다. 
도메인 지식을 활용한 파생 특징 생성도 중요합니다. 또한 U-Map이나 PCA 같은 차원 축소 기법으로 고차원 데이터의 본질적 구조를 찾을 수 있고, 임베딩 기법으로 복잡한 패턴을 저차원으로 표현할 수 있습니다.

이상 점수를 실제 경보로 전환하려면 적절한 임계값 설정이 필수입니다. 
가장 일반적인 방법은 점수 분포를 기반으로 상위 몇 퍼센트를 이상으로 분류하거나, 허용 가능한 거짓 양성율 FPR 예산을 설정하는 것입니다. 
강건한 통계량도 유용합니다. z-점수, MAD, IQR 등이 있습니다. 
더 정교한 방법으로는 극값 이론 EVT을 사용해서 상위 꼬리 부분의 분포를 모델링하고, 이를 통해 희귀 사건의 확률을 추정할 수 있습니다. 
이는 일관된 경보량을 유지하는 데 도움이 됩니다.

실제 운영 환경에서는 시간이 지남에 따라 데이터가 변합니다. 이를 드리프트라고 합니다. 
데이터 드리프트는 입력 분포의 변화, 특징 드리프트는 특징 자체의 변화, 개념 드리프트는 입력과 출력 관계의 변화를 의미합니다. 이에 대응하는 방법들이 있습니다. 
기준선을 주기적으로 재보정하거나, 슬라이딩 윈도우를 사용해서 최근 데이터에 더 가중치를 두거나, 부분적으로 모델을 재학습할 수 있습니다. 
모니터링 지표로는 PSI, KS 검정, 그리고 성능 대용 지표들을 사용해서 드리프트를 조기에 감지할 수 있습니다.

첫 번째 섹션을 정리해보겠습니다. 
우리는 이상도를 산출하는 다섯 가지 주요 패러다임을 살펴봤습니다. 밀도, 거리, 확률, 재구성, 아이솔레이션 기반 접근법 입니다. 
각각의 장단점을 이해하고 데이터 특성에 맞는 방법을 선택하는 것이 중요합니다. 
또한 이상 점수를 실용적인 임계값으로 전환하는 방법과, 시간에 따른 안정화 전략을 배웠습니다. 
마지막으로 데이터와 개념 드리프트에 대응하는 기본 전략들을 다뤘습니다. 이제 구체적인 알고리즘들을 살펴볼 차례입니다.

두 번째 섹션인 '정상 패턴 학습 및 이상치 식별'을 시작하겠습니다. 
이 섹션의 학습 목표는 다음과 같습니다. 먼저 정상 패턴을 모델링하는 두 가지 주요 프레임워크인 One-Class 학습과 재구성 학습을 비교해보겠습니다. 
다음으로 주요 알고리즘들과 시계열 데이터에 특화된 접근법들을 살펴보겠습니다.
 마지막으로 캘리브레이션, 앙상블 방법, 그리고 실무에서 중요한 해석가능성에 대해 다루겠습니다. 
이 섹션을 통해 이론을 실제 구현으로 연결하는 다리 역할을 하겠습니다.

정상 패턴을 학습하는 접근법은 크게 두 가지로 나뉩니다. 
첫 번째는 One-Class 학습입니다. 이는 정상 데이터를 둘러싸는 경계를 학습하는 방법으로, OC-SVM이나 Deep SVDD가 대표적입니다. 
두 번째는 재구성 학습으로, 데이터를 압축했다가 복원하는 과정에서 발생하는 재구성 오차를 이용합니다. 오토인코더나 VAE가 여기에 속합니다. 
어떤 방법을 선택할지는 데이터의 규모와 구조, 실시간성 요구사항, 해석 가능성 필요도에 따라 달라집니다. 
작은 데이터셋에는 OC-SVM이, 대용량 복잡 데이터에는 딥러닝 기반 재구성 방법이 적합할 수 있습니다.

클러스터링을 이용한 이상탐지는 간단하면서도 효과적입니다. 
k-means를 사용할 때는 각 포인트와 가장 가까운 군집 중심까지의 거리를 이상도 점수로 사용합니다. 
가우시안 혼합 모델(GMM)은 더 유연한데, 각 포인트가 어떤 군집에 속할 사후확률을 계산하고, 모든 군집에 대한 확률이 낮으면 이상치로 판단합니다. 
이는 소프트 할당 방식이라 더 정교합니다. 
클러스터링 방법의 장점은 빠르고 구현하기 쉽다는 것입니다. 하지만 군집의 개수를 미리 정해야 하고, 구형이나 타원형 형상 가정에 제한될 수 있습니다.

밀도 기반 방법들을 구체적으로 살펴보겠습니다. 
DBSCAN은 밀도가 높은 영역에서 군집을 형성하고, 어떤 군집에도 속하지 않는 포인트들을 이상 후보로 봅니다. 
LOF는 더 정교해서, 각 포인트의 국소 밀도를 주변 이웃들과 비교합니다. 
만약 어떤 포인트의 밀도가 이웃들보다 현저히 낮다면 이상치일 가능성이 높습니다. 
이는 군집 가장자리에 있는 이상치를 잘 감지합니다. 하지만 이 방법들은 파라미터 설정에 민감합니다. 
DBSCAN의 eps와 min-Pts, LOF의 k값 선택이 결과에 큰 영향을 미치므로 신중한 튜닝이 필요합니다.

One-Class SVM은 정상 데이터만을 사용해서 결정 경계를 학습합니다. 
원점으로부터 데이터를 분리하는 하이퍼플레인을 찾고, nu 파라미터로 이상치 비율을 제어합니다. 
커널 트릭을 사용하면 비선형 경계도 학습할 수 있습니다. 
Deep SVDD는 이 아이디어를 딥러닝으로 확장한 것입니다. 
신경망을 통해 데이터를 저차원 잠재 공간으로 매핑하고, 그 공간에서 구의 내부에 포함되는지 여부로 정상성을 판단합니다. 
이 방법들의 장점은 복잡한 비선형 경계를 다룰 수 있고 고차원 데이터에 효과적이라는 것입니다.
 하지만 하이퍼파라미터 튜닝이 까다로울 수 있습니다.

오토인코더 계열은 매우 다양합니다. 
기본 오토인코더는 입력을 그대로 복원하도록 학습하고, 디노이징 오토인코더는 노이즈가 추가된 입력에서 원본을 복원하도록 해서 일반화 능력을 강화합니다.
 VAE(Variational Autoencoder)는 확률적 잠재 공간을 학습해서 우도 기반 점수화도 가능합니다.
 시계열 데이터에는 LSTM이나 TCN을 사용한 오토인코더가 효과적이고, 이미지나 스펙트럼 데이터에는 컨볼루션 오토인코더가 적합합니다. 
각각의 구조는 데이터의 특성과 패턴의 복잡도에 맞춰 선택해야 하며, 정상 데이터의 다양성과 이상 패턴의 예상 형태를 고려하는 것이 중요합니다.

시계열 데이터의 이상탐지는 특별한 접근이 필요합니다. 예측 기반 방법은 Arima, Prophet, LSTM 등으로 미래값을 예측하고, 실제값과의 차이를 이상도로 사용합니다. 
분해 기반 방법은 STL이나 RobustSTL로 시계열을 추세, 계절성, 잔차 성분으로 분해한 후, 잔차에서 이상을 탐지합니다. 
이는 계절적 패턴이 있는 데이터에서 특히 유용합니다. 
변화점 탐지는 시계열의 통계적 성질이 급격히 바뀌는 지점을 찾습니다. 
BOCPD나 Ruptures 같은 알고리즘을 사용합니다. 각 방법의 선택은 데이터의 주기성, 노이즈 수준, 변화의 특성에 따라 달라집니다.

이상 점수를 일관되고 해석 가능한 형태로 변환하는 것이 캘리브레이션입니다. 
분위 기반 방법은 역사적 점수 분포에서 99.5% 같은 상위 백분위수를 임계값으로 사용합니다. 
롤링 윈도우를 적용하면 최근 데이터에 적응적으로 반응할 수 있습니다. 
강건화 기법으로는 MAD를 사용한 modified 제트-score, Huber 변환, 윈저라이징 등이 있습니다. 
이들은 극값에 덜 민감합니다. 
극값 이론의 POT 방법은 임계값을 넘는 상위 꼬리 부분의 분포를 모델링해서 희귀 사건의 확률을 정확히 추정합니다. 이를 통해 일관된 알림량을 유지할 수 있습니다.

단일 모델의 한계를 극복하기 위해 앙상블을 사용할 수 있습니다. 
모델 앙상블에서는 Isolation Forest, 오토인코더, OC-SVM 등 서로 다른 알고리즘의 점수를 스태킹하거나 평균을 구합니다. 
각 모델이 서로 다른 종류의 이상을 잘 탐지할 수 있기 때문입니다. 
규칙 결합 방식은 휴리스틱 룰을 가드레일로 두고, 모델 점수와 게이팅합니다. 예를 들어, 특정 임계값을 넘는 경우에만 모델을 신뢰하는 방식입니다. 
앙상블의 장점은 강건성이 향상된다는 것입니다. 
하지만 복잡도와 운영 비용이 증가하므로, 성능 향상과 복잡성 사이의 트레이드오프를 신중히 고려해야 합니다.

실무에서는 '왜 이상인지' 설명할 수 있어야 합니다. 재구성 기반 모델에서는 재구성 히트맵을 통해 어느 부분이 잘 복원되지 않았는지 시각화할 수 있습니다. 
이미지나 스펙트로그램 데이터에서 특히 유용합니다. 
특징 중요도 분석에는 Shap을 사용해서 각 특징이 이상 점수에 미친 기여도를 계산할 수 있습니다. 
이는 비록 근사치이지만 유용한 통찰을 제공합니다. 
Counterfactual 설명은 '어떻게 바뀌어야 정상이 될지'를 보여줍니다. 
사후 진단 리포트에는 가능한 원인 후보들과 영향을 받을 수 있는 지표들을 포함해서, 운영자가 빠르게 대응할 수 있도록 돕습니다.

실제 운영에서는 비즈니스 목표와 연결된 임계값 설정이 중요합니다. 
FPR 예산 기반 접근에서는 '하루에 경보 N건까지 허용'과 같은 운영 제약을 고려합니다. 
Top-k 리뷰 방식은 상위 k개만 검토하도록 하여 인적 자원을 효율적으로 활용합니다. 
비용 민감 최적화에서는 놓친 이상의 비용과 거짓 경보의 비용을 비교해서 최적 임계값을 찾습니다. 
계층형 임계값은 고객 세그먼트나 제품군별로 다른 기준을 적용합니다. 
VIP 고객은 더 낮은 임계값으로, 일반 사용자는 높은 임계값으로 설정하는 방식입니다. 
이런 전략들은 비즈니스 가치와 운영 효율성을 균형 있게 고려합니다.

두 번째 섹션을 정리해보겠습니다. 정상 패턴을 모델링하는 다양한 기법들의 강점과 약점을 파악했습니다. 
One-Class 방법과 재구성 방법, 클러스터링과 밀도 기반 방법들 각각의 특성을 이해했습니다. 
캘리브레이션과 앙상블을 통해 모델을 강건화하는 방법도 배웠습니다. 
마지막으로 해석가능성과 운영 지표를 통해 현업의 수용성을 확보하는 전략을 다뤘습니다. 
이제 이론과 알고리즘을 실제 산업 현장에서 어떻게 적용하는지 사례를 통해 살펴보겠습니다.

마지막 섹션인 '이상탐지 실무 적용 사례'를 시작하겠습니다. 
이 섹션의 학습 목표는 세 가지입니다. 먼저 주요 도메인에서의 적용 패턴을 살펴보겠습니다. 
금융, 제조, 보안, AI-Ops 각 분야의 특성을 이해할 것입니다. 
다음으로 실시간 스트리밍 아키텍처와 MLOps 운영 방법을 다루겠습니다.
마지막으로 실무에서 자주 발생하는 경보 피로도문제와 휴먼 인 더 루프 전략을 살펴보겠습니다. 
이 섹션을 통해 이론을 실제 비즈니스 가치로 연결하는 방법을 배우겠습니다.

실제 산업 현장에서 이상탐지가 어떻게 적용되는지 네 가지 대표 도메인을 살펴보겠습니다. 
금융에서는 사기 거래 탐지가 핵심이고, 제조업에서는 설비 고장을 미리 예측하는 예지보전이 중요합니다. 
네트워크 보안에서는 침입이나 악성 활동을 실시간으로 탐지해야 하고, AI-Ops에서는 복잡한 IT 시스템의 장애를 조기에 발견해야 합니다.
각 사례마다 데이터 소스, 선택한 모델, 측정하는 성과 지표, 그리고 직면한 과제들이 다릅니다. 
또한 오프라인 실험실에서의 성능과 실제 온라인 운영에서의 성능 차이도 고려해야 할 중요한 요소입니다.

금융 거래 이상탐지 사례를 구체적으로 살펴보겠습니다. 
데이터로는 거래 메타데이터, 단말기 정보, 위치 정보, 그리고 고객 관계 그래프에서 추출한 특징들을 사용합니다. 
모델은 Isolation Forest와 VAE를 결합한 하이브리드 접근법을 선택했습니다.
 Isolation Forest는 명시적인 이상 패턴을 빠르게 탐지하고, VAE는 복잡한 정상 패턴을 학습해서 subtle한 이상을 포착합니다. 
성과 측면에서는 경보의 정밀도가 향상되었고, 기존 수동 룰 대비 탐지력이 크게 개선되었습니다. 하지만 과제도 있습니다. 
인간 전문가의 개입 비용이 높고, 사기범들이 탐지 시스템에 적응해서 새로운 패턴을 만들어내는 문제가 있습니다.

다음은 제조업 예지보전 사례입니다. 
전체 파이프라인을 보면, 센서에서 데이터를 수집해서 피쳐 엔지니어링을 거쳐 LSTM 오토인코더로 학습하고 경보를 생성합니다. 
해당 사례에서는 전처리 단계가 매우 중요합니다. 
여러 센서의 데이터를 시간 동기화하고, 결측치와 센서 자체의 이상치를 클린징해야 합니다. 
계절성 제거를 통해 주기적 변동을 정규화합니다. 
LSTM-AE는 시계열의 복잡한 temporal dependency를 학습합니다. 
성과로는 설비 고장을 몇 시간에서 며칠 전에 미리 예측할 수 있게 되어 조기 경보 리드타임이 늘어났고, 계획되지 않은 다운타임이 크게 감소했습니다. 
이는 생산성과 비용 절감에 직접적으로 기여합니다.

네트워크 보안 영역의 침입 탐지 사례입니다. 데이터는 네트워크 플로우 통계, 패킷 레벨 정보, 그리고 시스템 로그 이벤트를 활용합니다. 
실시간 처리를 위해 스케치 기반 특징을 사용해서 고속으로 요약 통계를 계산합니다. 
모델로는 OC-SVM과 Isolation Forest를 조합해서 사용합니다. 
운영 측면에서는 스트리밍 집계를 통해 sliding window 방식으로 실시간 처리하고, 과탐지를 완화하기 위한 세밀한 튜닝이 필요합니다. 
네트워크 보안의 특성상 지연 시간이 매우 중요하므로, 밀리초 단위의 응답 시간을 유지하면서도 높은 정확도를 달성해야 하는 것이 주요 과제입니다.

AI-Ops는 복잡한 IT 시스템을 자동으로 모니터링하는 분야입니다. 여기서는 멀티시그널 상관분석이 핵심입니다. 
시계열 메트릭의 이상과 로그 클러스터링 결과를 함께 분석해서 더 정확한 판단을 합니다. 
루트코즈 추정을 위해서는 서비스 의존성 그래프를 활용해서 이상이 어떻게 전파되는지 분석합니다. 
예를 들어, 데이터베이스 응답 지연이 웹 서버 오류로 이어지고, 최종적으로 사용자 경험 저하로 나타나는 연쇄 과정을 추적합니다. 
경보 중복제거를 통해 동일한 근본 원인에서 발생한 여러 경보들을 하나로 묶고, 자동으로 인시던트를 생성해서 담당자에게 통지합니다. 이를 통해 MTTR(Mean Time To Resolution)을 크게 단축할 수 있습니다.

실시간 이상탐지를 위한 스트리밍 아키텍처를 살펴보겠습니다. 
데이터 수집 단계에서는 Kafka나 Amazon Kinesis 같은 메시지 큐를 사용해서 대용량 데이터를 안정적으로 처리합니다. 
스트림 처리 엔진으로는 Apache Flink나 Spark Streaming을 사용해서 실시간으로 특징을 계산하고 모델을 실행합니다. 
특징 저장소(Feature Store)를 통해 온라인과 오프라인 특징의 일관성을 보장합니다. 이는 모델 학습과 서빙 간의 데이터 불일치를 방지합니다. 
알림 시스템에는 서킷브레이커 패턴으로 과부하를 방지하고, 기능 토글로 유연하게 운영할 수 있으며, SLA와 연계해서 우선순위를 관리합니다.

이상탐지 시스템의 MLOps 운영은 일반적인 ML과 몇 가지 차이점이 있습니다. 
드리프트 감지가 특히 중요한데, 데이터 드리프트, 스키마 변경, 개념 드리프트를 실시간으로 모니터링해야 합니다. 
모델 레지스트리를 통해 버전을 관리하고, 카나리 배포나 섀도우 모드로 새 모델을 안전하게 테스트합니다. 이때, 피드백 루프 구축이 핵심입니다. 
도메인 전문가나 운영자로부터 라벨을 수집하고, 이를 바탕으로 주기적으로 재학습합니다. 
하지만 이상탐지의 특성상 양성 피드백이 부족하므로, 액티브 러닝이나 준지도 학습 기법을 활용해서 효율적으로 학습 데이터를 확보하는 전략이 필요합니다.

실무에서 가장 큰 도전 중 하나는 경보 피로도입니다. 
너무 많은 거짓 경보가 발생하면 운영자들이 무감각해져서 진짜 이상도 놓칠 수 있습니다. 
이를 해결하기 위한 트리아지 워크플로를 설계해야 합니다. 
먼저 자동으로 명확한 케이스들을 분류하고, 불확실한 경우만 인간 전문가에게 전달합니다. 
우선순위는 위험도와 비용을 기반으로 라우팅합니다. 
액티브 러닝을 활용해서 모델이 가장 불확실한 샘플들을 선택적으로 라벨링 요청하면, 학습 효율을 높일 수 있습니다.
또한 피드백을 통해 모델이 지속적으로 개선되도록 하는 것이 중요합니다. 
이런 휴먼-AI 협업 시스템을 통해 정확도와 효율성을 동시에 향상시킬 수 있습니다.

오늘 발표의 핵심 내용을 정리하겠습니다. 이상탐지의 원리에서는 다섯 가지 패러다임을 배웠습니다
. 밀도, 거리, 확률, 재구성, 아이솔레이션 기반 접근법입니다. 
모델링 관점에서는 One-Class 학습과 재구성 학습의 차이점, 그리고 시계열에 특화된 방법들을 살펴봤습니다.
 운영 측면에서는 스트리밍 아키텍처, MLOps 파이프라인, 휴먼 루프 전략을 다뤘습니다. 
다음 단계로는 여러분의 도메인에 맞는 파일럿 프로젝트를 설계하고, 데이터 품질을 점검하며, 명확한 성공 지표를 설정하는 것을 권합니다. 
이상 지금까지 강의를 들어주셔서 감사합니다.
