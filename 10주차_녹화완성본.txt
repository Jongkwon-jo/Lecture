이겸
안녕하십니까. 오늘은 자연어 처리 기술이 어떻게 진화해왔는지, 그리고 Transformer가 가져온 구조적 혁신에 대해 말씀드리겠습니다.
최근 ChatGPT와 같은 대화형 AI가 전 세계적으로 주목받고 있죠. 이러한 혁신의 중심에는 바로, NLP와 Transformer 기술이 자리하고 있습니다.

본 강의는 총 4개 파트로 구성되어 있습니다. 첫 번째는 자연어 처리 기술의 발전 과정을 다룹니다. 규칙 기반에서 출발해 통계적 방법론을 거쳐, 최종적으로 신경망 기반 모델로 어떻게 진화했는지 살펴보겠습니다.
두 번째 파트에서는 Transformer의 구조적 혁신성을 깊이 있게 분석합니다. Self-Attention 메커니즘과 병렬 처리 능력이 어떻게 패러다임을 바꿨는지 확인하실 수 있을 겁니다.
세 번째로는 사전학습과 미세조정 전략을 다룹니다. Bert, GPT와 같은 모델들이 어떻게 학습되고 실제 업무에 적용되는지 알아보겠습니다.
마지막으로 실제 응용 사례와 운영 전략, 그리고 윤리적 고려사항까지 포괄적으로 다루며 마무리하겠습니다.

먼저 산업 현장에서 NLP가 왜 중요한지 살펴보겠습니다. 기업들은 매일 방대한 양의 비정형 텍스트 데이터를 생성하고 있습니다. 고객 문의, 리뷰, 내부 문서 등이 대표적이죠.
NLP 기술은 이러한 데이터에서 맥락을 이해하고 의미 있는 인사이트를 추출해냅니다. 단순히 키워드를 매칭하는 수준을 넘어서, 문장 전체의 뉘앙스와 의도까지 파악할 수 있습니다.
실제 도입 효과를 보면 놀랍습니다. 의도 분류 정확도는 95% 이상을 달성하고 있고요. 처리 속도는 기존 대비 10배 향상되었으며, 운영 비용은 대폭 절감되고 있습니다.
특히 법률, 의료, 금융처럼 전문성이 요구되는 분야에서도 도메인 미세조정을 통해 탁월한 성과를 내고 있죠.

이제 본격적으로 첫 번째 파트를 시작하겠습니다. NLP 기술이 어떤 과정을 거쳐 현재의 모습에 이르렀는지 역사적 맥락에서 살펴보죠.
규칙 기반 시스템의 한계에서 출발해, Word-two-Vec이 가져온 임베딩 혁명, 그리고 RNN의 등장과 한계, 마지막으로 Attention 메커니즘의 돌파구까지 순차적으로 알아보겠습니다.

NLP의 발전은 크게 세 가지 패러다임으로 나눌 수 있습니다. 1990년대부터 2000년대 초반까지는 규칙과 통계가 주류였습니다. 언어학자들이 수작업으로 문법 규칙을 정의하고, n-gram 같은 통계적 확률 모델을 활용했죠.
2010년대 들어서면서 딥러닝이 본격적으로 도입됩니다. 2013년 Word-two-Vec의 등장은 혁명적이었습니다. 단어를 벡터로 표현하면서 의미적 관계를 수학적으로 계산할 수 있게 되었거든요.
그리고 2017년, Google이 발표한 Transformer 논문이 모든 것을 바꿔놓았습니다. "Attention is All You Need"라는 제목처럼, Self-Attention 메커니즘만으로도 놀라운 성능을 달성할 수 있다는 것을 증명했죠.
2022년 ChatGPT의 등장은 이러한 기술이 대중화되는 결정적 계기가 되었습니다.

전통적인 NLP 방식들의 한계를 먼저 짚어보겠습니다. 규칙 기반 시스템은 높은 정확도를 보일 수 있지만, 예외 처리가 극도로 어렵습니다. 새로운 표현이나 문맥이 등장할 때마다 규칙을 추가해야 하기 때문이죠.
통계 기반 방식은 데이터에서 패턴을 학습한다는 점에서 진일보했습니다. 하지만 희소성 문제에 직면합니다. 학습 데이터에 없었던 단어 조합은 제대로 처리할 수 없었거든요.
여기서 주목할 것이 표현 방식의 전환입니다. 전통적인 원핫 인코딩은 단어 수만큼 차원이 증가하며, 대부분의 값이 0인 희소 벡터를 만들어냅니다. 어휘가 10만 개라면 벡터 크기도 10만이 되는 거죠.
반면 신경망 기반 임베딩은 고정된 저차원 밀집 벡터로 의미를 압축합니다. 512차원 정도면 충분하며, 모든 값이 실수로 채워져 있어 정보 밀도가 월등히 높습니다.
가장 흥미로운 것은 벡터 연산으로 의미 관계를 표현할 수 있다는 점입니다. "King" minus "Man" plus "Woman"을 계산하면 "Queen"과 유사한 벡터가 나옵니다.

단어 임베딩의 이론적 기반은 분포 가설에 있습니다. "단어의 의미는 그 주변 단어들에 의해 결정된다"는 언어학적 통찰이죠. 비슷한 문맥에서 자주 등장하는 단어들은 유사한 의미를 가질 가능성이 높다는 것입니다.
Word-two-Vec은 이를 신경망으로 구현했습니다. C-bow 방식은 주변 단어로 중심 단어를 예측하고요. Skip-gram은 반대로 중심 단어로 주변 단어를 예측하죠.
GloV는 좀 더 진화된 접근법을 취합니다. 전체 코퍼스의 동시 등장 행렬을 분석해, 지역적 문맥과 전역적 통계를 모두 활용합니다.
이러한 임베딩 기술들이 현대 NLP의 토대가 되었습니다.

RNN과 Sequence-two-Sequence 모델은 순차적 데이터 처리에서 획기적인 발전이었습니다. 입력 순서를 고려하고, 가변 길이 문장을 자연스럽게 다룰 수 있었거든요.
하지만 구조적 한계도 명확했습니다. 첫째, 장기 의존성 문제입니다. 문장이 길어질수록 앞쪽 정보가 점차 희석되며, 역전파 과정에서 기울기 소실이 발생합니다.
둘째, 병렬화가 불가능하다는 치명적 약점이 있습니다. 이전 시점의 계산이 완료되어야 다음 시점을 계산할 수 있어, GPU의 대규모 병렬 연산 능력을 제대로 활용하지 못했죠.
추가로 Sequence-two-Sequence 모델의 성능 저하를 발생시키는 요인은 정보 병목 현상입니다. Sequence-two-Sequence는 입력 문장 전체를 고정 크기 벡터 하나에 압축해야 합니다. 
문장 길이가 30단어를 넘어가면 번역 품질이 급격히 떨어지는 경향을 보였습니다. 결국 학습 속도가 느리고, 대용량 데이터로 확장하는 데 큰 제약이 있었습니다.

Attention은 RNN의 병목 현상을 해결하기 위해 등장했습니다. 핵심 아이디어는 간단합니다. 고정된 벡터 하나에 모든 정보를 담는 대신, 필요할 때마다 입력 문장의 관련 부분을 다시 참고하는 것이죠.
디코더가 단어를 생성할 때마다 인코더의 모든 Hidden State와 유사도를 계산합니다. 그리고 가중치를 적용해 동적으로 Context Vector를 만들어냅니다.
이를 통해 두 가지 문제가 해결되었습니다. 첫째, 긴 문장에서도 멀리 떨어진 단어 정보를 직접 접근할 수 있어 장기 의존성이 크게 개선되었습니다.
둘째, Attention Weight를 시각화하면 모델이 어느 부분에 집중하는지 해석할 수 있게 되었죠. 다양한 연구 이후 기계 번역의 표준이 되었으며, Transformer의 기반이 됩니다.

두 번째 파트로 넘어가겠습니다. Transformer는 단순히 성능이 좋은 모델이 아니라, NLP 패러다임 자체를 완전히 바꿔놓은 아키텍처입니다.
Self-Attention, 멀티헤드, 포지셔널 인코딩이 어떻게 조화를 이루는지 상세히 알아보겠습니다.

Transformer는 크게 인코더와 디코더로 구성됩니다. 인코더는 6개의 동일한 레이어가 쌓여 있으며, 각 레이어는 Multi-Head Self-Attention과 Feed-Forward Network를 포함하죠.
중요한 점은 양방향으로 문맥을 학습한다는 것입니다. 문장 전체를 한 번에 보면서 각 단어 간의 관계를 파악합니다.
디코더 역시 6개 레이어로 구성되지만, 추가적으로 Cross-Attention 레이어가 있습니다. 이를 통해 인코더의 출력을 참조하죠. 또한 Masked Self-Attention을 사용해 미래 토큰을 보지 못하도록 합니다.
순환 구조가 없기 때문에 단어의 순서 정보가 자동으로 반영되지 않습니다. 이를 해결하기 위해 포지셔널 인코딩을 입력 임베딩에 더해줍니다. 사인과 코사인 함수를 사용해 각 위치에 고유한 패턴을 부여하는 방식이죠.
코드로 보면 구조가 명확합니다. Encoder와 Decoder를 각각 초기화하고, 최종적으로 선형 레이어를 통해 어휘 크기만큼의 로짓을 출력합니다.

Self-Attention의 핵심은 Query, Key, Value 구조입니다. 검색 시스템과 유사한 개념이죠. "내가 찾고자 하는 것"이 쿼리이고, "데이터베이스의 색인"이 Key입니다.
Query와 Key의 내적을 계산해 유사도를 구한 뒤, 소프트맥스를 취해 가중치로 변환합니다. 그리고 이 가중치를 Value에 곱해 최종 출력을 만들어냅니다.
이 메커니즘의 강력한 점은 두 가지입니다. 첫째, 모든 토큰을 동시에 병렬 처리할 수 있어 연산 효율이 극대화됩니다. 둘째, 문장 내 거리가 아무리 멀어도 Attention Score를 통해 직접 연결되므로 장기 의존성이 완벽히 해결됩니다.

Transformer의 성능은 단순한 Attention만으로 나오는 것이 아닙니다. 여러 보조 메커니즘들이 정교하게 결합되어 있죠.
멀티헤드 어텐션은 입력을 여러 서브공간으로 나누어 병렬 처리합니다. 마치 여러 개의 눈으로 문장을 동시에 바라보는 것과 같습니다. 각 헤드는 서로 다른 문맥적 관계를 포착하게 되죠.
Add & Norm은 두 가지 기법의 조합입니다. Residual Connection은 입력을 출력에 직접 더해줌으로써 정보 소실을 방지합니다. 레이어 Normalization은 각 레이어의 출력을 정규화해 학습을 안정화시키죠.
이 조합 덕분에 깊은 신경망을 안정적으로 학습시킬 수 있게 되었습니다.
포지션 와이즈 FFN는 각 토큰마다 독립적으로 적용되는 비선형 변환입니다. 일반적으로 렐루 활성화 함수를 사용한 2층 구조를 가지며, 모델의 표현력을 크게 강화합니다.

포지셔널 인코딩은 단어의 순서 정보를 주입하는 전략입니다. 크게 세 가지 방식이 있습니다.
원조 Transformer는 Sinusoidal 방식을 사용했습니다. 사인과 코사인 함수로 위치를 정의하죠. 장점은 학습 없이도 작동하며, 훈련 시 보지 못한 긴 문장도 처리할 수 있다는 것입니다.
Bert와 GPT-Two는 Learned Positional Encoding을 채택했습니다. 데이터로부터 최적의 위치 정보를 학습하는 방식이죠. 더 유연하지만 최대 시퀀스 길이가 고정되는 단점이 있습니다.
최근 LLM들은 Relative Positional Encoding이나 RoPE를 선호합니다. 절대 위치 대신 토큰 간 상대적 거리를 학습하기 때문에 긴 문맥 처리에 매우 강력하죠.
각 방식은 트레이드오프가 있으며, 모델의 목적에 따라 적절히 선택해야 합니다.

RNN과 Transformer의 가장 큰 차이는 연산 방식에 있습니다. RNN은 순차적 연산을 수행합니다. 이전 Hidden State가 계산되어야만 다음 상태를 계산할 수 있죠. 시퀀스 길이에 비례해 O (n)의 순차 연산이 필요합니다.
반면 Transformer는 Self-Attention으로 모든 토큰을 동시에 처리합니다. 순차적 연산 횟수가 O (1)로 일정하죠. 이것이 GPU 병렬화에 최적화된 이유입니다.
물론 트레이드오프가 있습니다. Transformer는 모든 토큰 쌍을 비교하기 때문에 메모리와 연산량이 O (n제곱)으로 증가합니다. 문장이 길어질수록 부담이 커지는 거죠.
이를 해결하기 위해 Sparse Attention, FlashAttention 같은 최적화 기법들이 등장했습니다. 효율성을 유지하면서도 긴 문맥을 처리할 수 있게 되었죠.

Context Window의 진화는 눈부십니다. 초기 Bert는 512토큰, GPT-Three는 2,048토큰을 처리했습니다. 그런데 최근 모델들은 수십만, 심지어 백만 토큰 이상을 다룰 수 있습니다.
Claude-Two는 10만 토큰, Gemini 1.5는 무려 100만 토큰 이상을 처리하죠. 책 수십 권을 한 번에 읽고 이해할 수 있다는 뜻입니다.
이와 함께 주목할 것이 Scaling Laws입니다. 컴퓨팅 파워, 데이터 크기, 파라미터 수가 증가하면 모델 성능이 멱법칙을 따라 개선된다는 것이 실험적으로 증명되었습니다.
더 흥미로운 것은 창발적 능력입니다. 모델이 특정 규모를 넘어서면 이전에는 보이지 않던 복잡한 추론 능력이 갑자기 나타납니다. 이를 Emergent Abilities라고 부르죠.
그 결과 Zero-shot이나 Few-shot 학습이 가능해졌습니다. 별도의 파인튜닝 없이도 프롬프트만으로 다양한 태스크를 수행할 수 있게 된 것입니다.

Layer Normalization의 위치는 학습 안정성에 결정적 영향을 미칩니다. 원조 Transformer는 Post-LN 방식을 사용했습니다. 서브레이어 출력 후에 정규화를 수행하는 거죠.
문제는 출력층 근처에서 그래디언트가 커지면서 학습이 불안정해진다는 것입니다. Learning Rate를 천천히 올리는 Warm-up 단계가 필수적이었죠.
GPT-two와 GPT-three는 Pre-LN 방식으로 전환했습니다. Layer Norm을 서브레이어 입력 전에 수행하는 것이죠. 그래디언트가 하위 층으로 직접 전달되는 Identity Path가 생겨 훨씬 안정적입니다.
실제로 Pre-LN은 Warm-up 없이도 수렴이 잘 되며, 깊은 모델을 만들 때 특히 유리합니다. 최근 대부분의 거대 모델들이 Pre-LN을 표준으로 채택하고 있습니다.
추가적으로 Gradient Clipping, Stochastic Depth, Dropout 같은 기법들도 함께 사용되어 과적합을 방지하고 일반화 성능을 높입니다.

Transformer는 목적에 따라 세 가지 형태로 분화했습니다. 첫째, Encoder-only 구조입니다. 벌트, Ro-berta, 알버트가 대표적이죠. 양방향 문맥을 파악하는 데 특화되어 있어 분류, 개체명 인식, 질의응답 같은 이해 태스크에 강합니다.
둘째, Decoder-only 구조입니다. GPT 계열이 여기 속하죠. 단방향 자기회귀 생성에 최적화되어 있으며, 자연스러운 텍스트 생성, 대화형 AI, 코드 생성 등에서 탁월한 성능을 보입니다.
셋째, 인코더-디코더 구조입니다. T-Five, Bart, MarianMT가 대표적입니다. 입출력의 길이나 언어가 다를 때 유리하며, 기계 번역, 문서 요약, 문장 변환 같은 Sequence-To-Sequence 태스크에 적합합니다.
각 아키텍처는 명확한 역할 분담이 있습니다. Bert는 자연어 이해의 강자이고, GPT는 자연어 생성의 대표주자입니다. T-Five는 모든 문제를 텍스트-투-텍스트로 통합하는 범용적 접근을 취하죠.
실무에서는 태스크 특성을 고려해 적절한 아키텍처를 선택하는 것이 중요합니다.

세 번째 파트로 넘어갑니다. 거대 언어 모델의 핵심은 사전학습과 미세조정이라는 2단계 패러다임입니다.
어떤 학습 목표로 사전학습하고, 어떻게 효율적으로 파인튜닝하는지 자세히 살펴보겠습니다.

모델 학습 전에 텍스트를 토큰으로 나누는 과정이 필수적입니다. 서브워드 알고리즘이 널리 쓰이는데, 대표적으로 세 가지가 있습니다.
BPE는 빈도 기반으로 문자 쌍을 반복적으로 병합합니다. GPT-투, GPT-쓰리, Ro-berta가 사용하죠. 간단하고 효과적이지만 빈도에만 의존한다는 한계가 있습니다.
WordPiece는 Bert의 핵심입니다. 병합 시 우도를 최대화하는 쌍을 선택하며, 샵 기호로 서브워드를 구분하죠. 통계적으로 더 정교한 접근법입니다.
SentencePiece는 T-Five와 LaMa에서 사용됩니다. 공백도 하나의 문자로 취급해 언어에 구애받지 않습니다. 다국어 모델에 특히 적합하죠.
선택 기준은 OOV, 즉 사전에 없는 단어에 대한 사전 처리 능력, 언어 특성, 그리고 기존 모델과의 호환성입니다.

사전학습 방식은 크게 세 가지로 나뉩니다. 첫째, Masked Language Modeling입니다. Bert가 대표적이죠. 문장에서 일부 단어를 가리고 양방향 문맥으로 예측합니다. 이해 태스크에 강력하지만 생성은 불가능합니다.
둘째, Causal Language Modeling입니다. GPT 계열의 방식이죠. 이전 토큰들만 보고 다음 단어를 순차적으로 예측합니다. 생성 능력은 탁월하지만 양방향 이해는 제한적입니다.
셋째, Span Corruption입니다. T-Five가 사용하는 방식으로, 연속된 토큰 구간을 마스킹하고 이를 생성해냅니다. 이해와 생성을 모두 아우르는 하이브리드 접근이죠.
각 방식은 명확한 트레이드오프가 있습니다. MLM은 문맥 깊이가 뛰어나지만 생성이 약하고, CLM은 생성은 강하지만 이해가 얕습니다. Span Corruption은 균형을 추구하죠.

2018년은 NLP의 분수령이었습니다. Bert와 GPT-One이 동시에 등장하면서 사전학습 패러다임을 확립했죠.
Bert는 Masked LM으로 양방향 문맥을 학습해 자연어 이해 성능을 비약적으로 향상시켰습니다. GPT는 단방향 생성으로 유창한 텍스트 작성 능력을 보여줬고요.
T-Five는 모든 NLP 태스크를 텍스트-투-텍스트로 통합했습니다. 번역, 요약, 분류 모두 같은 프레임워크로 처리할 수 있게 되었죠.
Electra는 학습 효율성에 집중했습니다. Generator, Discriminator 구조로 교체된 토큰을 탐지하는 방식인데, 같은 데이터로 10배 빠르게 학습할 수 있습니다.
GPT-Three는 1,750억 개 파라미터로 규모의 경제를 입증했습니다. Few-shot 학습만으로도 놀라운 성능을 보여줬죠.

파인튜닝은 성능과 자원 효율성 사이의 균형을 찾는 과정입니다. Full Fine-Tuning은 모델의 모든 파라미터를 업데이트합니다. 최고 성능을 낼 수 있지만, 막대한 GPU 메모리가 필요하고 태스크마다 독립된 모델 복사본을 관리해야 하죠.
Adapters나 Prefix Tuning은 작은 모듈만 학습합니다. 파라미터의 3~5%만 업데이트하며, 하나의 백본 모델에 여러 어댑터를 교체해 쓸 수 있습니다. 다만 추론 시 레이어가 추가되어 약간의 지연이 발생합니다.
Lora는 현재의 사실상의 표준입니다. 저랭크 분해를 통해 학습 파라미터를 1% 미만으로 줄이면서도, Full Fine-Tuning에 준하는 성능을 달성합니다.
가장 큰 장점은 추론 시 가중치를 원본에 병합할 수 있어 추가 지연이 전혀 없다는 것입니다. Q-LoRA는 여기에 양자화까지 결합해 메모리 효율을 극대화했죠.
실무에서는 대부분 Lora를 선택합니다. 성능, 속도, 비용의 스위트 스팟이기 때문입니다.

ChatGPT의 성공 비결은 RLHF 파이프라인에 있습니다. 총 3단계로 구성되죠.
첫 번째는 Supervised Fine-Tuning입니다. 고품질 지시-응답 페어로 모델을 튜닝해 기본적인 지시 이행 능력을 확보합니다.
두 번째는 Reward Model 학습입니다. 인간이 여러 응답을 비교해 순위를 매기면, 이를 학습해 답변 품질을 점수로 예측하는 모델을 만듭니다.
세 번째는 강화학습입니다. PPO 알고리즘을 사용해 Reward Model 점수를 높이는 방향으로 정책을 업데이트하죠. 여기서 중요한 것은 KL Divergence Penalty입니다. 원본 모델에서 너무 멀어지지 않도록 제약을 걸어 안정성을 확보합니다.
이 과정을 거치면 정렬된 모델이 탄생합니다. 안전하고, 유용하며, 정직한 AI가 되는 거죠. Helpful, Harmless, Honest의 세 가지 핵심 가치를 달성하게 됩니다.

범용 LLM을 특정 산업에 특화시키는 과정을 살펴보겠습니다. 첫 단계는 Continued Pretraining입니다. 의료 논문, 법률 판례, 금융 리포트 같은 도메인 원시 코퍼스로 추가 학습을 진행하죠. 전문 어휘와 지식이 모델에 주입됩니다.
두 번째는 Synthetic Data 생성입니다. Self-Instruct나 Evol-Instruct 으로 LLM이 스스로 학습 데이터를 생성합니다. 희소 케이스나 복잡한 추론 예시를 대량으로 확보할 수 있죠.
세 번째는 Task-Specific Fine-Tuning입니다. 실제 업무 데이터로 질의응답, 요약, 분류 등을 학습하고, 도메인 벤치마크로 평가합니다.
전 과정에서 휴먼 인 더 루프 검증이 필수적입니다. 최종적으로 전문성, 데이터 효율성, 지식 커버리지가 극대화된 도메인 특화 LLM이 완성됩니다.

NLP 모델 성능을 객관적으로 평가하는 것은 매우 중요합니다. 대표적인 벤치마크로 Glue가 있습니다. 9개의 다양한 자연어 이해 태스크를 묶어놓은 것이죠. 평균 점수로 모델의 종합 이해력을 측정합니다.
Squad는 질의응답 성능을 평가합니다. F-one Score로 정답과의 겹침 정도를 측정하죠. Bleu는 기계 번역 품질을 평가하는 지표로, 정답 번역과의 N-gram 매칭을 계산합니다.
Perplexity는 언어 모델의 예측 불확실성을 나타냅니다. 낮을수록 모델이 다음 단어를 확신 있게 예측한다는 뜻이죠.
최근에는 Human Evaluation도 중요해졌습니다. 유창성, 일관성, 사실성을 사람이 직접 평가하는 거죠. 특히 생성 태스크에서는 자동 지표만으로 품질을 판단하기 어렵기 때문입니다.

마지막 파트입니다. 지금까지 배운 기술들을 실제로 어떻게 적용하고 운영하는지 살펴보겠습니다.
태스크별 전략, Rag 시스템, 운영 아키텍처, 그리고 윤리적 고려사항까지 실무 관점에서 다루겠습니다.

NLP 태스크는 크게 네 가지 범주로 나뉩니다. 첫째, 분류입니다. 감성 분석, 스팸 필터링, 의도 분류 등이 해당되죠. Encoder 기반 모델이 효과적입니다.
둘째, 추출입니다. 개체명 인식, 관계 추출, 키워드 추출 등 텍스트에서 특정 정보를 뽑아내는 작업이죠. Bert 계열이 강점을 보입니다.
셋째, 생성입니다. 문서 요약, 기계 번역, 대화응답 생성 등이 포함됩니다. GPT나 T-Five 같은 생성 모델이 필요합니다.
넷째, 평가입니다. 텍스트 유사도 측정, 표절 감지, 품질 스코어링 등이죠. 임베딩 기반 유사도 계산이나 분류 모델을 활용합니다.
각 태스크에 맞는 모델과 전략을 선택하는 것이 성공의 핵심입니다.

래그는 검색과 생성을 결합한 혁신적 패러다임입니다. 세 단계로 구성되죠.
첫 번째는 인덱싱입니다. 문서를 청크로 나누고 임베딩해서 벡터 DB에 저장합니다. 의미적 검색이 가능한 인덱스를 만드는 거죠.
두 번째는 검색입니다. 사용자 질문을 임베딩하고 벡터 유사도로 관련 문서를 찾아냅니다. Top-K 개의 가장 관련성 높은 청크를 가져오죠.
세 번째는 생성입니다. 검색된 문서를 프롬프트에 포함시켜 LLM에 전달합니다. 모델은 이 컨텍스트를 바탕으로 답변을 생성하죠.
Rag의 장점은 명확합니다. 첫째, 환각을 크게 줄일 수 있습니다. 실제 문서에 기반하기 때문이죠. 둘째, 최신 정보를 반영할 수 있습니다. 벡터 DB만 업데이트하면 되니까요. 셋째, 출처를 제시할 수 있어 신뢰성이 높습니다.
기업의 내부 문서 검색, 고객 지원, 법률 리서치 등에서 널리 활용되고 있습니다.

모델 배포 방식은 크게 세 가지입니다. 첫째, 엣지 배포입니다. 모바일이나 IoT 기기에서 직접 실행하죠. 지연 시간이 가장 짧고 프라이버시가 보장되지만, 모델 크기가 매우 제한적입니다.
둘째, 온프레미스입니다. 자체 서버에 모델을 호스팅하는 방식이죠. 데이터 주권과 보안이 중요한 금융, 의료 분야에서 선호됩니다. 초기 투자 비용이 높지만 장기적으로 비용 효율적일 수 있습니다.
셋째, 클라우드입니다. AWS, Azure, GCP 같은 플랫폼에서 API 형태로 제공하죠. 가장 빠르게 시작할 수 있고 확장성이 뛰어나지만, 사용량에 따라 비용이 계속 발생합니다.
실무에서는 하이브리드 접근을 많이 씁니다. 민감한 데이터는 온프레미스에서 처리하고, 일반적인 작업은 클라우드로 보내는 식이죠.

모델 운영에서는 지속적인 모니터링이 필수적입니다. 네 가지 핵심 지표를 추적해야 하죠.
첫째, 정확성입니다. Task Accuracy, F-one Score, Hallucination Rate 등을 실시간으로 모니터링합니다. 특히 환각 비율은 치명적이므로 엄격하게 관리해야 합니다.
둘째, 속도입니다. Response Time, Throughput, Queue Depth를 추적하죠. SLA를 충족하는지 확인하고 병목 지점을 파악합니다.
셋째, 안전성입니다. Toxicity Score, Bias Metrics, Safety Filter Hit Rate 등을 모니터링합니다. 유해한 콘텐츠가 생성되지 않도록 방어합니다.
넷째, 자원 사용량입니다. GPU 또는 CPU 사용률, 메모리, 비용을 추적하죠. 캐싱 효율성도 중요한 지표입니다.
사용자 피드백도 적극 수집합니다. Thumbs Up and Down, 버그 리포트를 분석해 지속적으로 개선하죠.

LLM의 리스크는 크게 세 가지입니다. 첫째, 환각입니다. 사실이 아닌 내용을 그럴듯하게 생성하는 현상이죠. 래그, 사실 검증 레이어, 출처 제시 등으로 완화할 수 있습니다.
둘째, 유해성과 편향입니다. 모델이 학습 데이터의 편향을 그대로 반영하거나, 공격적인 콘텐츠를 생성할 수 있습니다. Content Filter, Bias Mitigation, Red Team Testing 등으로 대응하죠.
셋째, 프라이버시 침해입니다. 학습 데이터의 개인정보가 의도치 않게 출력될 수 있습니다. 데이터 익명화, Differential Privacy, PII Detection 등이 필요합니다.
기술적 대응뿐 아니라 정책적 대응도 중요합니다. 명확한 사용 가이드라인, 인간 검토 프로세스, 책임 소재 명시 등이 필요하죠.
계층적 방어 전략이 효과적입니다. 입력 검증, 모델 출력 필터링, 사후 모니터링을 다층으로 구축하는 것이죠.
윤리적 AI는 선택이 아니라 필수입니다. 신뢰받는 AI 시스템을 만드는 것이 장기적으로 성공의 열쇠입니다.

NLP 프로젝트의 경제성을 분석해보겠습니다. 3년 ROI를 시뮬레이션하면 명확한 그림이 그려집니다.
초기 투자 비용은 크게 세 가지입니다. 인프라 구축, 모델 개발, 인력 교육이죠. 클라우드 기반이라면 초기 투자는 줄어들지만 운영 비용이 지속적으로 발생합니다.
연간 기대 효과는 두 측면에서 나타납니다. 첫째, 직접적 비용 절감입니다. 인력 대체, 처리 시간 단축, 오류 감소 등이죠. 둘째, 간접적 가치 창출입니다. 고객 만족도 향상, 신규 서비스 개발, 데이터 인사이트 등입니다.
손익분기점은 보통 1~2년 내에 도달합니다. 고객 응대 자동화 같은 케이스는 6개월 만에 BEP를 달성하기도 하죠.
중요한 것은 단순 비용 절감을 넘어 전략적 가치를 고려하는 것입니다. 경쟁 우위, 시장 선점, 조직 역량 강화 같은 무형의 효과도 함께 평가해야 합니다.

지금까지 NLP와 Transformer의 발전 과정을 함께 살펴봤습니다. 핵심을 정리하면 이렇습니다.
첫째, NLP는 규칙 기반에서 데이터 기반으로, 다시 문맥 이해로 진화했습니다. 둘째, Transformer는 Self-Attention으로 병렬 처리와 장기 의존성을 모두 해결했습니다. 셋째, 사전학습과 파인튜닝 패러다임이 현대 NLP의 표준이 되었습니다. 넷째, 래그와 RLHF 같은 기법으로 실용성과 안전성이 크게 향상되었습니다.
실행 로드맵을 제시하겠습니다. 90일 계획으로 나눠볼 수 있습니다. 첫 30일은 기획입니다. 유스케이스 정의와 데이터 수집, 벤치마크 설정을 진행하죠.
다음 30일은 개발입니다. 모델 선택, 파인튜닝, 평가를 반복하며 프로토타입을 완성합니다.
마지막 30일은 배포입니다. 파일럿 런칭, 모니터링 구축, 피드백 수집 후 본격 확대하죠.
NLP 기술은 더 이상 먼 미래의 이야기가 아닙니다. 지금 당장 시작할 수 있으며, 실질적인 비즈니스 가치를 만들어낼 수 있습니다.
경청해주셔서 감사합니다.
