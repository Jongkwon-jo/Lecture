딥러닝 기초와 뉴럴네트워크 - 강의 스크립트
총 30분 분량 (슬라이드당 평균 50초)

슬라이드 1: 커버 (30초)
안녕하세요, 여러분. 오늘은 딥러닝의 기초와 뉴럴네트워크에 대해 함께 알아보겠습니다. 이 강의에서는 인공신경망의 구조와 학습 원리부터 시작해서, 실제 제조 현장에서 어떻게 적용할 수 있는지까지 살펴보겠습니다.

총 36페이지로 구성되어 있으며, 약 30분 정도 소요될 예정입니다. 그럼 지금부터 시작하겠습니다.

슬라이드 2: 목차 (40초)
먼저 전체 구성을 살펴보겠습니다. 첫 번째 파트에서는 인공신경망의 구조와 학습 원리를 다룹니다.

뉴런의 작동 원리부터 시작해서 Forward Propagation, Backpropagation, 그리고 최적화 기법까지 상세히 알아볼 것입니다. 두 번째 파트에서는 복잡한 패턴을 인식하는 방법을 배웁니다.

CNN, RNN, 그리고 최신의 Transformer 아키텍처까지 살펴보겠습니다. 마지막 세 번째 파트에서는 제조 현장에서의 실제 적용 사례와 도입 전략을 다룹니다.

슬라이드 3: 딥러닝 개요 (50초)
딥러닝이란 무엇일까요? 딥러닝은 다층 신경망을 통해 데이터에서 자동으로 특징을 학습하는 기술입니다.

전통적인 머신러닝과 다른 점은 사람이 직접 특징을 설계하지 않아도 된다는 것이죠. 대신 신경망이 데이터로부터 계층적으로 추상적인 특징을 스스로 학습합니다.

딥러닝의 핵심 특징은 비선형 모델링 능력입니다. 이를 통해 이미지 인식, 음성 처리, 자연어 이해 등 복잡한 패턴을 다룰 수 있습니다.

다만 대규모 데이터와 높은 연산 능력이 필요하다는 점을 기억해야 합니다.

슬라이드 4: 인공신경망의 역사 (50초)
인공신경망의 역사를 간단히 살펴보겠습니다. 1957년 퍼셉트론이 처음 등장했지만, 단층 구조의 한계로 오랜 기간 침체기를 겪었습니다.

1986년 역전파 알고리즘이 개발되면서 다층 신경망 학습이 가능해졌습니다. 1998년 LeNet이 등장하며 합성곱 신경망의 시대가 열렸죠.

하지만 진정한 돌파구는 2012년 AlexNet이었습니다. GPU를 활용한 대규모 학습으로 이미지 인식 성능을 획기적으로 향상시켰습니다.

2017년 Transformer가 등장하며 현재의 생성형 AI 시대로 이어지고 있습니다.

슬라이드 5: 뉴런의 구조 (50초)
이제 신경망의 기본 단위인 뉴런을 살펴보겠습니다. 뉴런은 여러 개의 입력 신호를 받아서 하나의 출력을 만들어냅니다.

각 입력에는 가중치가 곱해지고, 이들이 모두 더해진 후 편향이 더해집니다. 이 가중 합은 활성화 함수를 통과하여 최종 출력이 됩니다.

수식으로 표현하면 y = σ(w^T x + b)입니다. 여기서 σ는 활성화 함수, w는 가중치, b는 편향을 의미합니다.

이 간단한 구조가 수백, 수천 개 쌓이면서 복잡한 함수를 표현할 수 있게 됩니다.

슬라이드 6: 활성화 함수 (55초)
활성화 함수는 왜 필요할까요? 선형 변환만으로는 아무리 층을 쌓아도 하나의 선형 함수밖에 표현할 수 없습니다.

비선형 활성화 함수를 사용해야만 복잡한 패턴을 학습할 수 있습니다. 대표적인 활성화 함수로는 Sigmoid, Tanh, ReLU가 있습니다.

Sigmoid는 0과 1 사이의 값을 출력하지만 기울기 소실 문제가 있습니다. ReLU는 음수는 0으로, 양수는 그대로 출력하는 간단한 함수입니다.

현재 가장 많이 사용되는 ReLU는 계산이 빠르고 기울기 소실 문제를 완화합니다. 최근에는 Transformer에서 GELU라는 부드러운 형태의 활성화 함수도 많이 사용됩니다.

슬라이드 7: 신경망 아키텍처 (50초)
신경망의 전체 구조를 살펴보겠습니다. 가장 기본적인 구조는 입력층, 은닉층, 출력층으로 구성됩니다.

입력층은 원시 데이터를 받아들이는 첫 번째 층입니다. 은닉층은 하나 이상의 중간 층으로, 여기서 특징 추출과 변환이 일어납니다.

출력층은 최종 예측값을 생성합니다. 네트워크의 깊이(층의 개수)와 너비(각 층의 뉴런 개수)가 표현력을 결정합니다.

하지만 깊고 넓다고 무조건 좋은 것은 아니며, 과적합 위험도 함께 증가합니다.

슬라이드 8: Forward Propagation (45초)
순전파는 입력 데이터가 신경망을 통과하는 과정입니다. 첫 번째 층에서 입력과 가중치를 곱하고 활성화 함수를 적용합니다.

이 출력이 다음 층의 입력이 되어 같은 과정을 반복합니다. 최종적으로 출력층에서 예측값이 생성됩니다.

수식으로 표현하면 각 층 l에 대해 z^(l) = W^(l) a^(l-1) + b^(l)이고, a^(l) = σ(z^(l))입니다. 이 과정은 순차적으로 진행되며, 모든 층을 거쳐 최종 예측을 만들어냅니다.

슬라이드 9: Loss Function (50초)
손실 함수는 모델의 예측이 얼마나 틀렸는지를 수치화합니다. 회귀 문제에서는 주로 평균 제곱 오차(MSE)를 사용합니다.

분류 문제에서는 Cross-Entropy Loss가 가장 많이 사용됩니다. 손실 함수의 값이 작을수록 모델의 예측이 정답에 가깝다는 의미입니다.

학습의 목표는 이 손실 함수를 최소화하는 것입니다. 때로는 정규화 항을 추가하여 과적합을 방지하기도 합니다.

손실 함수의 선택은 문제의 특성에 따라 달라집니다.

슬라이드 10: Backpropagation (55초)
역전파는 딥러닝 학습의 핵심 알고리즘입니다. 손실 함수의 기울기를 출력층에서 입력층 방향으로 역순으로 계산합니다.

연쇄 법칙을 사용하여 각 가중치가 손실에 미치는 영향을 계산합니다. 수식으로는 ∂L/∂W와 ∂L/∂b를 구하는 과정입니다.

이 기울기 정보를 사용하여 가중치를 업데이트합니다. 역전파 없이는 깊은 신경망을 효율적으로 학습할 수 없습니다.

계산 그래프를 사용하면 자동으로 기울기를 계산할 수 있습니다. 현대의 딥러닝 프레임워크들은 모두 자동 미분 기능을 제공합니다.

슬라이드 11: Gradient Descent (50초)
기울기 하강법은 손실을 최소화하는 최적화 알고리즘입니다. 가중치를 기울기의 반대 방향으로 조금씩 업데이트합니다.

기본 SGD는 간단하지만 학습이 불안정할 수 있습니다. Momentum은 이전 방향을 일부 유지하여 진동을 줄입니다.

Adam은 현재 가장 많이 사용되는 최적화 알고리즘입니다. 학습률을 파라미터별로 적응적으로 조절하여 안정적이고 빠른 학습을 가능하게 합니다.

최근에는 학습률 스케줄링도 함께 사용하여 성능을 더욱 향상시킵니다.

슬라이드 12: 하이퍼파라미터 (50초)
하이퍼파라미터는 학습 전에 설정하는 값들입니다. 가장 중요한 것은 학습률로, 너무 크면 발산하고 너무 작으면 학습이 느립니다.

배치 크기는 한 번에 처리하는 데이터의 개수입니다. 크면 학습이 안정적이지만 메모리를 많이 사용합니다.

에폭은 전체 데이터를 몇 번 반복할지를 결정합니다. 가중 감쇠는 정규화의 강도를 조절합니다.

하이퍼파라미터 튜닝은 그리드 서치, 랜덤 서치, 베이지안 최적화 등을 사용합니다.

슬라이드 13: Overfitting vs Underfitting (50초)
과적합은 학습 데이터에만 지나치게 맞춰진 상태입니다. 학습 데이터에서는 성능이 좋지만 새로운 데이터에서는 성능이 떨어집니다.

과소적합은 모델이 너무 단순해서 패턴을 제대로 학습하지 못한 상태입니다. 학습 데이터에서도 성능이 낮게 나타납니다.

과적합을 해결하려면 정규화, 드롭아웃, 데이터 증강 등을 사용합니다. 과소적합은 모델의 복잡도를 높이거나 더 오래 학습시켜 해결합니다.

학습 곡선과 검증 곡선을 비교하면 어떤 문제인지 파악할 수 있습니다.

슬라이드 14: 정규화 기법 (45초)
정규화는 과적합을 방지하는 핵심 기법입니다. L1 정규화는 가중치의 절댓값 합을 페널티로 추가합니다.

L2 정규화는 가중치의 제곱 합을 페널티로 사용하며, 가장 일반적입니다. 데이터 증강은 기존 데이터를 회전, 이동, 크롭하여 학습 데이터를 늘립니다.

조기 종료는 검증 손실이 증가하기 시작하면 학습을 멈춥니다. 라벨 스무딩과 Mixup 같은 고급 기법도 사용됩니다.

슬라이드 15: BatchNorm & Dropout (50초)
배치 정규화는 학습 안정성을 크게 향상시킵니다. 각 미니배치의 평균과 분산을 0과 1로 정규화합니다.

내부 공변량 변화를 줄여 더 빠르고 안정적인 학습이 가능합니다. 드롭아웃은 학습 중 일부 뉴런을 랜덤하게 비활성화합니다.

이를 통해 특정 뉴런에 대한 의존도를 줄이고 일반화 성능을 높입니다. 주의할 점은 학습 모드와 추론 모드에서 동작이 다르다는 것입니다.

BatchNorm과 Dropout을 함께 사용할 때는 순서와 위치를 신중히 결정해야 합니다.

슬라이드 16: 학습 프로세스 (45초)
딥러닝의 전체 파이프라인을 살펴보겠습니다. 먼저 데이터를 수집하고 전처리 및 라벨링을 수행합니다.

학습 단계에서는 모델을 반복적으로 업데이트합니다. 검증 세트로 성능을 평가하고 하이퍼파라미터를 튜닝합니다.

최종적으로 테스트 세트로 최종 성능을 확인합니다. 배포 후에도 모니터링과 재학습을 통해 성능을 유지합니다.

슬라이드 17: 챕터 전환 - 패턴 인식 (30초)
이제 두 번째 파트로 넘어가겠습니다. 복잡한 패턴을 인식하는 고급 신경망 아키텍처를 배워보겠습니다.

CNN, RNN, Transformer 등 각각의 특징과 활용 분야를 알아보겠습니다.

슬라이드 18: 패턴 인식의 필요성 (45초)
왜 복잡한 패턴 인식이 필요할까요? 실제 데이터는 고차원이고 비선형적이며 잡음이 많습니다.

이미지는 수만 개의 픽셀로 구성되고, 음성은 연속적인 파형입니다. 센서 데이터는 시간에 따라 변화하는 복잡한 패턴을 가집니다.

딥러닝은 이런 복잡한 데이터에서 의미 있는 패턴을 자동으로 추출합니다. 이를 통해 정확도를 높이고 비용을 절감할 수 있습니다.

슬라이드 19: CNN 구조 (50초)
합성곱 신경망은 이미지 처리의 표준입니다. 합성곱 레이어는 필터를 사용하여 국소적인 특징을 추출합니다.

풀링 레이어는 특징 맵의 크기를 줄여 연산량을 감소시킵니다. 이 과정을 반복하면서 저수준 특징에서 고수준 특징으로 발전합니다.

초기 층에서는 에지나 코너 같은 단순한 특징을 학습합니다. 깊은 층으로 갈수록 얼굴, 물체 같은 복잡한 특징을 학습합니다.

최종 분류기는 완전 연결 층으로 구성됩니다.

슬라이드 20: 합성곱 레이어 (45초)
합성곱 연산을 자세히 살펴보겠습니다. 필터(또는 커널)가 입력 이미지 위를 슬라이딩하며 특징을 추출합니다.

각 위치에서 필터와 이미지 영역의 내적을 계산합니다. 스트라이드는 필터가 이동하는 간격을 결정합니다.

패딩은 출력 크기를 조절하기 위해 이미지 주변에 0을 채웁니다. 가중치 공유 덕분에 파라미터 수가 크게 줄어듭니다.

슬라이드 21: 풀링 레이어 (40초)
풀링은 특징 맵을 다운샘플링합니다. Max Pooling은 영역 내 최댓값을 선택하고, Average Pooling은 평균을 사용합니다.

풀링은 위치 불변성을 제공하여 작은 변화에 강건해집니다. 또한 연산량과 메모리를 절약할 수 있습니다.

다만 일부 정보 손실이 발생한다는 단점도 있습니다.

슬라이드 22: RNN 구조 (50초)
순환 신경망은 순차 데이터를 처리합니다. 은닉 상태가 이전 시점의 정보를 기억하며 다음 시점으로 전달됩니다.

시계열 데이터, 텍스트, 음성 같은 순서가 중요한 데이터에 사용됩니다. 하지만 기울기 소실 문제로 긴 시퀀스를 학습하기 어렵습니다.

이를 해결하기 위해 LSTM과 GRU가 개발되었습니다. RNN을 펼쳐 보면 깊은 신경망과 유사한 구조를 가집니다.

슬라이드 23: LSTM (50초)
LSTM은 장기 의존성 문제를 해결합니다. 망각 게이트는 과거 정보 중 버릴 것을 결정합니다.

입력 게이트는 새로운 정보 중 저장할 것을 선택합니다. 출력 게이트는 현재 출력할 정보를 결정합니다.

셀 상태는 장기 메모리 역할을 하며 정보를 보존합니다. 이 구조 덕분에 긴 시퀀스에서도 정보를 잘 유지할 수 있습니다.

제조 현장에서는 설비 고장 예측, 수요 예측 등에 활용됩니다.

슬라이드 24: Transformer (50초)
Transformer는 최신 딥러닝의 핵심입니다. Self-Attention 메커니즘으로 순차 처리 없이 병렬로 학습합니다.

Query, Key, Value를 사용하여 입력 간의 관계를 계산합니다. 다중 헤드 어텐션으로 다양한 관점에서 정보를 추출합니다.

위치 인코딩으로 순서 정보를 추가합니다. GPT, BERT 같은 대규모 언어 모델의 기반이 되었습니다.

RNN보다 빠르고 긴 의존성을 더 잘 학습합니다.

슬라이드 25: 모델 비교 (45초)
세 가지 주요 아키텍처를 비교해보겠습니다. CNN은 이미지와 공간 데이터에 최적화되어 있습니다.

RNN과 LSTM은 시계열과 순차 데이터에 적합합니다. Transformer는 범용성이 높으며 다양한 데이터 타입에 사용됩니다.

각 모델은 장단점이 있으므로 문제에 맞게 선택해야 합니다. 최근에는 여러 아키텍처를 결합한 하이브리드 모델도 많이 사용됩니다.

슬라이드 26: 전이 학습 (50초)
전이 학습은 실무에서 가장 유용한 기법입니다. 대규모 데이터로 사전 학습된 모델을 출발점으로 사용합니다.

특징 추출 레이어는 고정하고 분류기만 재학습합니다. 이를 통해 적은 데이터로도 높은 성능을 달성할 수 있습니다.

ImageNet으로 학습된 ResNet, EfficientNet 등이 대표적입니다. 미세 조정을 통해 목표 작업에 최적화합니다.

제조 현장에서도 일반 모델을 특정 제품에 맞게 조정하여 사용합니다.

슬라이드 27: 챕터 전환 - 제조 적용 (30초)
이제 마지막 파트입니다. 제조 현장에서 딥러닝을 어떻게 활용할 수 있는지 구체적인 사례를 살펴보겠습니다.

품질 검사부터 예지 보전까지 다양한 응용 분야를 다루겠습니다.

슬라이드 28: 제조업 AI 필요성 (45초)
제조업은 지금 큰 변화를 겪고 있습니다. 다품종 소량 생산으로 변동성이 커지고 있습니다.

숙련 인력이 부족하고 품질 기준은 더 엄격해지고 있습니다. AI는 이러한 문제를 해결할 핵심 기술입니다.

수율을 높이고 다운타임을 줄이며 비용을 절감할 수 있습니다. 안전사고를 예방하고 작업자를 보호할 수도 있습니다.

슬라이드 29: 품질 검사 자동화 (50초)
딥러닝은 품질 검사를 혁신하고 있습니다. 표면 결함을 1밀리미터 이하까지 정확하게 탐지할 수 있습니다.

조립 불량을 실시간으로 감지하여 즉시 대응합니다. 치수 편차를 자동으로 측정하고 분류합니다.

CNN 기반 모델로 분류, 검출, 세그멘테이션을 수행합니다. 정밀도와 재현율을 높여 불량품 유출을 최소화합니다.

사람보다 빠르고 일관성 있게 24시간 검사할 수 있습니다.

슬라이드 30: 예지 보전 (50초)
예지 보전은 설비 고장을 사전에 예측합니다. 진동, 전류, 온도 센서 데이터를 LSTM으로 분석합니다.

고장 확률을 실시간으로 계산하여 정비 시기를 최적화합니다. MTBF를 늘리고 갑작스런 다운타임을 줄입니다.

부품 교체 시기를 예측하여 재고 관리도 최적화합니다. 어떤 기업은 예지 보전으로 다운타임을 28% 감소시켰습니다.

비용 절감과 생산성 향상에 직접적인 효과가 있습니다.

슬라이드 31: 공정 최적화 (45초)
딥러닝으로 공정 파라미터를 최적화합니다. 온도, 압력, 속도 등 수십 개의 변수를 동시에 조정합니다.

실시간 센서 데이터를 분석하여 최적 조건을 찾습니다. 베이지안 최적화나 강화학습을 결합하기도 합니다.

Takt Time을 단축하고 에너지 효율을 높입니다. 공정 드리프트를 모니터링하여 품질을 일정하게 유지합니다.

슬라이드 32: 불량 탐지 시스템 (45초)
이상 탐지는 데이터가 부족한 상황에서 유용합니다. 정상 데이터만으로 학습하는 One-Class 방법을 사용합니다.

오토인코더는 정상 패턴을 학습하고 이상을 감지합니다. 복원 오차가 크면 불량으로 판단합니다.

이상 스코어를 계산하여 알림을 발생시킵니다. 공정 드리프트를 조기에 발견하여 대응할 수 있습니다.

슬라이드 33: 로봇 비전 (45초)
로봇 비전은 제조 자동화의 눈입니다. 픽앤플레이스 작업에서 물체를 정확히 인식하고 잡습니다.

자세 추정으로 로봇의 동작을 최적화합니다. 충돌 회피 시스템으로 안전을 확보합니다.

2D와 3D 비전을 결합하여 정확도를 높입니다. 키포인트 검출로 복잡한 형상도 처리합니다.

사이클 타임을 단축하고 픽 성공률을 높입니다.

슬라이드 34: 성공 사례 (50초)
실제 성공 사례를 살펴보겠습니다. A사는 표면 결함 검출로 불량률을 35% 감소시켰습니다.

수작업 검사 대비 10배 빠른 속도를 달성했습니다. B사는 예지 보전으로 다운타임을 28% 줄였습니다.

갑작스런 설비 중단이 70% 감소했습니다. 핵심 교훈은 데이터 품질의 중요성입니다.

MLOps 체계를 갖추고 현장과 긴밀히 협업해야 성공합니다.

슬라이드 35: 도입 로드맵 (50초)
AI 도입은 단계적으로 진행해야 합니다. 첫 번째 단계는 8~12주 파일럿 프로젝트입니다.

작은 범위에서 시작하여 ROI를 검증합니다. 두 번째 단계는 3~6개월 확장 단계입니다.

성공적인 사례를 다른 라인으로 확대 적용합니다. 세 번째는 지속적인 운영과 개선 단계입니다.

MLOps 체계를 구축하고 거버넌스를 확립합니다. 각 단계마다 명확한 KPI와 성공 기준을 설정해야 합니다.

슬라이드 36: 결론 및 Q&A (60초)
오늘 강의를 정리하겠습니다. 딥러닝은 복잡한 패턴을 학습하는 강력한 도구입니다.

신경망의 기본 원리부터 최신 Transformer까지 살펴봤습니다. 제조 현장에서는 품질 검사, 예지 보전, 공정 최적화 등에 활용됩니다.

실행 포인트를 말씀드리겠습니다. 먼저 데이터 인벤토리를 구축하고 품질을 확보하세요.

작은 파일럿으로 시작하여 ROI를 검증한 후 확장하세요. MLOps 체계를 갖추고 지속적으로 모니터링하세요.

참고 문헌은 Goodfellow의 Deep Learning 교과서, He et al.의 ResNet 논문, Vaswani의 Transformer 논문 등이 있습니다. 질문 있으시면 언제든지 말씀해 주세요. 감사합니다.
