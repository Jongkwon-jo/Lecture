📝 RAG 작동원리 교육 스크립트 (50분 분량)
업로드하신 PPT에 대한 발표 스크립트를 작성해드렸습니다. 슬라이드별로 자연스러운 호흡과 다양한 어미를 활용했습니다.

🎯 슬라이드 1: 표지 (1분)
안녕하세요, 여러분. 오늘 국립창원대학교 PRU 재직자 역량 강화 교육에 참여해주셔서 감사합니다.

오늘 우리가 함께 배울 주제는 'RAG 작동원리 및 실습'입니다. RAG는 Retrieval-Augmented Generation의 약자로, 검색 증강 생성 기술을 의미하죠.

이 기술은 최근 가장 주목받고 있는 AI 응용 기술 중 하나입니다. 특히 임베딩과 벡터 검색 구조를 이해하고, 사내 문서 검색 자동화 파이프라인을 직접 구축해보는 실습이 준비되어 있어요.

Python 실습, Vector DB, 그리고 LLM Application까지 다루게 될 거예요. 함께 50분간 집중해서 실무에 바로 적용 가능한 기술을 배워보시죠.

🎓 슬라이드 2: 학습 목표와 산출물 (2분)
먼저 오늘 교육의 목표를 명확히 하고 시작하겠습니다. 우리는 RAG 기술의 이론적 이해부터 사내 문서 검색 자동화 시스템 구축 실습까지 전 과정을 다루게 됩니다.

왼쪽에 보시는 핵심 학습 목표를 살펴볼까요? 첫째, RAG 개념을 완벽하게 이해하는 거예요. LLM의 한계를 어떻게 극복하는지 그 원리를 파악하게 됩니다.

둘째, Python 코드 중심 실습이에요. LangChain, ChromaDB 같은 최신 라이브러리를 직접 다뤄보실 겁니다.

셋째는 임베딩과 벡터 검색인데요, 텍스트를 벡터로 변환하고 유사도를 계산하는 원리를 이해하게 되죠. 넷째, 사내 문서 최적화 방법을 배웁니다. PDF나 Word 같은 비정형 데이터를 어떻게 처리하는지 알게 되는 거예요.

오른쪽에는 교육 산출물이 나와 있습니다. 여러분은 오늘 실습 노트북 파일, 벡터 DB 데이터, 그리고 Streamlit 기반의 미니 검색 앱을 직접 만들어보시게 될 거예요.

📋 슬라이드 3: 교육 과정 목차 (1.5분)
전체 커리큘럼을 살펴보겠습니다. 총 6단계로 구성되어 있어요.

첫 번째는 RAG 개념 이해입니다. RAG가 무엇이고 왜 필요한지, 전통적 검색과의 차이점을 배우게 되죠.

두 번째는 임베딩과 벡터 검색이에요. 텍스트 임베딩 모델의 원리를 이해하고 코사인 유사도 기반 검색을 실습합니다.

세 번째 단계는 오늘의 핵심이라고 할 수 있습니다. 사내 문서 검색 자동화를 위한 엔드-투-엔드 파이프라인을 구축해보는 거죠. 문서 파싱부터 청크 분할, 벡터 저장, LLM 연동까지 전 과정을 다룹니다.

네 번째는 실습 프로젝트예요. Python과 LangChain을 활용해서 미니 RAG 애플리케이션을 직접 구현해보실 겁니다.

다섯 번째는 평가 및 최적화인데요, Recall이나 MRR 같은 성능 평가 지표를 배우고 실전 최적화 전략을 익히게 됩니다. 마지막으로 핵심 내용을 요약하고 질의응답 시간을 가지겠습니다.

🔷 슬라이드 4: SECTION 01 시작 (0.5분)
자, 이제 본격적으로 첫 번째 섹션으로 들어가볼까요. RAG 개념과 필요성에 대해 알아보겠습니다.

왜 단순 LLM으로는 부족한 걸까요? RAG가 필요한 배경과 핵심 아이디어를 함께 이해해보시죠.

💡 슬라이드 5: RAG 정의 (2.5분)
RAG란 정확히 무엇일까요? Retrieval-Augmented Generation, 즉 검색을 통해 LLM의 지식을 확장하는 기술입니다.

기술 정의를 보시면, '검색 증강 생성'이라고 되어 있어요. 대규모 언어 모델, 즉 LLM에 외부 지식 베이스를 연결하는 아키텍처를 말하죠.

쉽게 비유하자면 '오픈북 방식'이라고 생각하시면 됩니다. 기억에만 의존하지 않고 참고 자료를 보면서 답안을 작성하는 시험과 비슷한 거예요.

핵심 3대 요소를 살펴볼까요? 첫째는 인덱싱입니다. 문서를 쪼개고 벡터로 변환해서 저장하는 과정이죠.

둘째는 검색, 즉 Retrieval이에요. 사용자 질문과 의미적으로 가장 유사한 문맥을 탐색하는 겁니다.

셋째가 생성, Generation인데요, 검색된 문서를 프롬프트에 포함시켜서 LLM이 최종 답변을 만들어내는 단계예요.

주요 기대 효과도 중요합니다. 첫째, 최신성을 확보할 수 있어요. 실시간 사내 문서나 뉴스 데이터를 즉시 답변에 반영할 수 있거든요.

둘째는 환각 감소입니다. AI가 근거 없이 거짓 답변을 하는 걸 줄이고, 사실 기반의 응답을 유도할 수 있죠. 셋째, 모델 재학습 없이도 새로운 정보를 즉시 반영할 수 있다는 점이에요. 마지막으로 답변의 출처를 명시할 수 있어서 신뢰도가 높아집니다.

🔍 슬라이드 6: 키워드 검색 vs 의미 기반 검색 (2분)
전통적인 키워드 검색의 한계를 먼저 이해해야 합니다. 왼쪽을 보시면 키워드 검색 방식이 나와 있어요.

BM25나 TF-IDF 같은 기법을 사용하는데요, 단순히 검색어와 문서 내 단어가 일치하는지만 확인하는 거예요. 예를 들어 "자동차"라고 검색하면, 문서에 "승용차"라고 쓰여 있으면 못 찾는 거죠.

동의어나 문맥을 이해하지 못하기 때문에 사용자가 정확한 용어를 모르면 정보를 찾기 어렵습니다. "차"라고 검색했을 때 Car를 말하는지 Tea를 말하는지 구분조차 못해요.

반면에 오른쪽의 의미 기반 검색을 보시죠. 텍스트를 고차원 벡터로 변환해서 의미적 유사도를 계산합니다.

단어가 달라도 문맥상 같은 의미라면, 예를 들어 "자동차"와 "승용차"는 높은 유사도로 검색되는 거예요. 벡터 공간에서 두 단어가 가까운 위치에 있기 때문이죠.

RAG의 핵심 가치가 바로 여기에 있습니다. 의미 기반 검색을 통해 LLM에게 질문과 가장 관련성 높은 참고 자료를 제공함으로써, AI가 팩트에 기반한 정확한 답변을 생성하도록 유도하는 거예요.

🏗️ 슬라이드 7: RAG 참조 아키텍처 (2분)
이제 RAG의 전체 데이터 흐름을 살펴보겠습니다. 크게 두 단계로 나뉘어요.

위쪽의 Knowledge Ingestion, 즉 지식 적재 단계는 오프라인에서 미리 처리됩니다. Documents, 즉 PDF나 Wiki, DOCX 같은 문서들을 수집하죠.

그 다음 Chunking 단계에서 텍스트를 토큰 단위로 쪼갭니다. 그리고 Embedding 단계에서 텍스트를 벡터로 변환하고요, 마지막으로 Vector DB에 인덱싱해서 저장하는 겁니다.

아래쪽은 Query & Generation, 즉 질의와 생성 단계예요. 이건 온라인에서 실시간으로 일어나죠.

사용자가 자연어로 질문을 하면, Top-K Search를 통해 유사도를 체크합니다. 가장 관련성 높은 상위 K개의 문서를 찾는 거예요.

그 다음 Prompting 단계에서 질문과 검색된 문맥을 함께 LLM에게 전달하고요, 최종적으로 LLM Response, 즉 출처가 포함된 답변을 받게 되는 겁니다.

이 흐름을 머릿속에 확실히 그려두시면 이후 실습이 훨씬 수월해지실 거예요.

🔷 슬라이드 8: SECTION 02 시작 (0.5분)
두 번째 섹션으로 넘어가보겠습니다. 임베딩과 벡터 검색 구조에 대해 깊이 있게 다뤄볼게요.

임베딩 모델과 유사도 검색의 핵심을 파악하고, 비정형 데이터를 수치화해서 의미적으로 검색하는 원리를 이해하게 되실 겁니다.

⚙️ 슬라이드 9: 임베딩 설계 포인트 (2.5분)
임베딩 설계에서 가장 중요한 결정 요소들을 살펴보겠습니다. 첫째, 모델 선택 전략이에요.

한국어 성능을 고려해야 합니다. MTEB 리더보드 상위권의 bge-m3나 ko-sroberta 같은 검증된 모델을 사용하는 게 좋죠. 문서 길이에 따라서도 선택이 달라져요. 512토큰까지만 처리하는 모델은 문장 단위에 적합하고, 8192토큰까지 가능한 모델은 긴 문서에 유리합니다.

제조나 기술 용어가 많은 경우라면 도메인 특화 사전학습 모델을 고려해보세요.

둘째는 차원 설계입니다. 성능과 효율성 사이의 균형이 중요해요. 고차원, 예를 들어 1024 이상은 정밀하지만 느리고, 저차원인 384는 빠르지만 정보 손실이 있을 수 있죠.

일반적인 RAG 구축에서는 768에서 1024 차원이 균형 잡힌 선택입니다. 최신 기법인 Matryoshka Embedding은 필요에 따라 앞부분 차원만 잘라 써도 성능이 유지되는 장점이 있어요.

셋째, 거리 척도와 정규화입니다. Cosine Similarity는 벡터 간의 각도, 즉 방향 유사도를 측정하는데 텍스트 검색에 가장 널리 사용돼요.

L2 Normalization을 적용하면 모든 벡터의 길이를 1로 만들어서, 내적 계산만으로도 코사인 유사도를 대체할 수 있습니다. Euclidean Distance는 벡터 간 직선 거리를 측정하는데, 정규화되지 않은 데이터에서는 크기 차이에 민감하죠.

💻 슬라이드 10: 실습 - 텍스트 임베딩 변환 (2.5분)
자, 이제 실제 코드로 들어가볼까요? Sentence-Transformers 라이브러리를 활용해서 자연어 문장을 고차원 벡터로 변환하는 과정을 실습합니다.

먼저 라이브러리를 임포트하고요, 한국어 임베딩 모델을 로드합니다. 여기서는 KoSimCSE-roberta-multitask 모델을 사용해요.

SentenceTransformer는 BERT 기반 모델을 쉽게 사용할 수 있게 해주는 Wrapper예요. 이 모델은 한국어 문장 유사도 성능이 우수하죠.

두 번째 단계에서는 변환할 텍스트 데이터를 리스트 형태로 준비합니다. "RAG는 검색 증강 생성 기술입니다", "임베딩은 텍스트를 벡터로 변환합니다" 같은 문장들이죠.

세 번째 단계가 핵심입니다. model.encode 메서드를 호출하는데요, normalize_embeddings=True 옵션을 주목하세요. 이걸 켜면 생성된 벡터의 길이를 1로 만들어줍니다.

이렇게 하면 나중에 벡터 간 유사도를 계산할 때 복잡한 공식 없이 단순 내적만으로 코사인 유사도를 구할 수 있어요. convert_to_tensor=True는 결과를 PyTorch 텐서로 받겠다는 의미죠.

결과값을 출력하면 Shape가 [3, 768]로 나옵니다. 3개의 문장이 각각 768차원의 실수 벡터로 변환되었다는 뜻이에요.

팁 하나 드리자면, 대량의 문서를 처리할 때는 device='cuda' 옵션을 추가해서 GPU를 사용하면 처리 속도가 수십 배 빨라집니다.

🗄️ 슬라이드 11: 벡터 데이터베이스 핵심 구조 (2분)
벡터 데이터베이스의 구조 설계를 이해해봅시다. 세 가지 핵심 개념이 있어요.

첫째, 컬렉션입니다. 관계형 데이터베이스의 테이블과 비슷한 개념이죠. 데이터의 성격, 예를 들어 사내 규정, 기술 문서, 프로젝트 이력에 따라 논리적으로 분리해서 관리해요. 인덱스 설정의 기본 단위가 됩니다.

둘째, 벡터와 페이로드예요. 각 문서는 고유한 ID를 가지고 있고요, Vector 필드에는 고차원 숫자 배열인 임베딩 벡터가 저장됩니다. [0.12, -0.54, 0.88...] 이런 식이죠.

그리고 Document, 즉 페이로드 필드에는 원본 텍스트 내용이 들어갑니다. "2024년도 인사 평가 가이드라인에 따르면..." 이런 실제 내용이요. 검색할 때는 벡터로 유사도를 계산하고, 결과로는 텍스트를 반환하는 거예요.

셋째, 메타데이터 활용입니다. JSON 형태로 부서, 작성일, 보안 등급 같은 정보를 저장할 수 있어요. 이걸 활용하면 "HR 부서 문서 중에서 휴가 관련 내용 검색" 같은 하이브리드 검색이 가능해지죠.

메타데이터 필터링, 즉 Pre-filtering을 하면 불필요한 벡터 연산을 줄여서 검색 속도와 정확도를 획기적으로 높일 수 있습니다.

🔬 슬라이드 12: 실습 - 유사도 계산 및 Top-K 추출 (2.5분)
벡터 공간에서 질문과 문서 사이의 거리를 계산해서 가장 가까운 문서를 찾아내는 과정을 실습해볼게요.

먼저 검색 질의, 즉 Query를 임베딩으로 변환합니다. "RAG의 핵심 원리가 무엇인가요?"라는 질문을 벡터로 만드는 거죠.

두 번째 단계에서 코사인 유사도를 계산해요. util.cos_sim 함수는 두 벡터 사이의 각도 코사인 값을 계산합니다. 값이 1에 가까울수록 두 벡터가 같은 방향, 즉 높은 유사도를 가리키는 거예요. 0은 직교, 즉 관계없음을 의미하고, -1은 반대 방향이죠.

여기서 중요한 건 행렬 연산의 효율성입니다. 반복문을 사용하지 않고 행렬 곱셈을 통해 수천, 수만 개의 문서와의 유사도를 한 번에 계산해요. 이게 GPU 병렬 처리에 최적화된 방식이죠.

세 번째 단계는 상위 K개 결과 추출입니다. torch.topk를 사용해서 계산된 유사도 점수 중 가장 높은 K개의 값과 해당 문서의 인덱스를 추출하는 거예요. 이게 바로 검색 엔진의 핵심인 '랭킹' 과정입니다.

결과를 출력하면 "Doc 0 (Score: 0.8512): RAG는 검색 증강 생성 기술입니다" 이런 식으로 나오죠. 점수가 높을수록 질문과 관련성이 높다는 의미예요.

추가 팁인데요, 임베딩 벡터가 이미 정규화되어 있다면 단순 내적의 결과는 코사인 유사도와 동일합니다. 정규화된 내적은 계산 비용이 더 저렴하죠.

🔎 슬라이드 13: 실습 - 벡터 검색 (2분)
LangChain 라이브러리와 FAISS를 활용한 전체 과정을 실습해보겠습니다. 텍스트 데이터를 벡터화해서 저장하고, 질문과 가장 유사한 문서를 검색하는 거예요.

먼저 문서와 임베딩 모델을 준비합니다. 간단한 텍스트 리스트와 OpenAI 임베딩 모델을 사용하죠.

두 번째 단계가 벡터 인덱스 생성입니다. FAISS.from_texts() 메서드는 텍스트 리스트를 받아서 지정된 임베딩 모델로 벡터화한 후, 효율적으로 검색할 수 있는 인덱스 구조로 변환해요. 이건 메모리 상에 존재하죠.

db.save_local()을 통해 생성된 인덱스를 로컬 디스크에 파일 형태로 저장할 수 있습니다. 서버 재시작 후에도 데이터를 유지할 수 있고, load_local()로 다시 불러와서 사용하면 되는 거예요.

세 번째 단계에서 인덱스를 로드하고 검색을 실행합니다. similarity_search() 메서드가 핵심인데요, 입력된 쿼리를 벡터로 변환하고, 저장된 벡터들과의 거리를 계산해서 가장 가까운 k개의 문서를 반환해요.

"RAG가 무엇인가요?"라고 물으면 "RAG는 검색 증강 생성입니다"라는 가장 유사한 문서를 찾아주는 거죠.

FAISS는 대규모 데이터의 고속 검색에 최적화된 인메모리 라이브러리고, Chroma는 메타데이터 필터링 같은 데이터베이스 기능이 강화된 솔루션입니다.

📊 슬라이드 14: 성능 최적화 전략 (2분)
검색 정확도와 재현율을 극대화하기 위한 핵심 튜닝 포인트를 알아보겠습니다. 크게 세 가지 영역이에요.

첫째, 청크 전략입니다. 최적의 크기를 설정하는 게 중요해요. 너무 작으면 문맥이 손실되고, 너무 크면 노이즈가 증가하죠. 보통 300에서 500 토큰이 권장됩니다.

의미 단위로 분할하는 것도 중요한데요, RecursiveCharacterTextSplitter 같은 도구를 활용해서 문단이나 섹션 단위로 절단하면 좋아요. 그리고 오버랩, 즉 중복 구간을 10에서 20% 정도 설정해야 문장 간 연결성이 유지됩니다.

둘째, 재랭킹 기법이에요. Cross-Encoder를 도입하면 벡터 검색 결과의 정확도를 높일 수 있습니다. 1차로 벡터 검색해서 상위 50개를 뽑고, 재랭킹해서 최종 3에서 5개를 선정하는 식이죠.

하이브리드 검색도 효과적입니다. BM25 같은 키워드 검색과 벡터 검색의 의미 기반 결과를 결합하면 상호 보완할 수 있어요.

셋째, 메타데이터 필터링입니다. Pre-filtering을 적용해서 검색 전에 부서, 날짜, 문서 유형으로 범위를 좁히면 연산량이 감소하죠. 접근 권한 제어에도 활용할 수 있어요. 사용자 권한에 맞는 문서만 검색되도록 메타데이터를 설정하는 겁니다.

🔷 슬라이드 15: SECTION 03 시작 (0.5분)
세 번째 섹션으로 넘어가겠습니다. 사내 문서 검색 자동화 구축이에요.

엔드-투-엔드 파이프라인 구현, 즉 문서 수집부터 임베딩, 검색, LLM 응답 생성까지의 전 과정을 실습하게 됩니다.

🏛️ 슬라이드 16: 시스템 아키텍처 개요 (1.5분)
사내 문서 수집부터 검색 서비스 제공까지의 엔드-투-엔드 파이프라인을 살펴보겠습니다. 크게 두 부분으로 나뉘어요.

위쪽은 Data Ingestion, 즉 데이터 적재 단계입니다. Source Connector에서 NAS, Confluence, Slack 같은 다양한 소스에서 데이터를 수집하죠.

Parser 단계에서는 PDF, DOCX, HWP 같은 파일을 읽고 필요하면 OCR도 적용합니다. Chunker에서 텍스트를 분할하고, Embedding 단계에서 밀집 벡터로 변환하는 거예요.

아래쪽은 Service & Consumption, 즉 서비스 활용 단계입니다. Vector DB에 Chroma나 FAISS를 사용해서 데이터를 저장하고요, Search API는 FastAPI나 LangChain으로 구현해요.

최종적으로 UI나 Bot, 예를 들어 Slack Bot이나 Web 인터페이스를 통해 사용자에게 서비스를 제공하는 거죠.

이 구조를 이해하면 실제 프로덕션 환경에서도 바로 적용할 수 있습니다.

📂 슬라이드 17: 실습 - 문서 로딩 (2분)
다양한 포맷의 비정형 데이터를 RAG 파이프라인에서 처리 가능한 표준화된 Document 객체로 변환하는 과정이에요.

먼저 LangChain의 document_loaders를 임포트합니다. PyPDFLoader, Docx2txtLoader, DirectoryLoader 같은 전용 로더들이죠.

단일 파일 로더를 보시면, PyPDFLoader는 페이지 단위로 PDF를 읽어옵니다. Docx2txtLoader는 워드 문서의 텍스트와 일부 구조를 추출하고요. LangChain은 CSV, HTML, Markdown 등 거의 모든 포맷의 로더를 제공해요.

두 번째는 폴더 내 모든 파일을 재귀적으로 로딩하는 방법입니다. DirectoryLoader를 사용하는 거죠. 수백 개의 문서를 일일이 지정할 수 없으니까요.

glob="**/*.txt" 패턴을 통해 하위 폴더까지 재귀적으로 탐색해서 특정 확장자 파일만 일괄 로딩할 수 있습니다. show_progress=True 옵션을 켜면 진행 상황도 볼 수 있어요.

로딩 결과를 확인하면 page_content에는 본문이, metadata에는 출처나 페이지 번호 같은 정보가 담겨 있습니다. 이 메타데이터는 나중에 답변의 근거를 사용자에게 제시할 때 필수적이에요.

고급 팁인데요, 표나 이미지가 포함된 복잡한 문서는 UnstructuredLoader를 사용하거나 OCR 기능이 포함된 파서를 연동해야 데이터 유실을 최소화할 수 있습니다.

✂️ 슬라이드 18: 청크 분할 전략 (2분)
문맥을 유지하면서 검색 정확도를 높이기 위한 RecursiveCharacterTextSplitter 설정을 알아보겠습니다.

먼저 분할기를 초기화하는데요, 한국어 문서 구조를 고려해야 해요. chunk_size=500은 검색에 적절한 토큰 길이입니다. 너무 짧으면 문맥이 손실되죠.

chunk_overlap=50은 문장 간 연결성을 위한 중복 구간이에요. 이전 청크의 마지막 부분을 현재 청크의 앞부분에 포함시켜서 검색 시 문맥이 잘리는 걸 방지하는 겁니다.

separators 파라미터를 주목하세요. "\n\n", "\n", " ", "" 순서로 설정되어 있죠. 이게 Recursive 분할의 핵심입니다. 단락, 줄, 단어 순으로 분할을 시도하기 때문에 의미적 완결성을 더 잘 유지해요.

두 번째 단계에서 실제 문서 파일을 로드하고 분할합니다. text_splitter.create_documents() 메서드를 호출하면 raw_text가 여러 개의 청크로 나뉘어지죠.

결과를 확인하면 총 생성된 청크 수와 각 청크의 내용을 볼 수 있어요. Chunk Size 설정 팁을 드리자면, 임베딩 모델의 최대 입력 토큰 수를 넘지 않아야 합니다. 보통 300에서 500자 정도가 단일 질문에 대한 답변 근거로 적절해요.

🧬 슬라이드 19: 실습 - 임베딩 생성 (2.5분)
사내 문서를 벡터화하는 핵심 단계입니다. 고성능 다국어 모델인 BGE-M3를 활용해볼게요.

먼저 라이브러리를 임포트하고 모델을 로드합니다. BAAI/bge-m3 모델은 최신 RAG 구축에 널리 쓰이는 SOTA급 모델이에요. 한국어를 포함한 다국어를 지원하고, 최대 8192 토큰까지 처리 가능해서 긴 문서 이해력이 뛰어나죠.

device='cuda' 설정을 보세요. GPU를 사용할 수 있으면 자동으로 CUDA를 활용합니다. 문서 양이 많을 경우 CPU만으로는 처리가 매우 느려요. GPU VRAM을 활용하고 batch_size를 조절해서 메모리 효율성을 극대화해야 하죠.

두 번째 단계에서 문서 청크 데이터를 준비합니다. "출장비 규정: KTX 특실은 임원급 이상만 가능합니다", "복지 포인트: 매년 1월 1일 100만원이 지급됩니다" 같은 실제 사내 문서 내용들이에요.

세 번째 단계가 임베딩 벡터 생성입니다. model.encode() 메서드를 호출하는데요, batch_size=32로 설정해서 GPU 배치 처리를 최적화해요. normalize_embeddings=True는 앞서 배운 대로 코사인 유사도 계산을 간단하게 만들어주죠.

show_progress_bar=True 옵션을 켜면 진행 상황을 볼 수 있어요. BGE-M3 모델은 1024차원의 밀집 벡터를 생성합니다. 이후 벡터 데이터베이스 컬렉션을 생성할 때 이 차원 수를 정확히 맞춰줘야 해요.

실무 팁인데요, 문서가 수정되지 않았다면 매번 임베딩을 다시 생성할 필요가 없습니다. 파일의 해시값을 키로 사용해서 로컬 디스크에 임베딩 결과를 캐싱해두면 재실행 속도가 획기적으로 빨라져요.

💾 슬라이드 20: 실습 - 벡터 DB 저장 (2분)
생성된 임베딩 벡터와 원본 텍스트를 검색 가능한 형태의 데이터베이스로 저장하고, 디스크에 영구 보관하는 방법을 알아보겠습니다.

먼저 ChromaDB에 저장하는 방법이에요. Chroma.from_documents() 메서드를 사용하는데요, persist_directory를 지정하면 메모리가 아닌 로컬 디스크에 데이터가 저장됩니다.

SQLite 기반으로 작동하고요, 서버를 재시작해도 인덱스가 유지되기 때문에 사내 지식 베이스 구축에 필수적이죠. collection_name으로 데이터셋을 구분할 수 있어요. 예를 들어 "company_docs", "tech_manuals" 이런 식으로요.

두 번째는 FAISS 인덱스 생성 및 저장입니다. FAISS는 기본적으로 인메모리 방식인데, save_local() 메서드를 통해 인덱스 파일과 메타데이터 파일로 직렬화해서 저장할 수 있어요.

검색 속도가 매우 빠르지만, 메타데이터 관리는 Chroma보다 다소 복잡할 수 있습니다. load_local()로 저장된 인덱스를 다시 불러와서 사용하면 되죠.

메타데이터 활용 전략도 중요합니다. 부서, 작성 날짜, 문서 유형, 보안 등급 같은 정보를 메타데이터로 저장하면 필터링 검색이 가능해요. "2024년 이후 작성된 HR 부서 문서만 검색" 이런 식으로 세밀하게 제어할 수 있죠.

🤖 슬라이드 21-26 요약 (10분)
이 구간에서는 LLM 통합, 컨텍스트 재랭킹, 응답 생성과 인용, 평가 지표, 운영 최적화 전략을 다룹니다. Ollama 연동을 통한 RAG 체인 구성, Cross-Encoder를 활용한 2차 정밀 정렬, 답변 하단에 출처를 자동으로 표시하는 방법을 실습하게 돼요.

평가 지표로는 Retrieval 측면에서 Recall@5, MRR을, Generation 측면에서 Answer Relevance, Faithfulness, Context Precision을 배웁니다. 운영 최적화에서는 증분 인덱싱, PII 마스킹, RBAC 연동, 비용 모니터링 같은 실무 전략을 익히게 되죠.

🚀 슬라이드 27-30: 실습 프로젝트 (8분)
네 번째 섹션에서는 미니 RAG 앱을 직접 구현합니다. 프로젝트 폴더 구조를 보시면 data, src, chroma_db, app, config 디렉토리로 표준화되어 있어요.

전체 파이프라인 구현에서는 FastAPI 기반 엔드포인트를 만들고, RAG 로직을 통합합니다. /ask 엔드포인트로 질문을 받아서 벡터 검색, 재랭킹, LLM 생성까지 원스톱으로 처리하는 거죠.

Case Study를 보시면 실제 사내 규정과 공지 통합 검색 시스템을 구축한 사례가 나와요. 질의응답 정확도가 92%에 달하고, 정보 탐색 시간이 5분에서 30초로 단축된 성과를 얻었습니다.

🔧 슬라이드 31: 트러블슈팅 가이드 (2분)
실무에서 자주 마주치는 문제들과 해결책을 알아보겠습니다. 데이터 파싱 오류는 복잡한 레이아웃의 PDF에서 많이 발생해요. 이럴 땐 UnstructuredLoader나 전문 파서를 사용하면 되죠.

검색 품질이 저하될 때는 청크 크기를 조정하거나 재랭킹 모델을 추가해보세요. 환각 현상이 나타나면 프롬프트에 "문서에 근거해서만 답변하라"는 지시를 명확히 해야 해요.

보안 문제는 메타데이터 기반 접근 제어와 PII 마스킹으로 해결할 수 있습니다.

📝 슬라이드 32-34: 마무리 (3분)
다섯 번째 섹션에서 교육 과정을 마무리합니다. 핵심 요약을 보시면, RAG 아키텍처는 인덱싱, 검색, 생성의 3단계로 구성되어 있고, 임베딩 모델 선택과 청크 전략이 성능을 좌우한다는 걸 배웠어요.

실습에서는 LangChain, ChromaDB, FAISS를 활용한 엔드-투-엔드 파이프라인을 구축했고, 운영에서는 증분 인덱싱, 모니터링, 보안이 중요하다는 걸 익혔죠.

참고자료로는 LangChain 공식 문서, AWS, NVIDIA 블로그 같은 리소스를 활용하시면 됩니다. 이제 질의응답 시간을 갖도록 하겠습니다. 궁금하신 점 있으시면 편하게 질문해주세요.

오늘 교육에 참여해주셔서 감사합니다. 배우신 내용을 실무에 바로 적용해보시길 바랍니다!
