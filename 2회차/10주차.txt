📝 RAG 작동원리 및 실습 - 50분 강의 스크립트
슬라이드 1: 표지 (30초)
안녕하세요, 국립창원대학교 PRU 재직자 여러분. 오늘은 RAG 기술의 작동원리부터 실제 사내 문서 검색 자동화 시스템 구축까지 다루는 실습 중심 교육을 진행하겠습니다.

총 50분간 임베딩, 벡터 검색, 그리고 Python 기반 RAG 파이프라인 구축을 함께 경험하실 거예요. 실무에 바로 적용 가능한 핵심 기술들을 차근차근 배워보시죠.

슬라이드 2: 학습 목표와 산출물 (50초)
먼저 오늘 교육의 목표를 명확히 하고 시작하겠습니다. 우리가 달성할 세 가지 핵심 목표는 RAG 개념 이해, 임베딩과 벡터 검색 실습, 그리고 사내 문서 검색 자동화 시스템 구축이에요.

특히 중요한 건 실습 산출물인데요. 여러분은 오늘 Jupyter 노트북 기반의 실습 코드, 샘플 데이터셋, 그리고 실제 작동하는 미니 RAG 애플리케이션 스크립트를 직접 만들어보실 겁니다. 이 결과물들을 회사로 가져가셔서 바로 프로젝트에 적용하실 수 있도록 구성했어요.

슬라이드 3: 목차 (40초)
전체 교육 과정은 6개 섹션으로 구성되어 있습니다. RAG 개념부터 시작해서 임베딩과 벡터 검색 구조를 이해하고, 사내 문서 검색 자동화 파이프라인 구축까지 순차적으로 진행할 거예요.

그다음 실습 프로젝트를 통해 미니 RAG 앱을 직접 만들어보고, 평가와 최적화 방법을 배운 뒤 결론으로 마무리하겠습니다. 각 섹션마다 이론과 실습이 균형있게 배치되어 있으니 편안하게 따라오시면 됩니다.

슬라이드 4: 섹션 1 - RAG 개념과 필요성 (20초)
자, 이제 본격적으로 시작해볼까요? 첫 번째 섹션에서는 RAG가 왜 필요한지, 그리고 단순 LLM만으로는 왜 부족한지를 살펴보겠습니다.

슬라이드 5: RAG란 무엇인가? (80초)
RAG는 Retrieval-Augmented Generation의 약자입니다. 쉽게 말하면 '검색으로 보강된 생성'이라는 뜻이죠.

여기서 핵심은 세 가지 구성요소예요. 첫째, 인덱싱 단계에서 문서를 임베딩으로 변환해 벡터 데이터베이스에 저장합니다. 둘째, 검색 단계에서 사용자 질문과 유사한 문서를 찾아내죠.

셋째, 생성 단계에서 검색된 문서를 참고해 LLM이 답변을 만들어내는 겁니다. 이렇게 하면 세 가지 효과를 얻을 수 있어요.

최신 정보를 반영할 수 있고, 답변에 근거를 명시할 수 있으며, LLM의 환각 현상을 크게 줄일 수 있습니다. 기업에서 사내 문서 기반 Q&A 시스템을 만들 때 RAG가 필수인 이유죠.

슬라이드 6: 전통적 검색 vs RAG (90초)
전통적인 키워드 검색과 RAG 방식이 어떻게 다른지 비교해볼게요. 왼쪽 키워드 검색은 BM25나 TF-IDF 같은 방식을 사용합니다.

예를 들어 "자동차"라고 검색하면 정확히 그 단어가 들어간 문서만 찾아내죠. 하지만 "차량", "승용차" 같은 동의어가 포함된 문서는 놓치게 됩니다.

반면 오른쪽 RAG 방식은 의미 기반 검색을 해요. "자동차"를 벡터로 변환하면 "차량", "승용차"와 의미적으로 가까운 벡터가 생성되기 때문에 동의어 문서도 자동으로 찾아냅니다.

더 나아가 "친환경 이동수단"처럼 개념적으로 연관된 문서까지 검색할 수 있죠. 이게 바로 임베딩 기반 검색의 강력함입니다.

슬라이드 7: RAG 참조 아키텍처 (90초)
RAG 시스템의 전체 데이터 흐름을 파이프라인으로 보시죠. 맨 왼쪽 데이터 수집부터 시작합니다.

사내 PDF, Word, 공지사항, 위키 등 다양한 소스에서 문서를 수집하고요. 그다음 파싱 단계에서 텍스트를 추출하고 정제합니다.

청킹 단계에서는 긴 문서를 적절한 크기로 나누죠. 보통 300~500자 단위로 나누는데, 문맥이 끊기지 않도록 오버랩을 줍니다.

임베딩 단계에서 각 청크를 벡터로 변환하고, 벡터 데이터베이스에 저장해요. 사용자가 질문하면 질문도 벡터로 변환해서 유사한 청크를 검색하고, 그 결과를 LLM에게 전달해 최종 답변을 생성합니다.

마지막으로 출처까지 함께 표시하는 거죠. 이 7단계가 RAG의 기본 골격입니다.

슬라이드 8: 섹션 2 - 임베딩·벡터 검색 구조 (20초)
다음 섹션으로 넘어가볼게요. 임베딩과 벡터 검색이라는 RAG의 핵심 기술을 깊이 있게 다룰 차례입니다.

슬라이드 9: 임베딩 개념과 설계 포인트 (80초)
임베딩은 텍스트를 고차원 벡터로 변환하는 기술이에요. 쉽게 말하면 단어나 문장을 숫자 배열로 바꾸는 거죠.

설계할 때 중요한 포인트가 몇 가지 있습니다. 첫째, 모델 선택인데요, 한국어 문서라면 bge-m3이나 ko-sroberta 같은 한국어 최적화 모델을 사용하는 게 좋아요.

둘째, 차원 수는 보통 768차원이나 1024차원을 사용하며, 높을수록 표현력이 좋지만 저장 공간과 검색 속도에 영향을 줍니다. 셋째, 거리 척도는 코사인 유사도나 L2 거리를 쓰는데, 코사인이 방향만 보기 때문에 텍스트에는 더 적합하죠.

마지막으로 도메인 적합성이 중요해요. 법률 문서와 기술 문서는 특성이 다르니, 가능하다면 도메인별로 파인튜닝된 모델을 쓰는 게 좋습니다.

슬라이드 10: 실습 - 텍스트를 임베딩으로 변환 (100초)
이제 첫 번째 실습을 해보겠습니다. Sentence-Transformers 라이브러리로 임베딩을 생성하는 코드예요.

왼쪽 코드를 보시면 먼저 SentenceTransformer 모델을 로드합니다. 여기서는 paraphrase-multilingual-MiniLM 모델을 사용했어요, 한국어도 지원하는 경량 모델이죠.

encode 메서드로 문장 리스트를 한 번에 임베딩으로 변환할 수 있습니다. normalize_embeddings=True 옵션은 벡터를 정규화해서 코사인 유사도 계산을 빠르게 만들어주죠.

오른쪽 실행 결과를 보면 각 문장이 384차원 벡터로 변환되었어요. 이 숫자 배열이 문장의 의미를 수치적으로 표현한 겁니다.

배치 처리를 사용하면 대량의 문서도 효율적으로 처리할 수 있어요. GPU가 있다면 device='cuda'로 설정해서 10배 이상 빠르게 처리할 수 있습니다.

슬라이드 11: 벡터 데이터베이스 구조 (70초)
생성한 임베딩을 저장하는 벡터 데이터베이스 구조를 살펴볼게요. Chroma나 FAISS 같은 벡터 DB를 사용합니다.

컬렉션 안에는 네 가지 핵심 요소가 있어요. ID는 각 문서의 고유 식별자, Vector는 임베딩 벡터 자체, Content는 원본 텍스트, Metadata는 부서명, 작성일, 보안등급 같은 필터링용 정보죠.

예를 들어 "인사팀 문서만 검색"하고 싶을 때 metadata 필터를 사용합니다. 이렇게 설계하면 검색 성능과 정확도를 동시에 높일 수 있어요.

실무에서는 부서별, 문서 타입별로 메타데이터를 풍부하게 설계하는 게 중요합니다. 나중에 복잡한 검색 요구사항에도 유연하게 대응할 수 있죠.

슬라이드 12: 실습 - 유사도 계산 (100초)
벡터 간 유사도를 계산하는 실습입니다. 코사인 유사도가 가장 많이 쓰이는 방법이에요.

왼쪽 코드를 보면 util.cos_sim 함수로 쿼리 임베딩과 문서 임베딩들 간의 유사도를 계산합니다. 결과는 -1에서 1 사이 값인데, 1에 가까울수록 의미가 비슷하다는 뜻이죠.

Top-K 추출 부분을 보세요. argsort로 유사도 순위를 매기고 상위 3개를 선택합니다. 실제로는 K를 5~10 정도로 설정하는 경우가 많아요.

오른쪽 결과를 보면 "머신러닝 알고리즘"과 "딥러닝 기술" 문서가 높은 유사도를 보이죠. 질문과 의미적으로 가까운 문서들이 잘 검색되었습니다.

점수 스케일링을 해서 0~100 범위로 변환하면 사용자에게 보여주기 좋아요. 실무에서는 임계값을 설정해서 유사도가 낮은 문서는 걸러내기도 합니다.

슬라이드 13: 실습 - 벡터 검색 (FAISS/Chroma) (110초)
실제 벡터 데이터베이스를 사용하는 실습으로 넘어갑니다. LangChain으로 Chroma를 활용하는 코드예요.

먼저 Chroma 인스턴스를 생성할 때 persist_directory를 지정합니다. 이렇게 하면 벡터 데이터가 디스크에 저장돼서 나중에 다시 불러올 수 있어요.

from_documents 메서드로 문서 리스트와 임베딩 함수를 한 번에 전달하면 자동으로 벡터화해서 저장합니다. 엄청 편리하죠.

검색은 similarity_search 메서드로 합니다. k=3이면 상위 3개 문서를 반환해요. 오른쪽 결과를 보면 검색된 문서의 내용과 메타데이터가 함께 나옵니다.

FAISS를 사용할 경우 save_local과 load_local로 인덱스 파일을 저장하고 불러올 수 있어요. FAISS는 대용량 데이터에 특히 빠르지만, Chroma는 메타데이터 필터링이 더 강력합니다.

프로젝트 특성에 맞춰 선택하시면 돼요.

슬라이드 14: 성능 최적화 전략 (70초)
벡터 검색 성능을 높이는 전략들을 정리해드릴게요. 첫째, 청크 크기와 오버랩 조정이 중요합니다.

청크가 너무 작으면 문맥이 부족하고, 너무 크면 노이즈가 많아져요. 보통 300~500자에 50자 오버랩이 적당합니다.

둘째, 재랭킹 기법을 사용하세요. 1차 검색으로 Top-K 후보를 뽑고, Cross-Encoder로 재정렬하면 정확도가 크게 올라갑니다. BGE reranker 같은 모델이 좋아요.

셋째, 메타데이터 필터링을 활용하면 검색 범위를 줄여 속도를 높일 수 있죠. 넷째, 하이브리드 검색도 고려해보세요, 키워드 검색과 벡터 검색을 결합하면 각각의 장점을 살릴 수 있습니다.

슬라이드 15: 섹션 3 - 사내 문서 검색 자동화 (20초)
자, 이제 가장 실전적인 파트입니다. 사내 문서 검색 자동화 시스템을 엔드투엔드로 구축해보겠습니다.

슬라이드 16: 시스템 아키텍처 개요 (80초)
사내 문서 검색 자동화의 전체 아키텍처를 보시죠. 맨 왼쪽 소스 커넥터부터 시작합니다.

SharePoint, Google Drive, 로컬 파일 시스템에서 문서를 자동으로 수집하고요. 파서 단계에서 PDF, Word, HTML 등을 텍스트로 변환합니다.

청크러가 문서를 적절한 크기로 나누고, 임베딩 모듈이 벡터로 변환하죠. 벡터 DB에 저장된 데이터는 API 레이어를 통해 서비스됩니다.

마지막으로 웹 UI나 슬랙 봇 같은 인터페이스에서 사용자가 질문하면 전체 파이프라인이 작동하는 구조예요. 각 단계가 모듈화되어 있어서 유지보수가 쉽습니다.

실제 기업 환경에서는 스케줄러를 추가해서 매일 밤 신규 문서를 자동으로 인덱싱하도록 구성하죠.

슬라이드 17: 실습 - 문서 로딩 (100초)
다양한 포맷의 문서를 로딩하는 실습입니다. LangChain의 Document Loader들을 사용할 거예요.

코드를 보면 PyPDFLoader로 PDF를 로드하고, Docx2txtLoader로 Word 파일을 처리합니다. UnstructuredMarkdownLoader는 마크다운 파일용이죠.

DirectoryLoader를 사용하면 폴더를 재귀적으로 탐색해서 모든 문서를 한 번에 로드할 수 있어요. glob 패턴으로 파일 확장자를 필터링할 수도 있습니다.

각 로더의 load 메서드는 Document 객체 리스트를 반환하는데, page_content에 텍스트가, metadata에 파일 경로와 페이지 번호 같은 정보가 들어있죠.

실무 팁을 드리자면, 한글 인코딩 문제가 있을 때는 encoding='utf-8'을 명시하고, PDF의 경우 OCR이 필요하면 pdf2image와 pytesseract를 함께 사용하세요.

슬라이드 18: 청크 분할 전략 (90초)
로드한 문서를 청크로 나누는 전략을 코드로 보겠습니다. RecursiveCharacterTextSplitter가 가장 많이 쓰여요.

chunk_size를 400으로 설정했습니다. 이게 각 청크의 최대 문자 수예요. chunk_overlap은 50인데, 이건 청크 간 겹치는 부분입니다.

왜 오버랩이 필요하냐면, 문장이 청크 경계에서 잘릴 때 문맥이 손실되는 걸 방지하기 위해서죠. separators 리스트를 보면 문단, 문장, 단어 순으로 구분자를 시도합니다.

코드 하단의 split_documents 메서드가 실제 분할을 수행해요. 반환된 chunks 리스트는 각각 독립적으로 임베딩되고 벡터 DB에 저장됩니다.

실무에서는 문서 타입에 따라 청크 크기를 다르게 설정하기도 해요. 예를 들어 기술 문서는 500자, 공지사항은 300자 이런 식으로요.

슬라이드 19: 실습 - 임베딩 생성 (100초)
이제 청크들을 임베딩으로 변환하는 실습입니다. 한국어에 최적화된 bge-m3 모델을 사용할게요.

코드 상단을 보면 HuggingFaceEmbeddings 클래스로 모델을 로드합니다. model_name에 "BAAI/bge-m3-unsupervised"를 지정했죠.

model_kwargs에서 device를 'cuda'로 설정하면 GPU를 사용해서 훨씬 빠릅니다. 만약 GPU가 없으면 'cpu'로 바꾸시면 돼요.

encode_kwargs의 normalize_embeddings는 벡터를 정규화하는 옵션입니다. 이렇게 하면 코사인 유사도를 내적으로 계산할 수 있어서 속도가 빨라져요.

embed_documents 메서드로 청크 리스트를 배치 처리합니다. 수천 개 문서도 몇 분 안에 처리 가능하죠.

캐싱을 구현하면 같은 문서를 중복으로 임베딩하지 않아서 더 효율적입니다. SQLite나 Redis로 간단히 만들 수 있어요.

슬라이드 20: 실습 - 벡터 DB 저장 (100초)
생성한 임베딩을 Chroma에 저장하는 실습입니다. 영구 저장 방식을 사용할 거예요.

from_documents 메서드를 보세요. documents는 청크 리스트, embedding은 임베딩 함수, persist_directory는 저장 경로입니다.

이렇게 하면 ./chroma_db 폴더에 벡터 데이터가 파일로 저장돼요. 프로그램을 종료하고 다시 시작해도 데이터가 유지됩니다.

FAISS를 사용할 경우 코드가 약간 달라요. FAISS.from_documents로 인덱스를 만들고, save_local로 파일로 저장하죠.

메타데이터 스키마를 보면 source, department, security_level 같은 필드가 있습니다. 이걸로 나중에 필터링할 수 있어요.

실무에서는 증분 업데이트가 중요합니다. 신규 문서만 add_documents로 추가하고, 삭제된 문서는 delete로 제거하는 로직을 구현하세요.

슬라이드 21: 실습 - 검색 쿼리 처리 (110초)
사용자 질문을 처리하는 검색 로직을 구현해봅시다. 먼저 질문을 임베딩으로 변환합니다.

embed_query 메서드가 질문 텍스트를 벡터로 바꿔줘요. 이 쿼리 벡터로 similarity_search_with_score를 호출하면 유사한 문서들과 점수가 반환됩니다.

k=5는 상위 5개 문서를 가져오라는 뜻이죠. filter 파라미터를 보세요, 메타데이터로 부서를 필터링하고 있습니다.

예를 들어 "인사팀 문서만 검색"하고 싶을 때 filter={"department": "HR"}을 추가하면 돼요. 보안등급 필터링도 같은 방식으로 구현할 수 있습니다.

검색 결과를 포맷팅하는 부분을 보면 문서 내용, 출처, 유사도 점수를 함께 출력하고 있어요. 실제 서비스에서는 점수가 낮은 문서는 제외하는 임계값을 설정하는 게 좋습니다.

보통 0.7 이상만 사용하는 식으로요.

슬라이드 22: 컨텍스트 재랭킹 (100초)
검색 정확도를 한 단계 더 높이는 재랭킹 기법입니다. Cross-Encoder 모델을 사용할 거예요.

1차 검색으로 Top-K 후보를 많이 뽑습니다, 예를 들어 20개요. 그다음 Cross-Encoder가 쿼리와 각 문서를 직접 비교해서 정밀한 점수를 매깁니다.

코드를 보면 CrossEncoder 모델로 BAAI/bge-reranker-large를 로드했어요. predict 메서드에 쿼리-문서 쌍 리스트를 전달하면 재랭킹 점수가 나옵니다.

그 점수로 다시 정렬해서 상위 5개만 최종 선택하죠. 이렇게 하면 초기 벡터 검색의 리콜(Recall)을 유지하면서도 프리시전(Precision)을 크게 높일 수 있어요.

득점 임계값을 설정하는 것도 중요합니다. 예를 들어 0.5 이하 문서는 아예 버리는 식으로요. 실무에서는 재랭킹 모델을 도메인 데이터로 파인튜닝하면 더 좋은 결과를 얻습니다.

슬라이드 23: 실습 - LLM 통합 (Ollama & Prompt) (110초)
검색된 문서를 LLM과 연결하는 실습입니다. LangChain으로 Ollama를 사용할 거예요.

먼저 ChatOllama 인스턴스를 생성합니다. model은 로컬에 설치된 llama2나 mistral 같은 모델을 지정하면 돼요.

프롬프트 템플릿을 보세요. "당신은 사내 문서 전문가입니다"라는 시스템 지시어로 시작하죠. context 변수에 검색된 문서들이 들어가고, question에 사용자 질문이 들어갑니다.

지시어 설계가 중요한데, "반드시 제공된 문서 내용만 참고하세요"라고 명시해야 LLM이 외부 지식을 사용하지 않습니다. 또 "출처를 명시하세요"라고 요청하면 답변에 문서 출처가 포함돼요.

체인을 구성하는 부분을 보면 프롬프트와 LLM, 그리고 출력 파서를 파이프로 연결했습니다. 이제 invoke 메서드로 질문하면 전체 RAG 파이프라인이 실행되죠.

클라우드 LLM을 쓸 경우 ChatOpenAI나 ChatAnthropic으로 바꾸시면 됩니다.

슬라이드 24: 실습 - 응답 생성과 인용 (100초)
최종 답변을 생성하고 출처를 인용하는 로직입니다. format_docs_with_source 함수를 보세요.

각 문서의 내용과 메타데이터에서 파일명, 페이지를 추출해서 "[출처: xxx.pdf, p.5]" 형식으로 만듭니다. 이렇게 포맷된 컨텍스트를 LLM에게 전달하면 답변에 출처가 자연스럽게 포함돼요.

하이라이트 기능도 구현할 수 있습니다. 답변에서 문서 원문을 인용한 부분을 찾아 HTML로 강조 표시하는 거죠.

토큰 길이 제어도 중요해요. 컨텍스트가 너무 길면 LLM의 입력 한계를 초과할 수 있으니, tiktoken 라이브러리로 토큰을 세고 필요하면 잘라내야 합니다.

보통 최대 컨텍스트 길이를 3000토큰 정도로 제한하고, 그 안에서 가장 관련성 높은 문서들만 포함시키죠. 답변 품질을 유지하면서도 비용을 절감하는 방법입니다.

슬라이드 25: 평가 지표와 A/B 테스트 (90초)
RAG 시스템의 성능을 어떻게 측정할까요? 두 가지 측면으로 나눠서 평가합니다.

첫째, 검색 정확도예요. Recall@K는 정답 문서가 상위 K개 안에 포함되었는지 봅니다. MRR은 정답 문서가 몇 번째에 등장했는지 순위를 평가하죠.

둘째, 답변 품질입니다. 정확성은 답변이 사실적으로 맞는지, 근거성은 검색된 문서에 기반했는지, 유창성은 문장이 자연스러운지를 평가해요.

차트를 보시면 파란색 바가 검색 지표, 주황색이 답변 지표입니다. A/B 테스트를 할 때는 베이스라인 시스템과 개선 시스템을 실제 사용자 그룹에 나눠서 배포하고요.

클릭률, 만족도, 재검색률 같은 지표를 수집해서 비교합니다. 통계적 유의성을 확보하려면 최소 200~300건의 쿼리 샘플이 필요해요.

슬라이드 26: 운영 최적화 팁 (70초)
실제 운영 환경에서 필요한 최적화 전략들을 정리해드릴게요. 첫째, 증분 인덱싱 자동화입니다.

매일 밤 신규 문서만 크롤링해서 벡터 DB에 추가하는 스케줄러를 구성하세요. Apache Airflow나 Cron으로 구현할 수 있어요.

둘째, 접근제어 시스템을 통합해야 합니다. 사용자의 부서와 권한에 따라 검색 결과를 필터링하죠. 메타데이터에 security_level을 저장하고 쿼리 시 체크하면 됩니다.

셋째, 모니터링이 중요해요. 검색 지연 시간, 오류율, 임베딩 드리프트를 대시보드로 추적하세요. 넷째, 비용 관리인데, 임베딩 모델을 로컬에서 돌리면 API 비용을 아낄 수 있습니다.

클라우드 LLM 사용량도 캐싱으로 줄일 수 있죠.

슬라이드 27: 섹션 4 - 실습 프로젝트 (20초)
자, 이제 배운 내용을 종합해서 실제 미니 RAG 앱을 만들어볼 시간입니다. Python으로 직접 구현해보죠.

슬라이드 28: 프로젝트 폴더 구조 (80초)
표준화된 RAG 프로젝트 폴더 구조를 보시겠습니다. 왼쪽 탐색기 뷰를 보면 최상위에 프로젝트 루트가 있어요.

data 폴더에는 원본 문서들, raw 서브폴더에 PDF와 Word 파일을 넣습니다. processed에는 청크 결과를 저장하죠.

loaders 폴더에 문서 로더 스크립트들, splitters에 청킹 로직, embeddings에 임베딩 생성 코드를 둡니다. vectordb 폴더는 Chroma나 FAISS 인덱스 저장소예요.

api 폴더에 FastAPI 기반의 REST API 서버, ui에 Streamlit 웹 인터페이스를 구성합니다. notebooks는 실험용 Jupyter 노트북, configs는 설정 파일들이죠.

이렇게 모듈화하면 각 컴포넌트를 독립적으로 개발하고 테스트할 수 있어요. 팀 협업에도 유리하고, 나중에 확장하기도 쉽습니다.

슬라이드 29: 전체 파이프라인 구현 (120초)
전체 파이프라인을 통합한 핵심 코드를 살펴보겠습니다. 네 개의 주요 스크립트로 구성돼 있어요.

첫 번째, ingest.py는 문서를 로드하고 청크로 나누는 역할입니다. DirectoryLoader로 data/raw 폴더를 읽고, RecursiveCharacterTextSplitter로 분할하죠.

두 번째, index.py는 청크를 임베딩하고 벡터 DB에 저장합니다. HuggingFaceEmbeddings로 벡터를 생성하고 Chroma에 persist하는 코드예요.

세 번째, query.py는 검색 로직입니다. 질문을 임베딩으로 변환하고 similarity_search를 실행한 뒤, Cross-Encoder로 재랭킹까지 수행합니다.

네 번째, app.py는 FastAPI 서버입니다. POST /query 엔드포인트로 질문을 받아서 query.py의 검색 함수를 호출하고, LLM으로 답변을 생성해서 JSON으로 반환하죠.

이 네 스크립트만 있으면 기본적인 RAG 시스템이 완성됩니다. 실행 순서는 ingest → index → app 서버 실행 순이에요.

슬라이드 30: 케이스 스터디 - 사내 규정/공지 통합 검색 (120초)
실제 기업 적용 사례를 보시죠. 어느 중견기업의 사내 규정과 공지사항 검색 시스템입니다.

배경을 보면 인사규정, 회계규정, 보안정책, 공지사항이 SharePoint, Confluence, 이메일 등에 분산되어 있었어요. 직원들이 필요한 정보를 찾는 데 평균 20분이 걸렸죠.

문제점은 키워드 검색의 한계와 문서 접근 권한 관리 부재였습니다. 솔루션으로 RAG 기반 통합 검색을 구축했어요.

SharePoint API로 자동 수집, 부서별 접근제어, 의미 기반 검색을 구현했죠. 결과를 보면 검색 시간이 20분에서 30초로 단축되었습니다, 97.5% 감소예요.

정확도는 82%에서 94%로 향상되었고, 직원 만족도가 3.2에서 4.6으로 올랐습니다. 비용 효과는 연간 3억 원의 업무 시간 절감이었죠.

핵심 성공 요인은 도메인 특화 임베딩 모델 파인튜닝과 지속적인 사용자 피드백 반영이었습니다.

슬라이드 31: 트러블슈팅 가이드 (70초)
현장에서 자주 발생하는 문제들과 해결책을 정리했어요. 첫 번째, 데이터 파싱 오류입니다.

스캔된 PDF는 텍스트 추출이 안 되니 Tesseract OCR을 사용하세요. 한글 인코딩 깨짐은 chardet 라이브러리로 자동 감지해서 해결합니다.

두 번째, 중복 문서 문제예요. 같은 내용이 다른 파일명으로 여러 개 있으면 해시값으로 중복을 제거하세요.

세 번째, 과도한 청크입니다. 청크가 너무 많으면 검색 속도가 느려지니, 불필요한 부분을 전처리 단계에서 제거하세요. 헤더, 푸터, 목차 같은 부분들이죠.

네 번째, 임베딩 모델 불일치예요. 인덱싱 때와 검색 때 다른 모델을 쓰면 결과가 이상하니 꼭 같은 모델을 사용하세요. 다섯 번째, 권한 오류는 메타데이터에 security_level을 꼭 명시하고 쿼리 시 검증 로직을 넣으면 됩니다.

슬라이드 32: 섹션 5 - 마무리 (20초)
여기까지가 실습 파트였고요. 이제 오늘 배운 내용을 정리하고 마무리하겠습니다.

슬라이드 33: 핵심 요약 (70초)
오늘 교육의 핵심 포인트를 다시 짚어보겠습니다. RAG 아키텍처는 인덱싱, 검색, 생성의 3단계 구조예요.

임베딩은 텍스트를 벡터로 변환하고, 벡터 DB는 의미 기반 검색을 가능하게 하며, LLM은 검색된 문서를 바탕으로 답변을 생성하죠. 실습 포인트는 청크 크기 조정, 재랭킹 적용, 메타데이터 필터링이었습니다.

운영 팁으로는 증분 인덱싱 자동화, 접근제어 통합, 모니터링 대시보드 구축을 강조했어요. 가장 중요한 건 사용자 피드백을 지속적으로 수집해서 시스템을 개선하는 거예요.

RAG는 완성형이 아니라 계속 진화하는 시스템입니다. 여러분의 도메인 데이터로 파인튜닝하고, A/B 테스트로 검증하면서 발전시켜 나가세요.

슬라이드 34: 참고자료 및 Q&A (70초)
추가 학습을 위한 리소스들을 안내드릴게요. 핵심 라이브러리 문서로는 LangChain 공식 문서, FAISS GitHub, Chroma 튜토리얼을 참고하세요.

Sentence-Transformers 문서에는 다양한 한국어 임베딩 모델이 소개되어 있습니다. 기술 블로그로는 AWS의 RAG 가이드, NVIDIA의 RAG 설명, Elastic의 벡터 검색 문서가 유용해요.

한국어 모델은 bge-m3, ko-sroberta-multitask, KoSimCSE를 추천합니다. 실습 코드와 샘플 데이터는 GitHub 링크에서 다운받으실 수 있어요.

이제 Q&A 시간을 갖겠습니다. 오늘 배운 내용 중 궁금한 점이나, 실무 적용 시 고려사항에 대해 자유롭게 질문해주세요. 특히 여러분 회사의 구체적인 상황에 맞는 조언을 드릴 수 있도록 상황을 설명해주시면 좋겠습니다.
