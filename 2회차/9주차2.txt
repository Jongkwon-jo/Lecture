"안녕하세요, 국립창원대학교 PRU 재직자 교육에 참여해주신 여러분 환영합니다. 오늘은 '이미지-언어 결합 및 불량 이미지 설명 자동 생성'이라는 주제로 함께 공부하겠습니다. 

최근 제조업에서 비전검사와 품질관리의 중요성이 더욱 커지고 있죠. 특히 불량품을 단순히 탐지하는 것을 넘어서, '왜 불량인지', '어떤 종류의 불량인지'를 자동으로 설명할 수 있다면 얼마나 좋을까요? 오늘 우리가 배울 Vision-Language Model이 바로 그 답을 제시해줄 거입니다."

먼저 오늘 우리가 무엇을 배우고 실습할지 목표부터 확인해보겠습니다.

### Slide 2: 학습 목표와 일정 (90초)
오늘 교육의 핵심 목표는 세 가지입니다. 

첫 번째, 이미지와 언어를 결합하는 AI 모델의 원리를 이해하는 것입니다. 단순히 '되니까 쓰자'가 아니라 '왜 이렇게 작동하는지' 원리를 아셔야 실무에 응용할 수 있어요.

두 번째, CLIP과 BLIP 모델을 직접 코딩해보는 것입니다. 파이썬 코드를 함께 실행하면서 실제로 불량 이미지를 분류하고 설명을 생성해보겠습니다.

세 번째, 완성된 자동화 파이프라인을 구축하는 것입니다. 오늘 끝나면 여러분만의 불량 설명 시스템을 가져가실 수 있어요.

교육 방식은 강의 40%, 실습 60%입니다. 노트북을 꺼내시고, 같이 코딩해보면서 배우겠습니다.

그럼 이제 Vision-Language Model이 무엇인지부터 알아보겠습니다.

---

### Slide 3: 섹션 1 구분 (15초)
첫 번째 섹션에서는 Vision-Language Model의 기본 개념을 살펴보겠습니다. 복잡한 수식보다는 직관적으로 이해할 수 있도록 설명드리겠습니다.


### Slide 4: 왜 이미지-언어 결합인가? (90초)
"왜 이미지와 언어를 결합해야 할까요? 

전통적인 비전 AI는 '불량'이라고만 알려줍니다. 하지만 작업자가 정말 필요한 건 '왼쪽 상단에 3mm 길이의 선형 스크래치가 있습니다'라는 구체적인 설명이죠.

이미지-언어 결합의 가장 큰 장점은 세 가지입니다.

첫째, 설명 가능한 AI입니다. 단순한 OK/NG가 아니라 구체적인 근거를 제시해줍니다.

둘째, 라벨링 비용을 대폭 줄일 수 있습니다. 제로샷, 소샷 학습으로 새로운 불량 유형도 바로 인식 가능해요.

셋째, 작업자의 의사결정을 지원합니다. 불량 원인과 조치 방안까지 자동으로 제안할 수 있거든요."

"제조 현장에서 불량품이 나왔을 때 가장 오래 걸리는 게 뭔가요? 원인 분석이죠. 이 시간을 AI가 단축해줄 수 있습니다."

---

### Slide 5: 멀티모달 학습의 기본 구조 (90초)
"멀티모달 학습의 핵심은 '공통 언어'를 만드는 것입니다. 

이미지 인코더는 사진을 숫자 벡터로 변환하고, 텍스트 인코더는 문장을 숫자 벡터로 변환합니다. 핵심은 이 두 벡터가 같은 '공간'에서 만나야 한다는 점이에요.

예를 들어, '스크래치가 있는 금속 표면' 이미지와 '스크래치'라는 단어가 벡터 공간에서 가까이 위치하도록 학습시키는 거죠.

CNN이나 Vision Transformer로 이미지를 처리하고, BERT 같은 모델로 텍스트를 처리한 다음, 이 둘을 연결하는 게 기본 아이디어입니다."

"여기서 중요한 건 '정렬(alignment)'입니다. 서로 다른 모달리티를 같은 의미 공간에 맞춰주는 거예요."

---

### Slide 6: 이미지-텍스트 정렬과 대조학습 (90초)

"CLIP의 핵심 기술인 대조학습을 설명드리겠습니다.

원리는 간단합니다. 맞는 쌍(이미지-텍스트)은 가깝게, 틀린 쌍은 멀게 배치하는 거예요.

예를 들어, 스크래치 사진과 '스크래치' 텍스트는 벡터 공간에서 가깝게, 스크래치 사진과 '오염' 텍스트는 멀게 학습시킵니다.

InfoNCE 손실함수가 이 역할을 해줍니다. 수식은 복잡하지만, 개념은 '좋은 건 끌어당기고, 나쁜 건 밀어내기'예요.

이렇게 학습하면 제로샷 분류가 가능해집니다. 한 번도 본 적 없는 불량 유형도 텍스트 설명만으로 분류할 수 있어요."

"조금 있다 CLIP 실습에서 이 원리를 직접 확인해보겠습니다."

---

### Slide 7: 산업 현장에서의 적용 포인트 (90초)

"이론은 좋은데, 실제 현장에서는 어떨까요?

세 가지 핵심 포인트가 있습니다.

첫째, 비전검사 업그레이드입니다. 기존 OK/NG 시스템에 설명 기능을 추가할 수 있어요. '불량'이라고만 하던 걸 '좌측 하단 3시 방향 스크래치, 길이 약 5mm'로 구체화할 수 있습니다.

둘째, 이상탐지와 설명의 결합입니다. 단순히 '이상하다'가 아니라 '왜 이상한지' 근거를 제시해줍니다. 작업자 교육에도 활용 가능해요.

셋째, 현실적 고려사항입니다. 조명 변화, 노이즈, 도메인 편향을 반드시 고려해야 합니다. 실험실과 현장은 다르거든요."

"현장에서 가장 어려운 불량 판정 케이스가 뭔지 궁금합니다. 나중에 사례로 활용해보겠습니다."

이제 구체적으로 어떤 모델들이 있는지 살펴보겠습니다."


### Slide 8: 섹션 2 구분 (15초)
"두 번째 섹션에서는 실제로 사용할 수 있는 주요 모델들을 비교해보겠습니다. CLIP, BLIP, LLaVA의 특징과 용도를 알아보죠."

### Slide 9: CLIP 모델 소개 (90초)
"CLIP부터 살펴보겠습니다. Contrastive Language-Image Pretraining의 줄임말이에요.

CLIP의 가장 큰 강점은 속도입니다. 가볍고 빨라서 실시간 처리가 가능해요. 제로샷 분류도 잘 됩니다. 

하지만 약점도 있어요. 자연어 생성 능력이 제한적입니다. '스크래치다/아니다' 정도는 판단하지만, '좌측 상단 3mm 스크래치'라는 상세 설명은 못 만들어요.

그래서 실무에서는 어떻게 쓸까요? 1차 필터링 용도가 좋습니다. 전체 이미지 중에서 불량 후보를 빠르게 찾아내는 거죠. 그 다음에 정밀한 분석은 다른 모델로 하는 식으로요."

"조금 있다 실습에서 CLIP으로 불량 이미지를 실시간으로 찾아보겠습니다."

---

### Slide 10: BLIP/BLIP-2 모델 소개 (90초)

"BLIP은 Bootstrapping Language-Image Pre-training입니다. CLIP보다 한 단계 진화한 모델이에요.

BLIP의 핵심 기능은 두 가지입니다. 이미지 캡셔닝과 VQA(Visual Question Answering)예요.

이미지 캡셔닝은 사진을 보고 설명을 만드는 것이고, VQA는 이미지에 대해 질문하면 답해주는 기능입니다.

BLIP-2는 여기서 더 나아가서 대형 언어모델(LLM)을 연결했어요. 이미지 인코더와 LLM 사이에 Q-Former라는 브릿지를 두어서 고품질 설명을 생성합니다.

우리 프로젝트에서는 BLIP이 핵심 역할을 해요. 불량 이미지의 상세 설명을 자동으로 생성하는 거죠."

"BLIP-2의 혁신은 이미지 정보를 LLM이 이해할 수 있는 형태로 변환하는 Q-Former에 있습니다."

---

### Slide 11: LLaVA 모델 소개 (90초)
"LLaVA는 Large Language and Vision Assistant입니다. 가장 고급 기능을 제공해요.

가장 큰 장점은 대화가 가능하다는 점입니다. 이미지를 보여주고 '이 불량의 원인이 뭘까?', '어떻게 해결할 수 있을까?' 같은 복잡한 질문도 할 수 있어요.

추론 능력도 뛰어납니다. 단순히 보이는 걸 설명하는 게 아니라, 원인을 분석하고 해결책을 제안할 수 있거든요.

하지만 단점도 있어요. 무겁고 리소스를 많이 먹습니다. GPU 메모리도 많이 필요하고요.

그래서 언제 쓸까요? 복잡한 불량 분석이 필요할 때, 또는 작업자 교육용 설명을 만들 때 유용합니다."

"현장 적용 시에는 비용 대비 효과를 반드시 고려해야 해요. 간단한 작업에는 CLIP, 상세 설명은 BLIP, 복합 분석은 LLaVA 이런 식으로 역할을 나누는 게 좋습니다."


### Slide 12: 모델 선택 가이드 (90초)

"그럼 어떤 모델을 선택해야 할까요? 세 가지 기준이 있습니다.

첫 번째, 속도 vs 품질의 트레이드오프입니다. 
- 빠른 처리가 필요하면 CLIP
- 상세한 설명이 필요하면 BLIP-2나 LLaVA

두 번째, 리소스 제약입니다.
- GPU 메모리가 8GB 미만이면 CLIP
- 16GB 정도면 BLIP
- 그 이상이면 LLaVA도 가능

세 번째, 한글 지원입니다. 대부분 모델이 영어 기준이라 한글 처리 전략이 필요해요. 
- 번역 파이프라인을 추가하거나
- 한글 파인튜닝된 모델을 쓰거나
- 용어사전을 활용하거나

실무에서는 보통 여러 모델을 조합해서 씁니다. CLIP으로 1차 필터링하고, BLIP으로 상세 설명하는 식으로요."

"이론은 이 정도로 하고, 이제 직접 환경을 셋업해서 실습해보겠습니다."


### Slide 13: 섹션 3 구분 (15초)
"이제 본격적인 실습을 위해 파이썬 환경을 설정하겠습니다. 함께 따라해보세요."

### Slide 14: 필수 요구사항 (60초)

"실습 환경부터 체크해보겠습니다.

파이썬은 3.10 이상, PyTorch는 2.x 버전이 필요합니다. CUDA가 있으면 좋지만 CPU로도 실습은 가능해요.

GPU는 8-16GB VRAM을 권장합니다만, 메모리가 부족하면 양자화 옵션을 쓸 수 있어요.

개발 환경은 Jupyter 노트북이나 VSCode 뭐든 상관없습니다. 가상환경은 꼭 만드시고요."

"노트북 켜시고 `python --version` 한번 확인해보실까요? 3.10 이상이면 됩니다."


### Slide 15: 설치 스크립트 (120초)

"이제 필요한 패키지를 설치하겠습니다. 화면에 나온 코드를 그대로 따라해보세요.

첫 번째 줄은 가상환경을 만드는 거고요, 두 번째는 활성화입니다.
세 번째는 PyTorch를 CUDA와 함께 설치하는 거예요. GPU가 없으면 cpu 버전으로 바꾸시면 됩니다.
나머지는 우리가 쓸 주요 라이브러리들이에요."

"다 같이 실행해보겠습니다. 설치가 좀 오래 걸릴 수 있어요. 설치 중에 질문 있으시면 말씀해주세요."

"혹시 설치 에러 나시는 분 계신가요? CUDA 버전 문제일 수 있어요. 그러면 CPU 버전으로 진행하겠습니다."

### Slide 16: 프로젝트 폴더/데이터 구조 (120초)

"프로젝트 폴더 구조를 만들어보겠습니다. 나중에 찾기 쉽게 체계적으로 정리해야 해요.

data 폴더에는 불량 이미지들을 카테고리별로 정리합니다.
prompts 폴더에는 프롬프트 템플릿을 언어별로 저장해요.
notebooks는 실습용 주피터 파일들, src는 파이썬 스크립트들입니다.
outputs에는 결과물들이 저장돼요.

폴더를 미리 만들어두면 나중에 편합니다. 터미널에서 mkdir 명령어로 만드시거나, 파일 탐색기에서 만드세요."

"같이 폴더를 만들어보겠습니다.  이렇게 하면 한 번에 만들 수 있어요."

 "환경 설정이 끝났으니, 이제 CLIP 모델부터 실습해보겠습니다."

### Slide 17: 섹션 4 구분 (15초)
"이제 첫 번째 실습입니다. CLIP 모델로 제로샷 분류와 유사도 검색을 해보겠습니다. 불량 이미지를 자동으로 찾는 실습이에요."

### Slide 18: CLIP 불러오기와 임베딩 (120초)

"CLIP 모델을 불러와보겠습니다. 새로운 셀을 만들고 따라해보세요.

첫 번째 줄들은 필요한 라이브러리를 불러오는 거고요, 
model과 processor를 로드하는 부분이 핵심입니다.

processor는 이미지와 텍스트를 모델이 이해할 수 있는 형태로 변환해주는 역할이에요.
model은 실제 AI 모델 자체구요.

처음 실행하면 모델을 다운로드받느라 시간이 좀 걸릴 거예요. 약 600MB 정도 됩니다."

"다 같이 실행해보겠습니다. 로딩이 완료되면 'CLIPModel'이라고 출력될 거예요."

### Slide 19: 텍스트 프롬프트 설계 (90초)
"CLIP의 성능은 프롬프트 설계에 달려 있습니다. 어떻게 설명하느냐에 따라 결과가 크게 달라져요.

좋은 프롬프트의 예를 보겠습니다:
- '스크래치가 있는 금속 표면'
- '오염 없는 깨끗한 양품'
- '표면에 균열이 보이는 부품'

포인트는 세 가지입니다.

첫째, 구체적이고 명확해야 해요. '불량'보다는 '스크래치'가 좋습니다.

둘째, 도메인 용어를 포함시키세요. 제조업 현장에서 쓰는 용어들 말이에요.

셋째, 동의어를 확장하세요. 같은 의미를 다르게 표현한 여러 프롬프트를 준비하는 거죠."

"한국어 프롬프트도 가능하지만, 영어가 더 정확해요. 번역 도구를 활용하시거나, 영한 혼용도 괜찮습니다."

---

### Slide 20: 제로샷 분류 및 유사도 점수 (150초)
"이제 실제로 이미지를 분류해보겠습니다. 코드를 따라해보세요.

코드 설명드리겠습니다.
- processor가 이미지와 텍스트를 동시에 처리해요
- torch.no_grad()는 메모리 절약을 위한 거구요
- softmax로 확률값으로 변환합니다
- 가장 높은 점수가 예측 결과예요"

"실행해보시면 각 라벨별로 점수가 나올 거예요. 0.8 이상이면 꽤 확신한다고 보시면 됩니다."

"에러 나시는 분 있나요? 이미지 URL 문제일 수 있어요. 로컬 파일로 바꿔서 해보겠습니다."


### Slide 21: 성능 향상 팁 (90초)

"CLIP 성능을 높이는 세 가지 팁을 알려드리겠습니다.

첫 번째, 다중 프롬프트 앙상블입니다.
하나의 불량 유형을 여러 방식으로 설명한 다음, 평균이나 최댓값을 쓰는 거예요.

두 번째, 이미지 전처리입니다.
CLAHE로 대비를 높이거나, 중앙 크롭을 써보세요.

세 번째, 하드 네거티브 추가입니다.
헷갈리기 쉬운 반대 사례를 명시적으로 포함시키는 거예요.
'스크래치'와 '그림자'는 구분하기 어려우니까 둘 다 라벨에 넣는 식으로요."

"완벽한 성능을 바라지 마세요. 80-85% 정확도면 1차 필터링 용도로는 충분합니다."

"CLIP 실습이 끝났습니다. 이제 더 상세한 설명을 만드는 BLIP 모델을 해보겠습니다."

## 📝 Section 5: BLIP 실습 (8분)

### Slide 22: 섹션 5 구분 (15초)
"다섯 번째 섹션에서는 BLIP 모델로 이미지 캡셔닝을 실습하겠습니다. 불량 이미지에 자동으로 설명을 붙이는 거예요."


### Slide 23: BLIP 로딩과 기본 캡션 (120초)
"BLIP 모델을 로드해보겠습니다. 새로운 셀에서 실행해보세요.

BLIP은 CLIP과 달리 텍스트를 생성합니다. 이미지를 보고 자연어 설명을 만드는 거예요.

max_new_tokens은 생성할 단어 수 제한이고, skip_special_tokens은 특수 기호를 빼는 옵션입니다.

첫 번째 실행에서는 모델 다운로드로 시간이 좀 걸릴 거예요. BLIP이 CLIP보다 좀 더 큰 모델이거든요."

"실행해보시면 영어로 설명이 나올 거예요. 'a metal surface with scratches' 이런 식으로요."


### Slide 24: 생성 제어 파라미터 (90초)
"설명의 품질과 스타일을 조절할 수 있는 파라미터들을 알아보겠습니다.

각 파라미터 설명드리겠습니다:
- max_new_tokens: 짧게 하면 간결하고, 길게 하면 상세해져요
- temperature: 낮으면 안전하고 반복적, 높으면 창의적이지만 부정확할 수 있어요
- top_p: 확률 상위 몇 %만 사용할지 결정
- num_beams: 여러 후보를 비교해서 최선을 선택

실무에서는 온도 0.3-0.7, top_p 0.8-0.95 정도가 적당해요."

"불량 설명에는 정확성이 중요하니까 온도는 낮게, 빔 서치는 높게 설정하는 게 좋습니다."


### Slide 25: 한글 설명 생성 전략 (90초)
"BLIP은 기본적으로 영어 모델이에요. 한글 설명을 만드는 세 가지 전략이 있습니다.

첫 번째, 번역 파이프라인을 추가하는 방법입니다.

두 번째, 프롬프트에 한국어를 섞는 방법입니다.
'표면에 스크래치가 보임' 이런 식으로 미리 프롬프트를 주는 거죠.

세 번째, 용어사전을 활용하는 방법입니다.
'scratch → 스크래치', 'contamination → 오염' 이런 매핑 테이블을 만드는 거예요."

"번역이 가장 간단하지만 약간의 품질 손실이 있어요. 중요한 용어는 사전을 병행 사용하시는 게 좋습니다."

### Slide 26: BLIP-2로 품질 향상 (120초)

"더 좋은 품질을 원한다면 BLIP-2를 써보세요. 


BLIP-2의 장점은:
1. 더 정확하고 상세한 설명
2. 프롬프트 기반 제어 가능
3. 복잡한 추론 능력

단점은:
1. 메모리를 많이 씀 (8bit 양자화 권장)
2. 속도가 느림
3. GPU가 거의 필수"

"메모리 부족 에러가 나면 load_in_8bit=True 옵션을 쓰세요. 품질은 거의 그대로면서 메모리는 반으로 줄어들어요."

"이제 실제 불량 데이터셋을 구축하는 방법을 알아보겠습니다."



### Slide 27: 섹션 6 구분 (15초)

"여섯 번째 섹션에서는 실제 불량 이미지 데이터셋을 구축하는 방법을 다루겠습니다. 카테고리 정의부터 주석, 전처리까지 전 과정을 살펴보죠."


### Slide 28: 불량 카테고리와 정의 (90초)

"데이터셋의 품질은 라벨 정의에서 결정됩니다. 명확하고 일관된 기준이 필요해요.

주요 불량 카테고리 예시:
1. 스크래치 (Scratch): 선형 손상, 깊이별 분류
2. 오염 (Contamination): 이물질 부착, 얼룩
3. 찍힘 (Dent): 움푹한 변형, 압흔
4. 균열 (Crack): 금이 감, 갈라짐
5. 변색 (Discoloration): 색상 변화, 녹

각 카테고리마다 세부 기준을 만드세요:
- 포함 기준: 어떤 것을 이 카테고리로 분류할지
- 제외 기준: 헷갈리는 경계 사례는 어떻게 처리할지
- 심각도: 경미/보통/심각으로 나눌지

라벨러 간 일치도(Inter-annotator agreement)도 중요해요. 같은 이미지를 여러 사람이 보고 같은 결과가 나와야 합니다."

"현장 전문가와 함께 기준을 만드세요. 이론적 분류보다 실무 경험이 더 중요합니다."

### Slide 29: 주석 형식과 메타데이터 (90초)
"데이터 주석 형식을 표준화해야 합니다. COCO 포맷이나 CSV 포맷을 권장해요.

CSV 포맷도 간단하고 좋아요:

메타데이터가 중요합니다:
- 촬영 조건: 조명, 각도, 카메라
- 환경 정보: 온도, 습도, 시간
- 설비 정보: 라인 번호, 제품 타입

이런 정보가 있어야 나중에 성능 분석할 때 유용해요."

"데이터셋 버전 관리도 중요해요. DVC나 Git-LFS를 추천합니다."

### Slide 30: 전처리/증강 파이프라인 (120초)
"좋은 모델을 만들려면 데이터 전처리와 증강이 필수입니다.

전처리 파이프라인:
1. 이미지 정규화: 크기 통일, 해상도 표준화
2. 노이즈 제거: 가우시안 필터, 미디언 필터
3. 색보정: 화이트 밸런스, 대비 조절
4. ROI 추출: 관심 영역만 크롭

데이터 증강 기법:
- 기하학적: 회전, 뒤집기, 크롭
- 광학적: 밝기, 대비, 색상 변경
- 노이즈 추가: 가우시안, 솔트앤페퍼
- 블러링: 모션 블러, 가우시안 블러

데이터 분할은 7:2:1 (train:val:test) 비율을 권장해요."

"실제 현장 조건을 반영한 증강이 중요해요. 너무 인위적인 증강은 오히려 성능을 떨어뜨릴 수 있어요."

