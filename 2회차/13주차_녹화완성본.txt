이겸
안녕하세요, 여러분. 국립창원대학교 재직자 교육 과정에 오신 것을 환영합니다.
오늘은 엘엘엠 오류 대응, 특히 할루시네이션 탐지와 검증 절차에 대해 함께 공부하게 될 텐데요. 실무에서 바로 적용 가능한 Python 중심의 실습으로 진행하겠습니다.

먼저 이 강의를 통해 무엇을 얻어가실 수 있는지 말씀드릴게요. 교육 목표는 명확합니다.
LLM이 만들어내는 환각 현상, 즉 할루시네이션을 탐지하고 검증하며 완화하는 실무 스킬을 습득하는 겁니다. 이론만 배우는 게 아니라 실제로 손으로 코드를 작성하면서 체득하는 과정이죠.
수강 대상은 정확성이 중요한 LLM 기반 서비스를 개발하거나 연구하시는 실무자분들입니다. 그리고 핵심 산출물로는 즉시 적용 가능한 최소 기능 검증 파이프라인, 즉 MVP 수준의 Python 코드를 직접 만들어보실 거예요. 실무에 돌아가서 바로 쓸 수 있는 코드를 가져가시는 겁니다.

자, 그럼 오늘 강의의 전체 구성을 살펴볼까요? 총 6개 섹션으로 나뉘어 있는데요.
첫 번째로 LLM 할루시네이션의 정의와 문제점, 유형, 발생 원인을 이해하고요. 두 번째로는 탐지 기법 개요를 다룹니다. 통계적 방법, 일관성 체크, 외부 지식 활용까지 세 가지 축을 배우게 되죠.
세 번째는 Python 실습 첫 번째 시간입니다. 환경을 세팅하고 불확실성과 일관성을 검사하는 기본 코드를 작성해볼 거예요. 네 번째로는 검증 절차 프레임워크를 표준 워크플로우와 임계값 설정 관점에서 배웁니다.
다섯 번째는 고급 검증 실습 시간인데요, 래그 인용 검증, 앙상블 기법, 자동 평가까지 다룰 예정이고요. 마지막으로 강의를 요약하며 마무리 하겠습니다. 본 강의는 이론 학습과 Python 코드 실습을 교차하며 진행된다는 점 기억해주세요.

자, 그럼 첫 번째 섹션을 시작하겠습니다. L-LM 할루시네이션을 제대로 이해하는 시간인데요.
정의부터 시작해서 문제점, 유형, 발생 원인, 그리고 평가 지표까지 순차적으로 다루게 됩니다. 기초를 탄탄히 다지는 시간이니 집중해주시기 바랍니다.

할루시네이션, 이 단어 자체는 환각을 의미하는데요. LLM에서는 어떤 의미일까요?
정의를 먼저 보시면, 모델이 문맥적으로는 그럴듯하지만 사실과 다르거나 조작된 내용을 생성하는 현상을 말합니다. 학계에서는 이를 "확률적 앵무새" 현상이라고도 부르죠. 
단어의 확률 분포만 학습했을 뿐, 진짜 의미를 이해한 게 아니라는 비판적 표현입니다.
주요 유형을 살펴보면 네 가지로 나눌 수 있어요. 첫째, "사실오류"는 실제 세계와 다른 정보를 생성하는 거예요. 예를 들어 연도를 틀리게 말하거나 인물을 잘못 언급하는 경우죠. 둘째, 출처 위조는 존재하지 않는 논문이나 URL을 그럴듯하게 만들어내는 겁니다.
셋째, 논리 비약은 추론 과정에 오류가 있는 경우고요. 넷째, 환각적 인용은 제시한 근거가 실제 내용과 무관한 경우를 말합니다. 
비즈니스 영향은 어떨까요? 잘못된 정보 생성은 서비스 신뢰도 하락으로 직결되고, 법적·윤리적 리스크를 발생시키며, 검증 및 운영 비용을 크게 증가시킵니다.

이번에는 각 유형별로 어떤 징후가 나타나는지 구체적으로 살펴보겠습니다.
"사실오류"는 실제 세계의 사실과 다른 정보를 생성하는 건데요, 식별 징후로는 과도한 단정과 숫자 남발이 있어요. "100% 확실하다"라는 표현이나 구체적이지만 틀린 숫자를 나열하는 패턴이 보이죠.
근거 불명 유형은 주어진 컨텍스트나 학습 데이터에 존재하지 않는 내용을 생성하는 건데, 내부 모순이 징후로 나타납니다. 앞 문장과 뒷 문장이 충돌하거나 논리가 연결되지 않는 거예요.
논리 비약은 전제에서 결론으로 이어지는 추론 과정의 오류를 말하는데요, 특정 키워드 반복이 징후입니다. 모델이 자신감이 없을 때 특정 단어나 문구를 계속 반복하는 경향이 있거든요. 이를 Degeneration이라고 부릅니다.
출처 위조는 존재하지 않는 논문이나 기사를 날조하는 건데, 인용 링크가 작동하지 않는 게 명확한 징후죠. 제시된 URL이 4/공/4 에러를 반환하거나 엉뚱한 페이지로 연결되는 겁니다.

그렇다면 왜 이런 오류가 발생할까요? 네 가지 주요 기술적 요인이 있습니다.
첫 번째는 데이터 문제예요. 훈련 데이터 자체에 노이즈가 있거나 편향된 데이터 분포로 인한 일반화 오류가 발생하죠. 또한 최신 정보가 없는 구식 데이터, 즉 Knowledge Cutoff 문제도 있습니다.
두 번째는 프롬프트 문제입니다. 모호한 지시로 인해 모델이 의도를 잘못 해석할 여지가 생기고요. 충분한 맥락, 즉 Context를 제공하지 않거나, 모델을 압박하는 유도성 질문을 던지면 환각이 발생하기 쉬워져요.
세 번째는 디코딩 파라미터 문제인데요. Temperature가 높으면 다양성을 추구하다 보니 무작위성이 증가합니다. Top-p나 Top-k 샘플링 범위를 과도하게 확장하거나, 생성 길이 제약으로 문장이 미완성되면 왜곡이 일어나죠.
네 번째는 컨텍스트 윈도우 문제예요. 
윈도우를 초과하면 입력 정보의 앞부분이 소실되는데, 이를 "Lost-In-the-Middle" 현상이라고 합니다. 어텐션 메커니즘의 집중력이 분산되거나 너무 긴 입력으로 정보가 혼동되는 거죠.

정확한 검증을 위해서는 핵심 성능 지표를 이해해야 합니다.
먼저 Factuality와 Faithfulness의 차이를 아셔야 해요. Factuality는 사실성으로, 생성된 내용이 외부의 객관적 사실과 일치하는지를 보는 거예요. 반면 Faithfulness는 충실성으로, 제공된 컨텍스트 내용에 기반하여 충실하게 생성되었는지를 평가하죠.
정량적 측정 지표로는 Hallucination Rate가 있는데요, 전체 응답 중 할루시네이션이 발생한 비율을 의미합니다. 또한 Precision과 Recall의 Trade-off를 고려해야 하는데, 
탐지 모델이 오류를 얼마나 정확하게 잡는지가 정밀도고, 놓치지 않는지가 재현율이에요.
평가 방법론은 크게 두 가지입니다. Human Eval, 즉 사람이 직접 평가하는 방식은 정확도가 가장 높아 Gold Standard로 불리지만, 비용과 시간이 많이 들어요. Auto Eval은 LLM이나 규칙 기반으로 자동 평가하여 확장성을 확보하는 방식인데, 두 방법을 병행 사용하는 게 권장됩니다.

자, 이제 두 번째 섹션으로 넘어가보겠습니다. 오류를 찾아내는 핵심 전략들을 배우는 시간이에요.
통계적 신뢰 기반 탐지, 일관성 검증, 그리고 외부 지식 검증, 이 세 가지 축으로 접근하게 됩니다. 각각의 방법론을 하나씩 살펴보시죠.

첫 번째 전략은 통계적 신뢰 기반 탐지입니다. 모델 내부의 확률 분포와 출력 패턴을 분석하는 거죠.
불확실성 추정이 핵심인데요, 토큰별 로그확률, 즉 Log, probs와 엔트로피를 계산하여 모델의 확신 수준을 정량화합니다. 모델이 다음 단어를 예측할 때 확률이 여러 단어에 골고루 분산되어 있다면 불확실성이 높은 거예요.
파라미터 민감도 분석도 중요합니다. Temperature나 Top-p 설정에 따른 출력 변화를 분석하고, 오류 발생과의 상관관계를 파악하는 거죠. 마지막으로 규칙 기반 감지인데, 비정상적인 문장 길이나 요청한 포맷, 예를 들어 json 형식을 이탈하는 경우를 즉각 탐지할 수 있습니다.

두 번째 전략은 일관성 검증입니다. 모델 답변의 안정성과 논리적 일관성을 평가하는 거예요.
Self-consistency는 동일한 프롬프트로 여러 번 샘플링한 후, 답변들 간의 합의율을 측정하는 방법이에요. 모델이 진짜로 아는 내용이라면 일관된 답변을 내놓을 거고, 환각 상태라면 매번 다른 답변을 만들어낼 확률이 높겠죠.
Round-trip 기법은 생성된 답변으로부터 질문을 역으로 생성하는 QG 과정을 거쳐서, 원래 질문과의 일치도를 확인하는 겁니다. 일종의 왕복 검증이라고 볼 수 있어요.
NLI 모순 탐지는 자연어 추론 모델을 활용해 문장 간의 논리적 모순이나 함의 관계를 자동으로 판별하는 방식입니다. 기계가 논리를 체크하는 거죠.

세 번째 전략은 외부 지식 검증인데요, 3단계 프로세스로 진행됩니다.
1단계는 래그와 Context 주입이에요. 질의와 관련된 신뢰할 수 있는 문서를 검색해서 프롬프트에 컨텍스트로 주입하고, 인용을 강제하는 겁니다. 
프롬프트 예시를 보면 "제공된 컨텍스트만을 기반으로 답변하고, [1]과 같이 출처를 인용하세요"라고 지시하죠.
2단계는 Fact Checking입니다. 생성된 답변의 핵심 주장을 추출해서 Google Search, 위키피디아, 사내 DB 등과 대조하는 거예요. 액션으로는 claim을 추출한 다음 search_tool로 evidence를 찾는 과정이 들어갑니다.
3단계는 Citation Validation인데, 생성된 답변에 달린 인용 번호가 실제로 해당 문서의 내용을 올바르게 반영하고 있는지 매핑 검사를 수행합니다. 메트릭으로는 NLI 점수를 사용해서, 예를 들어 0.7 미만이면 거절하는 식으로 임계값을 설정하죠.

단일 모델의 한계를 극복하기 위한 전략들을 살펴볼게요.
모델 위원회 방식은 서로 다른 LLM이 결과를 상호 검증하거나 투표를 통해 다수결로 최종 답변을 결정하는 겁니다. 장점은 교차 검증을 통해 o탐지를 줄이고 편향을 상쇄할 수 있다는 거예요. 하지만 단점으로 다중 추론 및 검토 과정으로 API 비용이 상승하고 응답 속도가 저하되죠.
휴먼 인 더 루프, 즉 HITL 방식은 신뢰도 점수가 임계값 미만일 경우 답변을 보류하고 인간 전문가의 검토 과정을 강제하는 겁니다.
정밀도가 향상되는 장점이 있지만, 시스템 복잡도가 상승하는 단점도 있습니다.
규칙과 모델 하이브리드는 정규식이나 금지어 사전 같은 결정론적 규칙과 모델의 유연한 판단을 결합하는 방식인데요. 다양한 모델 관리와 파이프라인 구성 등으로 인해 아키텍처 및 유지보수 난이도가 증가한다는 점을 고려해야 합니다.

할루시네이션을 사전에 차단하기 위한 프롬프트 작성법을 알아봅시다.
첫 번째, 불확실성 회피입니다. 모델이 정답을 확신할 수 없을 때 억지로 지어내지 말고 모른다고 답하도록 명시적으로 지시해야 해요.
시스템 프롬프트 예시를 보면 "답변이 컨텍스트에 없다면, '모르겠습니다'라고 말하세요. 답을 만들어내려고 하지 마세요."라고 명시하죠.
두 번째, 근거 출처 요구입니다. 답변의 각 문장이나 주장에 대해 참고한 문서나 데이터의 출처를 명시하도록 강제하는 거예요.
"문서 ID를 참조하여 인용과 함께 답변을 제공하세요"라는 식으로 지시합니다.
세 번째, 포맷 강제는 json 스키마나 특정 형식을 강제하여 자유로운 텍스트 생성 중 발생하는 환각을 억제하는 방법이고요.
네 번째, 반박 질문은 생성된 답변을 다시 모델에게 입력해서 논리적 오류나 모순이 없는지 스스로 점검하게 하는 Self-Correction 기법입니다.

자, 이제 본격적으로 코드를 작성해보는 시간입니다.
환경 세팅부터 시작해서 불확실성 추정, 일관성 검증, 간단한 근거 확인까지 실습하게 될 겁니다. 실제로 손을 움직여보는 시간이니 집중해주세요.

실습을 위한 환경 구성부터 시작하겠습니다.
먼저 Python 3.10 이상 버전이 설치되어 있어야 해요. 최신 LLM 라이브러리와의 호환성을 위해서죠. 터미널에서 python-version을 실행해서 3.10.x 이상인지 확인하시면 됩니다.
가상 환경은 프로젝트별 패키지 충돌을 방지하기 위해 필수예요.
python, dash-M, v-env, .v-env 명령어로 가상환경을 생성합니다.
Windows는 .v-env, Scripts, activate로
Mac이나 Linux는 source .v-env, bin, activate로 활성화하시면 됩니다.
필수 라이브러리는 pip로 한 번에 설치할 수 있어요. transformers, datasets, faiss-cpu, tiktoken, scikit-learn 등이 포함되는데요. 
모델 로드, 벡터 검색, 토큰 처리를 위한 핵심 패키지들이죠.
개발 도구로는 Jupyter Notebook이나 VS Code를 준비하시면 됩니다. 실습 코드의 단계별 실행과 결과 확인에 편리하거든요. 
터미널 창에 pip install 쥬피터랩을 입력한 후 쥬피터랩 명령어로 쥬피터 노트북을 실행하거나, VS Code의 Python extension을 활용하시면 됩니다.

첫 번째 실습은 엔트로피를 이용한 불확실성 추정입니다.
엔트로피란 무엇일까요? 모델이 다음 단어를 예측할 때 확률 분포가 얼마나 평평한지를 나타내는 지표예요. 엔트로피가 높을수록 모델이 다음 내용을 확신하지 못해서 할루시네이션 위험이 증가하는 거죠.
코드를 살펴볼게요. 일번부터 육번 줄까지는 라이브러리를 불러오고 GPT-two 모델과 토크나이저를 로드하는 부분입니다. 
팔번부터 십사번 줄은 입력 텍스트를 처리하고 로짓을 추출하는 과정인데요, "대한민국의 수도는"이라는 텍스트를 입력하면 모델이 마지막 토큰의 예측값을 출력하죠.
십육번부터 십팔번 줄이 핵심입니다. 로짓을 Softmax 함수로 확률 분포로 변환하고, 엔트로피를 계산하는 거예요. 수식은 마이너스 확률 곱하기 로그 확률의 합으로 표현되죠. 
이십번 줄에서 결과를 출력하면 "토큰 엔트로피: 2.8451" 같은 값이 나오는데, 이 값이 높을수록 불확실하다는 의미입니다.
탐지 전략은 간단해요. 생성된 문장의 평균 엔트로피가 특정 임계값을 초과하면 답변을 거절하거나 래그, 즉 검색을 유도하는 겁니다.

두 번째 실습은 자가 일관성 검증입니다.
Self-consistency의 원리는 이렇습니다. 동일한 질문에 대해 모델이 여러 번 답변을 생성하게 하고, 그중 가장 빈번하게 등장하는 답변을 정답으로 채택하거나 합의율을 계산하는 거예요. 모델이 사실을 알고 있다면 일관된 답변을 내놓지만, 환각 상태라면 매번 다른 답변을 생성할 확률이 높다는 가정에 기반하죠.
코드를 보면 삼번부터 오번 줄에서 텍스트 생성 파이프라인을 설정하고 프롬프트를 준비합니다. "피타고라스 정리 설명과 공식?"이라는 질문이에요.
칠번부터 십이번 줄이 다중 샘플링 부분인데요, N은 5로 설정해서 5번 생성을 반복합니다. do_sample은 True로 설정해서 다양성을 확보하고, top-p는 0.9, temperature도 0.9로 파라미터를 조정하죠.
십사번부터 십구번 줄은 합의율을 계산하는 로직입니다. 핵심 키워드로 "a제곱 더하기 b 제곱은  c제곱"과 "직각삼각형"을 설정하고, 각 샘플이 이 키워드들을 모두 포함하는지 확인해요. 그 비율이 합의율인데, 1.0에 가까울수록 신뢰도가 높다고 판단하는 겁니다.

세 번째 실습은 외부 검색을 통한 근거 확보입니다.
실시간 근거 확보가 왜 중요할까요? LLM의 내부 지식, 즉 Parametric Memory에만 의존하지 않고 외부 검색 엔진을 호출하여 검증 가능한 최신 정보를 획득하기 위해서죠.
코드를 살펴볼게요. 사번 줄에서 검색어를 설정합니다. "세종대왕 즉위 연도"라는 쿼리예요. 칠번 줄은 위키피디아 검색 API 엔드포인트를 설정하는 부분이고요.
구번부터 이십사번 줄은 try-except 블록으로 감싸서, HTTP 요청을 처리합니다. requests-get으로 API를 호출하고, json을 파싱해서 결과를 추출하죠.
십오번부터 십구번 줄에서 가장 연관성 높은 문서의 제목과 설명을 출력하는데, 이게 바로 검색 결과예요.
사실여부 대조는 어떻게 할까요? 모델의 답변에서 추출한 키워드로 검색을 수행하고, 반환된 문서 제목이나 스니펫을 통해 생성된 답변의 타당성을 1차적으로 검증하는 겁니다. 쿼리에서 API로, API에서 Evidence로 이어지는 검증 플로우죠.

네 번째 실습은 근거 스니펫 매칭입니다.
스니펫 매칭이란 검색된 문서의 방대한 내용 중 질문에 대한 구체적인 정답 근거가 포함된 핵심 문장이나 단락을 추출하는 과정이에요.
코드를 보면, 사번 줄에서 위키피디아 페이지 키를 준비합니다. "세종"이라는 식별자죠. 
칠번, 팔번 줄은 페이지 전체 HTML 텍스트를 가져오는 부분이고요.
십이번, 십삼번 줄이 핵심인데요, 정규식으로 '연도 + 즉위' 패턴을 검색합니다. 예를 들어 "1418년 8월 ... 즉위하였다" 같은 패턴을 매칭하는 거예요. 
십육번부터 이십번 줄에서 결과를 추출하고 출력합니다. 매칭이 성공하면 "추출된 즉위 연도: 1418"이라는 결과가 나오고, 실패하면 "매칭되는 정보가 없습니다"라고 표시되죠.
정규식 활용은 연도, 날짜, 수치 등 정형화된 패턴을 찾을 때 빠르고 정확하게 데이터를 추출할 수 있어서 유용합니다.

네 번째 섹션으로 넘어가겠습니다. 검증 절차를 체계화하는 시간이에요.
표준 워크플로우를 설계하고, 임계값을 설정하며, 로깅과 추적성을 확보하는 방법을 배우게 됩니다. 실무에 바로 적용할 수 있는 프레임워크를 구축하는 거죠.

먼저 표준 워크플로우의 입력부터 최종 응답까지 7단계 검증 파이프라인을 살펴보겠습니다.
1단계는 입력 정규화 및 안전성 검사예요. 사용자 입력의 모호성을 제거하고 Prompt Injection 같은 공격을 방어하는 Input Guardrails를 설정하죠.
2단계는 래그 컨텍스트 주입입니다. 신뢰할 수 있는 외부 문서를 검색, 즉 Retrieval해서 프롬프트에 지식을 주입하는 거예요.
3단계는 1차 생성인데, Temperature를 낮춰서 0.1에서 0.3 정도로 설정하고 사실 기반의 초안을 생성합니다. 보수적 디코딩이라고 부르죠.
4단계는 내부 탐지로, 토큰 불확실성과 금칙어 포함 여부를 1차적으로 스크리닝하는 Detection Logic이 들어갑니다.
5단계는 외부 검증인데요, 검색 엔진이나 NLI 모델을 통해 생성문의 사실 관계를 교차 검증하는 External Verify 과정이죠.
6단계는 결정 및 조치로, 임계값 초과 시 재생성하거나 거절하거나 수정 후 사용자에게 전달하는 Decision Policy를 실행합니다.
7단계는 로깅과 모니터링인데, 입출력 및 판단 근거를 저장하고 할루시네이션 지표를 추적하는 옵저버빌리티를 확보하는 겁니다.

다음으로 임계값 및 정책 설정 가이드를 통해 안정적인 서비스 운영을 위한 4가지 핵심 기준을 알아봅시다.
첫 번째, 엔트로피와 합의율 임계값 설정입니다. 테스트 데이터셋을 통해 서비스가 허용 가능한 불확실성 상한과 일관성 하한 점수를 정의해야 해요.
예를 들어 max entropy는 0.85, min_agreement_rate는 0.7로 설정하고, 임계값 초과 시 답변 거절 로직을 수행하는 거죠.
두 번째, 금지 응답 정책입니다. 모델이 개인적 의견을 피력하거나 근거 없는 추측성 발언을 하지 않도록 명확한 규칙을 설정하는 거예요.
"personal opinion"이 포함되면 "AI로서 저는 데이터 기반의 사실을 제공합니다"라는 표준 응답을 반환하도록 하죠.
세 번째, 재시도 및 파라미터 전략인데요. 1차 생성 실패 시 사용할 보수적인 디코딩 파라미터와 재시도 횟수를 지정합니다. 예를 들어 temperature를 0.1로 낮추고 max_retries를 2로 설정하는 식이죠.
네 번째, 도메인 키워드 필터링입니다. 업무 도메인에 특화된 필수 포함 키워드인 화이트리스트와 금지 키워드인 블랙리스트를 관리하는 겁니다.

신뢰할 수 있는 AI 시스템을 위한 데이터 기록 절차를 알아봅시다.
먼저 입출력 전수 기록입니다. 입력 프롬프트, 모델 응답, 참조 컨텍스트뿐만 아니라 모델 버전 및 파라미터까지 모든 상호작용을 저장해야 해요. 나중에 문제가 생겼을 때 추적이 가능하도록 말이죠.
다음으로 판단 근거 추적인데요. 할루시네이션 탐지 점수, 래그 인용 출처, 
최종 결정 경로, 즉 Routing을 상세히 기록하여 추적하는 겁니다. 
왜 이런 답변이 나왔는지 설명 가능성을 확보하는 거예요.
마지막 민감 정보 마스킹입니다. 로그 저장 전 PII, 즉 개인정보 및 기밀 데이터를 식별하여 자동 마스킹 규칙을 적용해야 해요. GDPR 같은 보안 규정을 준수하는 게 필수적이죠.

다섯 번째 섹션, 고급 Python 실습 시간입니다.
래그와 사이테이션, 앙상블 검증, 자동 평가, 에러 라우팅까지 실전 수준의 검증 시스템을 구축해보겠습니다. 조금 더 복잡하지만 실무에서 이정도 수준은 필요하죠.

래그, 즉 검색 증강 생성에 대해 알아봅시다.
래그란 L-LM이 학습하지 않은 최신 정보나 사내 데이터를 외부 저장소에서 검색하여 프롬프트에 컨텍스트로 주입하는 거예요. 이를 통해 사실에 기반한 답변을 유도하죠.
코드를 살펴볼게요. 이번부터 사번 줄은 라이브러리를 불러오는 부분이고, 육번 줄에서 문서 리스트를 준비합니다.
구번, 십번 줄은 벡터화 과정인데요, 실제로는 OpenAI Embedding 같은 고급 임베딩을 사용하는 게 권장되지만 여기서는 TF-IDF를 예시로 쓰고 있어요.
십삼번, 십사번 줄은 faiss 인덱스를 생성하고 데이터를 추가하는 부분이죠. 벡터 검색의 핵심입니다.
십칠번부터 십구번 줄은 질의를 처리하고 검색을 수행하는 부분인데, Top-two 검색 결과를 가져오고 있어요.
이십이번, 이십삼번 줄에서 컨텍스트를 구성하는데 검색된 문서들을 줄바꿈으로 연결해서 하나의 컨텍스트로 만드는 겁니다.
벡터 검색은 질의와 문서를 벡터, 즉 숫자로 변환하여 의미적으로 가장 유사한 문서를 고속으로 찾아내는 기술이에요. Fais나 ChromaDB 같은 도구를 활용하죠.

인용 검증에 대해 알아보겠습니다.
검증 목표는 래그 모델이 생성한 답변 내의 인용 표기가 실제로 제공된 근거 문서에 존재하는지 확인하는 거예요. 존재하지 않는 문서를 인용하는 가짜 인용, 즉 Fake 싸이테이션을 방지하기 위해서죠.
코드를 보면 사번부터 팔번 줄에서 모델이 생성한 답변과 검색된 근거 문서를 준비합니다. 답변에는 [1], [2] 같은 인용 번호가 들어있고, refs 딕셔너리에는 실제 문서 내용이 매핑되어 있어요.
십이번 줄이 핵심인데요, 정규표현식으로 답변 내 사용된 인용구를 추출합니다. 해당 패턴은 대괄호 안의 숫자를 탐지합니다.
십사번 줄에서 추출된 인용 번호를 출력하고, 십칠번 줄에서 인용 유효성을 검증합니다. 사용된 인용이 모두 레퍼런스 키에 포함되어 있는지, 즉 부분집합 관계를 확인하는 거예요.
십구번부터 이십이번 줄에서 검증 결과를 출력하는데, 모든 인용이 유효하면 체크 표시와 함께 Passed 메시지가 나오고, 그렇지 않으면 경고 메시지가 나오죠.
Hallucinated Citations는 모델이 내용을 정당화하기 위해 존재하지 않는 레퍼런스 번호를 임의로 생성하는 현상을 말합니다.

하이브리드 검증 전략을 알아봅시다.
하이브리드 검증이란 규칙 기반의 빠른 필터링과 모델 기반의 정밀 검증을 결합하여 비용 효율성과 정확도를 동시에 확보하는 전략이에요.
코드를 보면 일번 줄에서 get_ensemble_score 함수를 정의하고, 이번부터 오번 줄에서 규칙 기반 점수를 계산합니다. 추측성 단어인 "아마도", "추측컨대", "확실치 않지만" 같은 키워드를 감지하면 0점 처리하는 거죠.
팔번, 구번 줄은 가중치 적용 부분인데요, 모델 점수에 60%, 규칙 점수에 40%의 가중치를 부여하고 최종 점수를 계산합니다.
십사번, 십오번 줄은 테스트 실행 부분이에요. "아마도 그럴 것입니다"라는 샘플 텍스트와 모델 신뢰도 0.95를 입력으로 줍니다.
십칠번 줄에서 앙상블 점수를 계산하고, 십구번, 이십번 줄에서 결과를 출력하는데요. 최종 신뢰도가 0.57로 나와요. 모델 점수가 높아도 규칙 패널티로 인해 신뢰도가 하락한 거죠.
가중치 합산 방식으로 특정 금지어 사용 시 패널티를 부여하고, 외부 NLI 모델의 사실성 점수를 가중 합산하여 최종 판단을 내리는 겁니다.

자, 이제 오늘 강의를 마무리하는 시간입니다.
핵심을 정리해볼게요. 첫째, LLM 할루시네이션은 사실오류, 출처 위조, 논리 비약, 근거 불명 등 다양한 형태로 나타나며, 비즈니스에 심각한 리스크를 초래합니다.
둘째, 탐지 기법은 크게 세 가지 축으로 접근하는데요. 통계적 신뢰 기반 탐지, 일관성 검증, 그리고 외부 지식 검증이 있습니다. 각각의 방법론을 상황에 맞게 조합해서 사용해야 하죠.
셋째, Python 실습을 통해 엔트로피 계산, Self-consistency 검증, 래그 기반 인용 검사, 앙상블 점수 계산까지 실무 수준의 코드를 작성해봤습니다. 이 코드들은 여러분의 실제 프로젝트에 바로 적용할 수 있어요.
넷째, 검증 절차 프레임워크는 입력 정규화부터 로깅까지 7단계 워크플로우로 체계화되며, 임계값 설정과 정책 관리가 핵심입니다.
마지막으로 기억할 원칙은 "Trust, but Verify"입니다. 
L-LM을 신뢰하되, 반드시 검증하라는 거죠.

긴 시간 경청해주셔서 감사합니다.

