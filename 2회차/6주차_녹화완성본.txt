안녕하세요, 여러분. 국립창원대학교 PRU 재직자 교육에 오신 것을 환영합니다. 오늘 우리가 함께 다룰 주제는 '문서 자동 처리 자동화'입니다.
PDF 문서에서 데이터를 추출하고, 요약하며, 결론을 자동으로 도출하는 파이프라인을 Python으로 직접 구현해보겠습니다. 실무에서 바로 활용 가능한 실전 중심의 교육이 될 거예요.

전체 과정은 크게 세 부분으로 나눌 수 있습니다. 먼저 기초 환경 설정과 PDF 추출, 그다음 자연어 처리와 요약 알고리즘, 마지막으로 결론 도출과 통합 프로젝트죠.
이론만 배우는 게 아니라 각 단계마다 실습 코드를 직접 작성하면서 진행할 겁니다. 실제로 여러분의 업무에 적용할 수 있는 수준까지 끌어올리는 게 목표예요.

이 과정의 대상자는 문서 기반 업무를 자동화하고 싶으신 분들입니다. Python을 조금이라도 다뤄보신 분이면 충분해요. 오늘 교육이 끝나면 여러분은 다섯 가지 핵심 역량을 갖추게 됩니다.
첫째, PDF에서 신뢰성 있게 텍스트와 메타데이터를 추출할 수 있어요. 둘째, 텍스트 전처리와 요약 알고리즘의 원리를 이해하게 되죠. 셋째, 추출적 요약과 추상적 요약을 모두 구현할 수 있습니다. 넷째, 키워드와 감성 분석으로 결론을 자동으로 도출하는 방법을 배우게 되고요. 마지막으로 이 모든 걸 하나의 파이프라인으로 통합하는 능력을 갖추게 됩니다.

그렇다면 왜 우리가 문서 자동화를 배워야 할까요? 현실을 한번 보시죠. 대량의 PDF 문서를 사람이 일일이 읽고 요약하는 건 시간도 오래 걸리고, 품질도 일정하지 않습니다.
정보가 너무 많아서 의사결정이 지연되는 경우도 흔하죠. 하지만 자동화 파이프라인을 구축하면 상황이 완전히 달라집니다. 반복 업무가 자동화되면서 생산성이 극적으로 향상되고요. 요약과 결론의 일관성도 확보할 수 있어요. 게다가 과거 데이터 아카이브를 분석해서 새로운 인사이트를 도출할 수도 있습니다. 실제로 처리 시간을 70% 이상 절감한 사례들이 많아요.

자, 이제 전체 프로세스를 한눈에 살펴볼까요? 입력은 PDF 파일입니다. 텍스트 기반일 수도 있고, 스캔본일 수도 있죠. 처리 흐름은 5단계예요.
1단계, PyPDF2나 pdfplumber로 텍스트와 메타데이터를 추출합니다. 2단계, 정규화와 토큰화, 불용어 제거 같은 전처리를 진행해요. 3단계가 핵심인데요, 요약 알고리즘을 적용하는 거죠. TF-IDF나 TextRank 같은 추출적 방법도 있고, Transformer 기반의 추상적 방법도 있습니다. 4단계에서는 키워드 추출과 감성 분석을 수행하고요. 마지막 5단계에서 결론을 도출하고 리포트를 생성합니다. 참고로 스캔본 PDF는 OCR 처리가 필요한데, 오늘은 선택사항으로 두겠습니다.

실습을 시작하기 전에 Python 환경부터 세팅해야겠죠? Python 3.10 이상을 권장합니다. 가상환경을 만드는 게 첫 단계예요. python -m venv .venv 명령어로 가상환경을 생성하세요.
그다음 활성화하는데요, 윈도우는 .venv\Scripts\activate, 맥이나 리눅스는 source .venv/bin/activate를 입력하시면 됩니다. pip와 wheel을 최신 버전으로 업데이트하고, Jupyter Notebook도 설치해두면 편리해요. 마지막으로 python -V로 버전을 확인해보세요.

이제 필요한 라이브러리들을 살펴보겠습니다. PDF 처리에는 PyPDF2와 pdfplumber를 쓸 거예요. 자연어 처리에는 regex, nltk, scikit-learn이 필요하고요.
요약 알고리즘 구현을 위해서는 scikit-learn으로 TF-IDF를, networkx로 TextRank를, transformers로 추상적 요약을 다룹니다. pandas, numpy, matplotlib은 데이터 처리와 시각화에 사용되고요. 선택적으로 OCR을 위한 pytesseract, 키워드 추출을 위한 yake, 한국어 토큰화를 위한 konlpy나 kiwipiepy도 설치할 수 있습니다.

첫 번째 PDF 라이브러리인 PyPDF2를 알아볼까요? 순수 Python으로 작성된 라이브러리로, 메타데이터와 페이지 접근에 최적화되어 있습니다. 설치는 간단해요. pip install PyPDF2 한 줄이면 끝입니다.
기본 사용법도 보시죠. PdfReader 객체를 만들고 파일을 열면, len(reader.pages)로 페이지 수를 확인할 수 있어요. 각 페이지는 인덱싱으로 접근하고, extract_text() 메서드로 텍스트를 추출합니다. 메타데이터 접근은 강력한데, 제목이나 작성자, 생성일 같은 정보를 쉽게 가져올 수 있어요.

두 번째 라이브러리는 pdfplumber입니다. PyPDF2보다 더 정밀한 추출이 가능한데요, 특히 표와 좌표 기반 데이터 추출에 강점이 있어요. 설치는 pip install pdfplumber로 하시면 됩니다.
사용법을 볼까요? with pdfplumber.open() 컨텍스트 매니저를 사용하면 파일이 자동으로 닫혀서 안전합니다. 각 페이지를 순회하면서 extract_text()를 호출하면 되는데요, None이 반환될 수 있으니 체크해야 해요. 보통 빈 페이지나 이미지만 있는 페이지에서 그렇죠. 누적된 텍스트를 출력해보면 처음 500자 정도 확인할 수 있습니다.

자, 이제 첫 실습입니다. 페이지를 순회하면서 텍스트를 추출하는 함수를 만들어볼게요. 절차는 간단해요. 파일을 열고, 페이지를 루프로 돌면서 텍스트를 누적하고, 마지막에 저장하는 거죠.
코드를 함께 보시죠. extract_text_basic 함수를 정의합니다. 빈 리스트를 만들고, pdfplumber로 파일을 열어요. 각 페이지를 enumerate로 순회하면서 텍스트를 추출하는데요, None 체크가 중요합니다. 텍스트가 있으면 리스트에 추가하고, 없으면 건너뛰죠. 마지막에 줄바꿈 문자로 조인해서 하나의 문자열로 만듭니다. Path 객체를 사용하면 파일 저장도 깔끔하게 처리할 수 있어요. utf-8 인코딩을 꼭 지정하세요.

실무에서는 예외 상황도 처리해야겠죠? 암호화된 파일이나 손상된 페이지를 만날 수 있습니다. 이번엔 더 안전한 버전을 만들어보겠습니다.
PyPDF2의 PdfReader를 사용하면 암호화 여부를 확인할 수 있어요. is_encrypted 속성으로 체크하고, 패스워드가 있으면 decrypt() 메서드를 호출합니다. 각 페이지 처리는 try-except 블록으로 감싸야 해요. 예외가 발생하면 경고를 출력하고 다음 페이지로 넘어갑니다. 빈 텍스트도 필터링해야 하는데요, strip() 메서드로 공백을 제거한 후 빈 문자열인지 확인하세요. 파일 저장할 때도 인코딩을 명시적으로 지정하는 게 좋습니다.

메타데이터는 문서 분류와 아카이빙에 매우 유용합니다. 제목, 작성자, 생성일, 주제 같은 정보를 프로그래밍 방식으로 추출할 수 있어요.
코드를 보시죠. PdfReader로 파일을 열고, metadata 속성에 접근합니다. info.title, info.author, info.creation_date 같은 방식으로 각 속성을 가져올 수 있어요. 페이지 크기도 확인 가능한데요, pages[0].mediabox를 출력해보면 너비와 높이가 나옵니다. 이런 메타데이터를 JSON으로 저장해두면 나중에 파이프라인에서 활용할 수 있어요. 문서 타입 분류나 자동 태깅에 특히 유용하죠.

이제부터는 추출한 텍스트를 분석하는 단계입니다. 자연어 처리의 핵심 개념들을 짚어볼게요. 문장 분할, 토큰화, 품사 태깅 같은 기초 작업이 있고요.
벡터화 기법도 중요합니다. Bag-of-Words나 TF-IDF로 텍스트를 숫자 벡터로 변환하죠. 코사인 유사도나 그래프 기반 순위 알고리즘도 활용할 거예요. 최신 기법으로는 BERT나 GPT 같은 사전학습 모델이 있는데, 파인튜닝을 통해 특정 도메인에 맞출 수 있습니다. 평가 지표로는 ROUGE가 요약 품질 측정에 많이 쓰이고, 키워드 추출에는 정밀도와 재현율을 봅니다.

"Garbage In, Garbage Out"이라는 말 들어보셨죠? 전처리가 엉망이면 분석 결과도 엉망입니다. 핵심 기법들을 알아보겠습니다.
정규화는 텍스트를 표준 형태로 만드는 거예요. 영문은 소문자로 통일하고, 공백과 특수문자를 정리합니다. 토큰화는 단어나 문장 단위로 분리하는 건데요, 공백이나 문장부호를 기준으로 하죠. 한국어는 형태소 기반 토큰화가 더 효과적이에요. 불용어 제거도 중요합니다. 조사, 접속사, 의미 없는 단어들을 걸러내야 해요. 표제어 추출이나 어간 추출은 영어에선 필수지만, 한국어는 선택적입니다. 실무 팁으로는 PDF에서 추출할 때 페이지 머리말이나 바닥글을 제거하는 로직도 추가하면 좋아요.

토큰화와 정규화는 자주 혼동되는 개념인데요, 명확히 구분해봅시다. 토큰화는 텍스트를 의미 단위로 분리하는 겁니다. 공백이나 구두점을 기준으로 나누죠. 한국어는 조사가 결합되어 있어서 품질에 영향을 미칩니다.
정규화는 형태를 통일하는 작업이에요. 소문자로 바꾸고, 숫자 포맷을 맞추고, 특수문자를 처리하고, 공백을 정리하는 거죠. 둘 다 전처리에 필수적이지만 목적이 다릅니다. 토큰화는 '분리'에, 정규화는 '표준화'에 초점을 맞추고 있어요.

자, 실제로 한글 텍스트 전처리 코드를 작성해보겠습니다. 간단한 정규식 토큰화와 불용어 제거를 구현할 거예요.
먼저 불용어 집합을 정의합니다. '그리고', '그러나', '하지만' 같은 단어들이죠. normalize 함수는 탭과 캐리지 리턴을 공백으로 바꾸고, 연속된 공백을 하나로 압축합니다. tokenize_ko 함수는 정규식으로 한글 2글자 이상, 영문 2글자 이상을 추출해요. 그리고 불용어를 필터링하죠. 실제로 돌려보면, 파일에서 텍스트를 읽어서 정규화하고, 토큰화한 결과의 처음 50개를 출력합니다. 이 패턴을 기반으로 여러분의 도메인에 맞게 커스터마이징하면 돼요.

본격적인 요약 단계로 넘어가볼까요? 요약 알고리즘은 크게 두 종류입니다. 추출적 요약은 원문 문장을 선별해서 재조합하는 방식이에요. TF-IDF나 TextRank가 여기 속하죠.
추상적 요약은 새로운 문장을 생성합니다. Transformer 기반 모델이 대표적이에요. 어떤 걸 선택할까요? 데이터 양과 도메인, 성능 요구사항, 비용, 배포 환경을 고려해야 합니다. 현업에서는 보통 하이브리드 전략을 씁니다. 빠른 1차 추출적 요약으로 후보를 좁히고, 중요한 문서만 고품질 추상적 요약을 적용하는 거죠.

두 방식을 직접 비교해보겠습니다. 추출적 요약의 장점은 속도가 빠르고, 재현 가능하며, 자원이 적게 든다는 거예요. 단점은 문장 연결이 부자연스럽고, 문맥을 재구성하는 데 한계가 있죠. 규정이나 보고서에서 핵심 문장을 뽑을 때 적합합니다.
추상적 요약은 유창성이 뛰어나고, 문맥을 잘 이해해서 요약합니다. 하지만 계산 비용이 크고, 환각(Hallucination) 위험이 있어요. 없는 내용을 지어내는 거죠. 기사나 리뷰, 서술형 문서에 적합합니다. 실무에서는 두 방식의 장단점을 이해하고, 상황에 맞게 선택하는 게 중요해요.

첫 번째 추출적 요약 알고리즘인 TF-IDF를 깊이 파고들어봅시다. 아이디어는 간단합니다. 문장을 토큰 벡터로 표현하고, TF-IDF 점수로 중요도를 매기는 거죠.
절차를 보시죠. 1단계, 정규식으로 문장을 분할합니다. 마침표나 느낌표 같은 종결 기호를 기준으로 하죠. 2단계, TF-IDF 벡터화를 수행해요. scikit-learn의 TfidfVectorizer가 이걸 자동으로 해줍니다. 3단계, 각 문장의 점수를 계산합니다. 문장에 포함된 단어들의 TF-IDF 값을 평균이나 합계로 내는 거예요. 4단계, 점수를 내림차순 정렬해서 상위 N개를 선택하고요. 5단계, 원문 순서대로 재정렬해서 자연스럽게 만듭니다. 주의할 점은 중복 문장 처리인데요, MMR 알고리즘이나 유사도 임계값으로 해결할 수 있어요.

이제 TF-IDF 요약을 직접 구현해보겠습니다. scikit-learn을 활용하면 생각보다 간단해요.
먼저 split_sentences 함수로 문장을 분리합니다. 정규식으로 마침표, 느낌표, 물음표, 한국어 종결 문자들을 기준으로 나누죠. summarize_tfidf 함수가 핵심인데요, 문장 리스트를 만들고 TfidfVectorizer를 생성합니다. token_pattern으로 한글 2글자 이상, 영문 2글자 이상만 추출하도록 설정해요. 벡터화한 결과의 평균을 구해서 문장별 점수를 계산하고요. argsort로 점수가 높은 순으로 인덱스를 뽑은 다음, 상위 5개만 선택합니다. 원문 순서를 유지하기 위해 인덱스를 정렬하고, 해당하는 문장들을 반환하죠. 실행하면 핵심 문장 5개가 출력됩니다.

두 번째 추출적 요약 알고리즘은 TextRank입니다. PageRank를 텍스트에 적용한 건데요, 그래프 기반 접근법이에요. 문장을 노드로, 유사도를 엣지로 표현하는 거죠.
절차를 보시면, 1단계는 문장을 TF-IDF로 벡터화합니다. 2단계, 문장 간 코사인 유사도 행렬을 만들어요. 3단계가 핵심인데, 이 행렬을 그래프로 변환하고 PageRank 알고리즘을 돌립니다. 각 문장이 얼마나 중요한지 점수가 매겨지는 거죠. 4단계, 상위 N개 문장을 선택하고 중복을 제거합니다. TextRank는 TF-IDF보다 문장 간 관계를 더 잘 고려하기 때문에, 맥락이 중요한 문서에 효과적이에요.

TextRank 구현도 함께 해보죠. networkx 라이브러리가 필요합니다.
코드 구조는 TF-IDF와 비슷한데요, 차이점은 유사도 행렬을 만든다는 겁니다. 문장을 벡터화한 후, cosine_similarity로 문장 간 유사도를 계산해요. 대각선은 0으로 채워야 하는데, 자기 자신과의 유사도는 의미가 없으니까요. 이 행렬을 networkx 그래프로 변환하고, pagerank_numpy로 점수를 계산합니다. 점수와 인덱스를 튜플로 만들어 정렬하고, 상위 5개를 선택하죠. 마지막으로 원문 순서대로 재정렬해서 반환합니다. 실행하면 그래프 기반으로 추출된 핵심 문장들을 볼 수 있어요.

이제 추상적 요약으로 넘어가겠습니다. Transformer 기반 모델은 게임 체인저예요. BERT, GPT 같은 사전학습 모델을 요약 태스크에 파인튜닝하거나, Zero-shot으로 바로 쓸 수도 있습니다.
장점은 분명합니다. 고품질이고 자연스러운 요약을 생성하죠. 단점은 자원 요구량이 크고, 환각 위험이 있다는 거예요. 실무 팁을 드리면, 입력 길이 제한을 고려해서 chunking 전략을 세워야 하고요. temperature 파라미터로 창의성을 조절할 수 있습니다. 중요한 문장은 프롬프트나 가중치로 보호하는 것도 방법이에요.

Hugging Face의 transformers 라이브러리로 추상적 요약을 구현해보겠습니다. 한국어 모델을 우선적으로 시도할 거예요.
코드를 보시죠. 모델 후보 리스트를 만듭니다. 'gogamza/kobart-summarization'은 한국어 특화 모델이고, 'facebook/bart-large-cnn'은 영문용이죠. for 루프로 순회하면서 pipeline 객체 생성을 시도해요. 성공하면 break로 빠져나오고, 실패하면 다음 모델을 시도합니다. 텍스트는 3000자로 제한하는데, 대부분 모델이 입력 길이 제한이 있기 때문이에요. max_length=200, min_length=60으로 요약 길이를 조절하고, do_sample=False로 결정적 출력을 보장합니다. 결과의 'summary_text' 키로 요약문을 추출할 수 있어요.

실무에서 Hugging Face를 쓸 때 체크리스트를 공유하겠습니다. 모델 선택부터 보안까지 네 가지 영역이 있어요.
모델 선택 시에는 언어, 도메인, 라이선스를 반드시 확인하세요. 성능과 비용을 고려할 때는 GPU를 활용하고, 배치나 청크 처리를 통해 효율을 높여야 합니다. 배포 단계에서는 토크나이저 캐싱으로 속도를 올리고, ONNX나 Quantization으로 모델을 경량화할 수 있어요. 데이터 보안도 중요합니다. 민감정보는 마스킹하고, 로그를 관리하며, 프롬프트 인젝션 같은 공격에 대비해야 해요.

이제 지금까지 배운 걸 통합할 시간입니다. 전체 파이프라인의 첫 번째 파트를 만들어보죠.
파이프라인 구성은 간단합니다. 입력은 PDF 경로고, 단계는 추출, 전처리, 요약 선택이에요. 출력은 요약문과 핵심 문장 파일이죠. extract_pdf_text 함수는 pdfplumber로 모든 페이지를 순회하면서 텍스트를 추출합니다. None 처리를 잊지 마세요. normalize 함수는 탭과 캐리지 리턴을 제거하고, 연속 공백을 압축합니다. 실제 사용은 간단해요. PDF에서 텍스트 추출하고, 정규화한 다음, 파일로 저장하면 끝입니다.

파이프라인의 두 번째 파트는 요약 모드 선택과 실행입니다. 세 가지 모드를 지원할 거예요.
summarize 함수를 보시죠. mode 파라미터로 'tfidf', 'textrank', 'transformer' 중 하나를 선택합니다. tfidf 모드는 앞서 만든 summarize_tfidf 함수를 호출하고, 5개 문장을 줄바꿈으로 조인해요. textrank도 마찬가지고요. transformer 모드는 조금 다릅니다. transformers 라이브러리를 import하고, pipeline을 생성한 다음, 3000자로 제한해서 요약합니다. 실행은 간단해요. 저장된 텍스트 파일을 읽고, 원하는 모드로 요약하고, 결과를 파일로 저장하면 됩니다. 이렇게 하면 상황에 따라 유연하게 요약 방식을 바꿀 수 있어요.

요약을 넘어서 결론을 자동으로 도출하는 단계로 넘어가겠습니다. 핵심 아이디어는 여러 신호를 결합하는 거예요.
데이터 소스는 세 가지입니다. 요약문, 추출된 키워드, 감성이나 주제 신호죠. 이걸 결합해서 구조화된 결론 템플릿을 만듭니다. 결론에는 어떤 게 들어갈까요? 핵심 결론 한 문장, 근거 문장 3개 정도, 그리고 권고사항 2개 정도가 적절해요. 각 근거는 원문의 출처나 페이지 번호와 연결되어야 하고요. 키워드는 가중치와 함께 표시되며, 감성 신호는 긍정/부정/중립으로 분류됩니다.

키워드 추출은 두 가지 방법이 있습니다. YAKE와 TF-IDF죠. YAKE는 비지도 학습 알고리즘으로, 통계적 특징을 활용해요.
코드를 보시죠. yake 라이브러리를 import하고, KeywordExtractor를 생성합니다. lan='ko'로 한국어를 지정하고, n=2는 2-gram까지 추출한다는 뜻이에요. top=20으로 상위 20개를 가져옵니다. extract_keywords 메서드를 호출하면 키워드와 점수가 튜플로 반환돼요. 점수가 낮을수록 중요한 키워드입니다. 상위 10개를 출력해보면 문서의 핵심 토픽을 파악할 수 있죠. 대안으로 TfidfVectorizer로 상위 토큰을 추출하는 방법도 있어요.

감성 분석은 텍스트의 긍정/부정 성향을 파악하는 기술입니다. 결론 도출에서 권고 사항의 톤을 조정하는 데 활용할 수 있어요.
간단한 사전 기반 방법을 보여드릴게요. 긍정 단어 집합과 부정 단어 집합을 미리 정의합니다. '우수', '개선', '만족' 같은 긍정 단어와 '문제', '지연', '오류' 같은 부정 단어죠. sentiment_score 함수는 텍스트에서 각 단어가 몇 번 등장하는지 세고, 긍정이 많으면 '긍정', 부정이 많으면 '부정', 같으면 '중립'을 반환합니다. 실제 프로젝트에서는 transformers pipeline으로 한국어 감성 모델을 로드해서 더 정교하게 분석할 수도 있어요.

마지막 실습입니다. 요약문, 키워드, 원문을 결합해서 구조화된 결론을 만들어보겠습니다.
로직을 보시죠. 먼저 파일에서 텍스트, 요약문, 키워드를 불러옵니다. 키워드는 YAKE 결과를 넣으면 되고요. 원문을 문장으로 분리하고, 각 키워드가 포함된 문장을 찾아서 근거로 사용합니다. 상위 5개 키워드에 대해 루프를 돌면서 매칭되는 첫 문장을 evidence 리스트에 추가해요. 결론 객체는 JSON 구조로 만듭니다. 'core_conclusion'에는 요약문의 첫 문장을, 'evidence'에는 근거 3개를, 'recommendations'에는 권고사항 2개를 넣죠. 마지막으로 JSON 파일로 저장하면 끝입니다. 한글이 깨지지 않도록 ensure_ascii=False를 꼭 지정하세요.

지금까지 배운 모든 걸 하나의 프로젝트로 통합해봅시다. 5개 마일스톤으로 나눠서 진행할 거예요.
M1은 PDF 추출 모듈입니다. extract.py 파일을 만들어서 pdfplumber 기반 추출 함수를 구현하세요. M2는 전처리와 문장 분할이에요. preprocess.py에 정규화와 토큰화 로직을 넣습니다. M3은 요약기 선택과 구현이죠. summarize.py에서 TF-IDF, TextRank, Transformer 모드를 모두 지원하도록 만드세요. M4는 키워드와 감성 분석입니다. insights.py에 YAKE 기반 키워드 추출과 감성 점수 계산을 넣으면 돼요. M5는 결론 리포팅인데, JSON이나 Markdown 형식으로 최종 리포트를 생성하는 겁니다. 프로젝트 구조는 src 폴더에 모듈들을 넣고, data 폴더에 입출력 파일을, main.py는 CLI 엔트리포인트로 만드는 게 좋아요.

실제로 이런 파이프라인이 어떻게 쓰이는지 사례를 보여드리겠습니다.
사례 1은 법률 분야예요. 계약서에서 핵심 조항을 자동으로 요약하고, '위약금', '해지', '손해배상' 같은 리스크 키워드를 추출합니다. 사례 2는 연구 분야죠. 연구 보고서의 초록을 자동 생성해서 연구자의 시간을 절약해줍니다. 사례 3은 회의록 요약인데요, 회의 내용을 요약하고 액션 아이템을 자동으로 도출해요. 사례 4는 고객 서비스 분야입니다. 민원이나 리뷰를 요약하고, 감성 추세를 분석해서 대응 우선순위를 정하죠. 실제 성과를 보면 처리 시간을 평균 70% 절감했고, 일관성도 크게 향상됐습니다.

자, 오늘 50분 동안 정말 많은 걸 배우셨습니다. 핵심만 정리해볼까요?
우리는 PDF에서 텍스트를 추출하고, 전처리하고, 요약하고, 키워드와 감성을 분석해서 결론을 도출하는 전체 파이프라인을 만들었습니다. 추출적 요약과 추상적 요약의 장단점을 이해했고, 실무에서 어떻게 조합해서 쓰는지도 배웠죠. 다음 단계로는 여러분의 도메인 데이터로 모델을 튜닝하거나, 경량화해서 실제 서비스에 배포하는 걸 고려해보세요. 질문 있으신 분 계신가요? 이메일이나 슬랙으로도 언제든 연락 주시면 됩니다. 오늘 고생 많으셨고, 감사합니다!
