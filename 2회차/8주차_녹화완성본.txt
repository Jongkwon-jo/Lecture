이겸
안녕하세요, 국립창원대학교 PRU 재직자 교육 과정에 오신 것을 환영합니다. 오늘은 '구조화 데이터 분석'이라는 주제로, KPI 분석부터 인사이트 생성까지 전 과정을 함께 실습해볼 예정입니다.
Python을 활용한 데이터 분석과 대시보드 구축까지, 실무에서 바로 활용 가능한 역량을 키우는 시간이 될 거예요. 

먼저 오늘 실습을 통해 어떤 역량을 갖추게 되는지 살펴보겠습니다. 크게 네 가지 핵심 영역으로 구성되어 있어요.
첫째, 핵심 KPI 정의 및 계산입니다. 매출, 마진, 재구매율 같은 비즈니스 성장에 필수적인 지표들을 직접 설계하고 계산하는 방법을 배웁니다.
둘째, 데이터 전처리 실습이죠. 결측치와 이상치 처리, 데이터 타입 변환 등 실제 분석이 가능한 형태로 데이터를 정제하는 노하우를 익히게 됩니다.
셋째는 EDA, 즉 탐색적 데이터 분석을 통한 인사이트 도출입니다. 데이터 속에 숨겨진 패턴을 발견하고, 통계 검정으로 가설을 검증하는 과정을 경험하실 거예요.
마지막으로 Streamlit을 활용한 대시보드 구축입니다. 분석 결과를 실시간으로 공유할 수 있는 인터랙티브 웹 대시보드를 Python만으로 구현해보겠습니다.

오늘의 전체 로드맵을 살펴볼까요? 데이터 로딩부터 대시보드 구축까지 7단계로 구성되어 있습니다.
Step 1은 데이터와 환경 소개입니다. 실습에 사용할 orders.csv 파일의 구조와 분석 환경을 먼저 확인할 거예요.
Step 2는 데이터 전처리 단계죠. 결측치, 이상치, 타입 변환을 실습하면서 데이터를 분석 가능한 상태로 만듭니다.
Step 3에서는 EDA를 통해 데이터의 패턴을 파악하고 기초 시각화를 진행합니다. Step 4에서는 매출, 마진, 재구매율 같은 핵심 KPI를 설계하고 계산해볼 거예요.
Step 5는 통계 검정과 고객 세분화 분석입니다. 가설을 검증하고 RFM 분석을 통해 고객을 세분화하는 방법을 배우게 됩니다.
Step 6에서는 Streamlit을 활용해 미니 대시보드를 제작하고, 마지막으로 전체 과정을 복습하며 인사이트 도출 미니 프로젝트를 수행합니다.

이제 실습에 사용할 데이터를 소개하겠습니다. 가상의 이커머스 주문 데이터인 orders.csv 파일을 활용할 예정이에요.
이 데이터는 총 12개의 컬럼으로 구성되어 있습니다. 주문ID, 주문일자, 고객ID부터 시작해서 상품명, 카테고리, 가격, 수량 정보가 포함되죠.
할인율과 원가 데이터도 있어서 마진 계산이 가능합니다. 지역, 유입 채널, 주문 상태 정보까지 포함되어 있어요.
약 5,000건의 주문 기록이 담겨 있으며, 실무 상황을 재현하기 위해 일부러 결측치와 이상치를 포함시켰습니다. 우리가 해결해야 할 과제인 거죠.
주요 분석 과업은 네 가지입니다. 총 매출과 마진율 계산, AOV와 재구매율 같은 고객 지표 산출, 월별 및 계절별 매출 추이 분석, 그리고 카테고리와 채널별 성과 비교예요.

실습을 원활하게 진행하기 위한 환경 설정을 알아보겠습니다. 먼저 Python 3.10 이상 버전이 필요해요.
필수 라이브러리는 여러 개 있습니다. 데이터 처리를 위한 pandas와 numpy, 시각화를 위한 mat-plot-lib과 sea-born이 기본이죠.
통계 검정용으로 scipy, 머신러닝용으로 scikit-learn, 그리고 대시보드 구축을 위한 streamlit까지 준비해야 합니다.
IDE는 VS Code나 Jupyter를 권장합니다. 그리고 매우 중요한 점은 가상환경을 사용하는 거예요.
v-env나 conda로 가상환경을 만들어서 의존성 충돌을 방지하는 것이 필수적입니다. 프로젝트 폴더 구조는 화면에 보이는 것처럼 루트 폴더 아래 가상환경, 데이터 폴더, 결과 저장용 폴더를 만들고 분석 노트북과 앱 파일을 배치하면 됩니다.

이제 실제 코드 레벨에서 환경을 구성해보겠습니다. 터미널에서 먼저 가상환경을 생성하고 활성화해야 해요.
Windows에서는 .v-env, Scripts, activate, Mac이나 Linux에서는 source .v-env, bin, activate 명령어를 사용합니다.
가상환경이 활성화되면 pip install 명령어로 필요한 라이브러리를 한 번에 설치할 수 있어요. pandas, numpy, matplotlib, seaborn, scipy, scikit-learn, streamlit을 모두 설치합니다.
Python 파일 상단에는 이렇게 패키지를 임포트하면 됩니다. pandas는 pd로, numpy는 np로 줄여서 쓰는 게 관례죠.
시각화 관련해서는 맷-plot-lib, pi-plot을 PLT로, sea-born을 sns로 임포트합니다. 통계 검정을 위해 sci-py의 stats 모듈도 가져오고요.
한글 폰트 설정도 중요합니다. Windows에서는 '맑은 고딕'을 지정하고, unicode 마이너스를 False로 설정해야 마이너스 기호가 제대로 표시됩니다.

본격적으로 데이터를 불러와서 구조를 확인해보겠습니다. pandas의 read_csv 함수로 데이터를 로드하면 되죠.
상위 3개 행만 확인하려면 head(3)을 사용합니다. 데이터가 어떻게 생겼는지 빠르게 파악할 수 있어요.
shape 속성으로 행과 열의 개수를 확인하면 5,000개 행, 12개 열이라는 걸 알 수 있습니다. columns.to-list()를 사용하면 컬럼 목록을 리스트로 볼 수 있고요.
주문 상태값의 분포도 체크해봐야 합니다. value_counts 함수를 사용하면 각 상태별로 몇 건씩 있는지 집계할 수 있어요.
결과를 보면 Completed가 4,250건으로 가장 많고, Refunded가 350건, Pending이 200건 정도 됩니다. Cancelled도 150건 있고, Nan, 즉 결측치도 50건 발견되네요.
이런 초기 점검을 통해 데이터의 전반적인 상태를 빠르게 파악할 수 있습니다.

데이터 타입과 결측치를 더 체계적으로 확인해보겠습니다. info() 메서드를 사용하면 각 컬럼의 데이터 타입과 Non-Null 개수를 한눈에 볼 수 있어요.
여기서 중요한 체크 포인트가 몇 가지 있습니다. order_date가 object 타입, 즉 문자열로 되어 있네요.
날짜 계산을 하려면 반드시 date-time으로 변환해야 합니다. category 컬럼은 4,950개만 Non-Null이라서 50개의 결측치가 있다는 걸 알 수 있죠.
이제 수치형 변수의 기술통계를 살펴볼까요? describe() 메서드에 numeric_only는 True 옵션을 주고, .T로 전치시키면 가독성이 좋아집니다.
여기서 발견할 수 있는 인사이트가 몇 가지 있어요. price의 최댓값이 50,000으로 평균 대비 엄청 크죠.
quantity도 최대 100인데 평균이 1.2 정도라서 극단적인 이상치일 가능성이 높습니다. discount 컬럼은 count가 4,800이라서 200개의 결측치가 있다는 것도 확인됩니다.
이렇게 기술통계를 통해 데이터의 분포와 이상 신호를 미리 포착할 수 있습니다. 다음 단계에서 본격적으로 처리하면 되죠.

결측치를 체계적으로 처리하는 방법을 알아보겠습니다. 3단계 프로세스로 접근하면 효과적이에요.
첫 번째는 탐지 단계입니다. is, na, .sum을 사용하면 각 컬럼별로 결측치가 몇 개 있는지 집계할 수 있죠.
sort_values로 내림차순 정렬하면 어느 컬럼에 결측치가 집중되어 있는지 한눈에 파악됩니다.
두 번째는 처리 전략을 수립하는 단계예요. 결측 비율이 5% 미만으로 매우 낮거나 데이터가 충분하면 제거를 고려할 수 있습니다.
하지만 데이터 손실을 막아야 할 때는 대체 방법을 선택해야 하죠. 수치형 변수는 평균이나 중앙값으로, 범주형 변수는 최빈값이나 'Unknown' 같은 새로운 라벨로 채웁니다.
세 번째는 실제 대체 작업입니다. 수치형 컬럼들은 median()을 사용해서 중앙값으로 채우는 게 좋아요.
이상치의 영향을 덜 받기 때문입니다. 범주형 변수인 카테고리는 'Unknown'이라는 명시적인 라벨을 부여하면 됩니다.
여기서 주의할 점이 있어요. 특정 패턴이 있는 결측치는 무작위 결측이 아닐 수 있습니다.
예를 들어 배송 실패로 인한 결측이라면, 단순 대체 전에 도메인 관점에서 발생 원인을 먼저 고민해봐야 해요.

데이터 타입을 적절하게 변환하는 것도 중요한 전처리 과정입니다. 왜 필요한지부터 이해해볼까요?
날짜 컬럼이 문자열 상태로 있으면 연도나 월 추출이 불가능합니다. pd.to-date-time()으로 변환해야 시계열 기능을 활용할 수 있죠.
범주형 변수도 마찬가지예요. '배송완료' 같은 문자열이 계속 반복되면 메모리 낭비가 심합니다.
astype('category')로 변환하면 문자열을 정수형 ID로 매핑해서, 메모리 사용량을 최대 90%까지 줄일 수 있어요.
코드를 보면 먼저 order_date를 date-time으로 변환합니다. 그다음 카테고리, region, channel, status 같은 범주형 컬럼들을 for 루프로 돌면서 카테고리 타입으로 바꾸죠.
월별 트렌드 분석을 위해 파생변수도 하나 만들어야 합니다. dt.to-period('M')를 사용하면 날짜를 '2025, 01' 형태로 변환할 수 있어요.
이렇게 하면 groupby 집계가 훨씬 편해집니다. 결과를 확인해보면 메모리 사용량이 1.2Mega-byte에서 684.2 킬로바이트로 줄어든 걸 볼 수 있죠.

이상치를 체계적으로 찾아내고 처리하는 방법을 알아보겠습니다. IQR, 즉 사분위수 범위를 활용한 방법이에요.
개념부터 설명하면, Q-one은 25번째 백분위수, Q-three는 75번째 백분위수입니다. 이 둘의 차이가 IQR이죠.
정상 범위는 Q-one에서 1.5배 IQR을 뺀 값부터, Q-three에 1.5배 IQR을 더한 값까지로 정의합니다. 이 범위를 벗어나면 이상치로 간주하는 거예요.
코드를 보면 price와 quantity 컬럼에 대해 for 루프를 돌립니다. 각 컬럼의 quantile(0.25)와 quantile(0.75)로 Q-one과 Q-three를 계산하죠.
하한선과 상한선을 구한 다음, clip 함수로 이상치를 경계값으로 대체합니다. 제거하는 게 아니라 조정하는 방식이에요.
결과를 보면 price의 최댓값이 12,000에서 182.5로, quantity는 100에서 6으로 줄어듭니다. 과도한 이상치는 평균을 왜곡하기 때문에 KPI 계산 전에 이런 처리가 필수적입니다.

이제 비즈니스 로직을 적용해서 핵심 지표를 계산할 파생변수를 만들어보겠습니다. 네 가지 변수가 필요해요.
첫 번째는 매출입니다. 단가에 수량을 곱하고 거기에 1에서 할인율을 뺀 값을 곱하면 됩니다.
할인율이 결측치인 경우는 Fill-Na-0으로 처리해줘야 해요.
두 번째는 총 비용이죠. 원가에 수량을 곱하면 간단하게 구할 수 있습니다.
세 번째는 마진입니다. 매출에서 총 비용을 빼면 되는데, 이게 우리가 실제로 벌어들인 순이익이에요.
네 번째는 마진율입니다. 여기서 주의해야 할 점이 있습니다. 매출로 나누는 계산이라서 매출이 0인 경우 에러가 발생할 수 있어요.
따라서, np.where를 사용해서 매출이 0보다 클 때만 나누고, 그렇지 않으면 nan으로 처리하는 예외 처리가 필수적입니다. 이렇게 하면 안전하게 모든 KPI 계산의 기초가 되는 파생변수들이 준비됩니다.

본격적인 탐색적 분석을 시작해봅시다. 먼저 시계열 패턴을 파악하기 위해 월별 매출 추이를 그려볼게요.
order_month로 그룹화해서 revenue를 합계 낸 다음, 라인 플롯으로 시각화하면 됩니다.
이런 차트에서 우리가 읽어내야 할 인사이트는 세 가지입니다. 첫째, 전반적인 성장 추세가 보이는가?
둘째, 특정 월이나 분기에 반복되는 계절성 패턴이 있는가? 셋째, 급격한 매출 변화나 스파이크가 있다면 그 원인은 무엇인가?
이렇게 시간의 흐름에 따른 변화를 먼저 파악해야 다음 분석의 방향을 잡을 수 있습니다.

이번에는 카테고리별 매출 기여도를 살펴보겠습니다. 어떤 제품군이 매출을 이끌고 있는지 파악하는 거죠.
카테고리로 그룹화해서 매출을 합산하고, 상위 10개만 추출해서 바 차트로 그립니다.
여기서 확인해야 할 포인트는 파레토 법칙이에요. 상위 20%의 카테고리가 전체 매출의 80%를 차지하는가를 체크해야하죠.
특정 카테고리에 대한 의존도가 너무 높으면 위험할 수 있습니다. 포트폴리오 균형을 점검해야 합니다.
반대로 매출은 적지만 성장률이 높은 틈새 카테고리를 발견하는 것도 중요합니다. 잠재력 있는 롱테일 상품을 찾아낼 수 있거든요.

비즈니스 성장을 측정하는 7가지 핵심 지표를 정의해보겠습니다. 명확한 KPI 정의는 분석의 출발점이에요.
첫 번째는 매출입니다. revenue의 합계로, 할인 후 총 판매 금액을 의미하죠.
두 번째는 주문수입니다. order_id의 고유값 개수로, 총 결제 완료 건수를 나타냅니다.
세 번째는 고객수예요. customer_id의 고유값 개수로, 기간 내 구매한 고객의 수를 집계합니다.
네 번째는 AOV, 즉 객단가입니다. 매출을 주문수로 나눈 값으로, 주문 한 건당 평균 결제액을 보여주죠.
다섯 번째는 ARPU입니다. 매출을 고객수로 나눈 값으로, 고객 1인당 평균 매출액을 의미해요.
여섯 번째는 마진율입니다. 총 마진을 총 매출로 나눈 비율로, 수익성을 판단하는 핵심 지표죠.
마지막은 재구매율입니다. 2회 이상 구매한 고객의 비율로, 고객 충성도를 나타내는 중요한 지표예요.

실제 Python 코드로 이 지표들을 계산해보겠습니다. 단계별로 살펴볼게요.
먼저 기본 집계부터 합니다. n-unique()로 주문수와 고객수를 세고, sum()으로 매출과 마진을 합산하죠.
다음은 비율 지표를 계산합니다. AOV는 매출을 주문수로 나누는데, 0으로 나누는 에러를 방지하기 위해 max(orders, 1)을 사용해요.
ARPU도 같은 방식으로 고객수로 나눕니다. 마진율은 매출이 0보다 클 때만 계산하고, 아니면 nan으로 처리하죠.
재구매율 계산이 조금 복잡합니다. customer_id로 그룹화해서 각 고객의 주문 건수를 세고, 2회 이상인 고객의 비율을 mean()으로 구하면 됩니다.
결과를 보면 매출이 약 1,254만원, 주문수 4,500건, AOV 2787원 정도 나오네요. 마진율은 15.4%, 재구매율은 24.5%입니다.
이런 숫자들을 대시보드에 표시하면 한눈에 비즈니스 상태를 파악할 수 있어요.

월별로 이 지표들이 어떻게 변화하는지 추적해봅시다. groupby로 order_month 단위로 집계하면 됩니다.
agg 함수를 사용해서 매출은 합계, 주문수와 고객수는 고유값 개수, 마진도 합계로 계산하죠.
그다음 AOV와 마진율 같은 파생 지표를 추가로 계산합니다. 테이블을 보면 7월부터 12월까지의 추이가 보이네요.
11월과 12월에 매출이 크게 증가했는데, 이건 시즌 특수일 가능성이 높습니다. 그런데 마진율이 다소 하락했어요.
할인 프로모션을 많이 진행했기 때문일 겁니다. 이렇게 월별 추이를 표로 정리하면 패턴을 빠르게 파악할 수 있습니다.

테이블로 보는 것도 좋지만 시각화하면 더 직관적입니다. 매출과 AOV를 나란히 그려서 비교해볼게요.
subplots으로 1행 2열의 차트를 만들고, 각각 라인 플롯으로 그리면 됩니다.
여기서 중요한 분석 포인트는 트렌드 동조화입니다. 매출과 AOV가 함께 상승하면 이상적인 성장이에요.
반대로 트렌드 이탈이 보인다면, 즉 매출은 늘지만 AOV가 급락한다면 저가 할인 위주 판매일 가능성이 높습니다.
특정 시즌에만 AOV가 높은지도 확인해야 해요. 예를 들어 연말에는 선물 구매로 고가 상품 판매가 늘 수 있거든요.

할인율이 실제로 매출에 어떤 영향을 미치는지 산점도로 확인해보겠습니다. 상관관계를 시각적으로 파악하는 거죠.
데이터 샘플을 추출해서 x축에 할인율, y축에 주문 매출을 놓고 scatterplot을 그립니다. alpha 값으로 투명도를 조절하면 밀도도 볼 수 있어요.
해석할 때 세 가지를 체크해야 합니다. 첫째, 비선형 관계가 있는가?
할인이 늘어난다고 매출이 정비례하지 않을 수 있습니다. 특정 구간에 점이 몰려 있는지 확인하세요.
둘째, 점이 뭉쳐 있는 곳이 주력 판매 구간입니다. 셋째, 극단치를 체크해야 해요.
할인 없이 매출이 매우 높거나, 과도한 할인 사례가 있는지 점검하는 게 중요합니다.

이제 통계 검정으로 할인 효과를 검증해보겠습니다. 독립표본 t-검정을 사용할 거예요.
귀무가설은 '할인 여부와 평균 매출 간에 차이가 없다'입니다. scipy.stats의 ttest_ind 함수를 사용하죠.
먼저 그룹을 분리합니다. 할인이 0인 그룹과 0보다 큰 그룹으로 나누는 거예요.
Welch's T-test를 적용하기 위해 equal_var는 False로 줍니다. 결과를 보면 할인 적용 그룹의 평균이 1,850원, 미적용 그룹이 1,200원이네요.
T-statistic이 5.234이고 p-value가 0.0000, 즉 거의 0에 가깝습니다. p-value가 0.05보다 훨씬 작으므로 귀무가설을 기각합니다.
결론은 명확합니다. 할인이 적용된 주문의 평균 매출액이 통계적으로 유의하게 높다는 거죠.

고객 행동을 시간의 흐름에 따라 추적하는 코호트 분석을 알아보겠습니다. 리텐션을 측정하는 핵심 프레임워크예요.
코호트란 특정 시점에 공통된 경험을 한 고객 그룹을 의미합니다. 예를 들어 2024년 1월에 첫 구매한 고객들을 하나의 코호트로 묶는 거죠.
이 그룹이 2월, 3월, 4월로 시간이 지나면서 얼마나 재구매하는지 추적하면 고객 충성도를 파악할 수 있습니다.

코호트 분석을 실제로 구현해보겠습니다. 세 단계로 진행됩니다.
첫째, 각 고객의 최초 구매월을 찾습니다. groupby로 customer_id별, order_date의 최솟값을 구하면 되죠.
둘째, 경과 기간을 계산합니다. 주문월에서 최초 구매월을 빼면 몇 개월이 지났는지 알 수 있어요.
셋째, 피벗 테이블로 리텐션 매트릭스를 만듭니다. 코호트별로 각 경과 기간에 몇 명이 재구매했는지 집계하는 거죠.
히트맵으로 시각화하면 어느 코호트의 리텐션이 높은지, 몇 개월 차에 이탈이 많은지 한눈에 보입니다.

고객을 가치에 따라 세분화하는 RFM 분석을 해보겠습니다. Recency, Frequency, Monetary, 세 가지 축으로 평가하는 거예요.
Recency는 최근성입니다. 마지막 구매일로부터 며칠이 지났는지 계산하죠.
Frequency는 구매 빈도입니다. 총 주문 건수로 측정합니다.
Monetary는 금액입니다. 총 구매 금액으로 계산해요.
세 점수를 합산하면 RFM 총점이 나옵니다. 이걸로 고객을 VIP, 일반, 이탈-위험군 같은 세그먼트로 나눌 수 있죠.

RFM 점수에 따라 고객별 맞춤 전략을 수립할 수 있습니다. 세 가지 주요 세그먼트를 살펴볼게요.
VIP 고객은 RFM 점수가 모두 높은 그룹입니다. 최근에도 자주 많이 구매하는 최고 가치 고객이죠.
이들에게는 VIP 전용 혜택이나 프리미엄 서비스를 제공해야 합니다.
성장 잠재 고객은 Frequency나 Monetary는 높은데 Recency가 낮은 경우예요. 한동안 구매하지 않은 거죠.
재참여 캠페인이나 쿠폰으로 다시 끌어들여야 합니다. 
이탈 우려 고객은 모든 지표가 낮은 그룹입니다. 대규모 할인이나 신규 고객 대우로 마지막 기회를 줄 수 있어요.

이제 분석 결과를 실시간으로 공유할 대시보드를 설계해보겠습니다. Streamlit을 사용할 거예요.
대시보드는 크게 세 영역으로 구성됩니다. 상단에는 핵심 KPI 카드를 배치해서 매출, 주문수, AOV, 마진율을 한눈에 보여줍니다.
왼쪽에는 필터를 넣어서 카테고리나 기간을 선택할 수 있게 하죠. 중앙에는 시각화 차트들을 배치합니다.
월별 매출 추이 라인 차트, 카테고리별 바 차트, RFM 세그먼트 분포 같은 인사이트 차트들이 들어가면 됩니다.

Streamlit으로 KPI 카드를 실제로 만들어보겠습니다. 코드는 정말 간단합니다.
st, .columns로 화면을 4등분하고, 각 열에 에스티 .metric을 배치하면 됩니다.
metric 함수는 라벨, 값, 변화량을 파라미터로 받습니다. 예를 들어 라벨은 '총 매출', 값은 '1,000만', 변화량은 '+15%' 이런 식이죠.
변화량은 자동으로 화살표와 색상으로 표시되어서 증가인지 감소인지 직관적으로 보입니다.
이렇게 네 개의 KPI 카드를 만들면 대시보드 상단이 완성됩니다. 실시간으로 데이터가 업데이트되는 느낌을 줄 수 있어요.

필터와 차트를 연동하는 방법을 알아보겠습니다. 사이드바에 멀티 셀렉트 필터를 만들어볼게요.
st.sidebar.multiselect로 카테고리 선택 위젯을 만듭니다. 사용자가 선택한 카테고리만 필터링하는 거죠.
필터링된 데이터로 월별 매출을 다시 집계하고, st.line_chart로 라인 차트를 그립니다.
바 차트도 st.bar_chart로 쉽게 만들 수 있어요. Streamlit의 장점은 코드 몇 줄로 인터랙티브한 차트를 만들 수 있다는 겁니다.
사용자가 필터를 변경하면 자동으로 차트가 다시 그려집니다. 이게 바로 반응형 대시보드의 핵심이죠.

분석 결과를 파일로 저장하고 자동화하는 방법도 알아야 합니다. 두 가지 방법이 있습니다.
첫째, 타임스탬프를 붙여서 결과 파일을 저장합니다. date-time.now()로 현재 시각을 구해서 파일명에 포함시키면 버전 관리가 쉬워지죠.
둘째, 스케줄링입니다. Linux에서는 Cron을 사용하고, Windows에서는 작업 스케줄러를 사용하면 매일 자동으로 분석을 실행할 수 있어요.
예를 들어 매일 오전 9시에 대시보드를 업데이트하고 이메일로 리포트를 보내는 것도 가능합니다.

데이터 품질을 보장하기 위한 체크리스트를 정리해봅시다. 세 가지 영역을 점검해야 해요.
첫째, 형식 검증입니다. 날짜 포맷, 숫자 범위, 필수 컬럼 존재 여부를 확인하죠.
둘째, 중복 검사입니다. order_id 같은 고유키가 실제로 중복되지 않는지 체크합니다.
셋째, 비즈니스 로직 검증입니다. 매출이 원가보다 낮거나, 할인율이 100%를 넘는 비현실적인 데이터는 없는지 점검해야 해요.

오늘 배운 방법은 이커머스뿐 아니라 다양한 분야에 적용 가능합니다. 두 가지 사례를 소개할게요.
제조업에서는 불량률, OEE, 설비 가동률 같은 KPI를 추적할 수 있습니다. 공정별, 라인별로 세분화해서 병목을 찾아내는 거죠.
교육 분야에서는 이수율, 리텐션, 학습 시간 같은 지표를 분석합니다. 코호트 분석으로 어느 시점에 학생들이 이탈하는지 파악할 수 있어요.
핵심은 도메인에 맞게 KPI를 재정의하고, 같은 프로세스를 적용하는 겁니다.

데이터가 커지면 성능 문제가 생깁니다. 최적화 전략을 알아볼까요?
첫째, Parquet 포맷을 사용하세요. CSV보다 훨씬 빠르고 용량도 작습니다.
둘째, Streamlit의 @st.cache_data 데코레이터를 활용하면 데이터를 메모리에 캐싱해서 반복 로딩을 방지할 수 있어요.
셋째, 청크 처리입니다. 수백만 행의 데이터는 한 번에 읽지 말고 chunksize 옵션으로 나눠서 처리하세요.
이런 기법들을 조합하면 대용량 데이터도 안정적으로 처리할 수 있습니다.

데이터 파이프라인의 신뢰성을 확보하는 방법도 중요합니다. Logging 모듈을 활용하세요.
각 처리 단계마다 로그를 남기면 문제가 생겼을 때 어디서 잘못됐는지 빠르게 찾을 수 있어요.
전처리 전후 데이터 개수, 결측치 처리 현황, 이상치 조정 건수 같은 정보를 기록하는 게 좋습니다.

데이터를 다룰 때는 보안과 윤리도 신경 써야 합니다. 특히 개인정보 보호가 핵심이에요.
PII, 즉 개인식별정보는 해싱이나 마스킹 처리를 해야 합니다. 고객 이름이나 이메일 같은 정보는 암호화하거나 익명화하세요.
데이터 편향성도 점검해야 합니다. 특정 지역이나 연령대가 과대 또는 과소 대표되지 않았는지 확인하죠.
편향된 데이터로 만든 인사이트는 잘못된 의사결정을 유도할 수 있습니다. 항상 데이터의 대표성을 의심하는 자세가 필요해요.

오늘 배운 내용을 네 가지로 요약하겠습니다.
첫째, 프로세스입니다. 데이터 로딩부터 전처리, 
EDA, KPI 계산, 
통계 검정, 대시보드까지 전체 흐름을 경험했어요.
둘째, 품질입니다. 결측치, 이상치, 타입 변환 같은 전처리가 정확한 분석의 기초라는 걸 배웠죠.
셋째, 분석입니다. KPI 정의, 코호트 분석, RFM 세분화 같은 실무 기법을 익혔습니다.
넷째, 액션입니다. 분석은 인사이트로 끝나는 게 아니라 비즈니스 의사결정으로 이어져야 한다는 점을 기억하세요.

여기서 더 나아가려면 어떻게 해야 할까요? 세 가지 방향을 제시합니다.
첫째, KPI 정의서를 작성하세요. 각 지표의 계산 로직, 목표 수준, 액션 트리거를 문서화하면 팀 전체가 같은 기준으로 일할 수 있어요.
둘째, 자동화 파이프라인을 구축하세요. Airflow 같은 도구로 데이터 수집부터 대시보드 업데이트까지 전 과정을 자동화하는 겁니다.
셋째, 고급 예측 모델링으로 나아가세요. 머신러닝을 활용해서 고객 이탈을 예측하거나, 수요를 예측하는 모델을 만들 수 있습니다.

오늘 강의는 여기까지입니다. 
대시보드 배포에 대해 궁금하신 분들을 위해 간단히 안내하면, Streamlit Cloud를 사용하면 무료로 웹에 배포할 수 있습니다.
GitHub 저장소와 연동하면 코드를 푸시할 때마다 자동으로 업데이트되죠. 실무에서 바로 활용해보시길 바랍니다.
오늘 수고 많으셨습니다!
