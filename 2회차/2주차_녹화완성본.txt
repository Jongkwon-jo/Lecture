안녕하세요, 여러분. 오늘 국립창원대학교 PRU 재직자 교육 과정에 함께하게 되어 반갑습니다. 오늘은 '생성형 AI의 기술적 원리'라는 주제로 여러분과 함께 시간을 보내려고 합니다.
최근 챗GPT를 비롯한 다양한 생성형 AI 서비스들이 우리 일상에 깊숙이 들어왔죠. 그런데 이 기술들이 실제로 어떻게 작동하는지, 그 내부 메커니즘을 이해하는 것이 매우 중요해졌습니다.

오늘 교육은 크게 세 개의 파트로 구성되어 있습니다. 먼저 Part A에서는 LLM이 단어를 생성하는 방식에 대해 살펴볼 건데요. 토큰화라는 개념부터 시작해서 트랜스포머 구조, 그리고 어텐션 메커니즘까지 차근차근 알아보겠습니다.
Part B에서는 확률 기반 언어 예측 모델의 원리를 다룹니다. Next Token Prediction이라는 핵심 개념과 함께, 실제로 AI가 어떤 확률 분포를 기반으로 다음 단어를 선택하는지 수식과 함께 이해해보겠습니다.
그리고 마지막 Part C에서는 파라미터 튜닝 실습과 함께 도메인별 프롬프트 적용, 최신 기술 동향까지 살펴보는 시간을 가질 예정입니다.

오늘 교육을 통해 여러분이 달성하게 될 목표들을 먼저 짚고 넘어가겠습니다. 첫째, LLM의 기본 구조를 명확히 이해하시게 될 겁니다. 단순히 "AI가 답변을 준다"는 표면적 이해를 넘어서, 그 내부에서 어떤 계산이 일어나는지 아시게 될 거예요.
둘째는 생성 원리를 수식으로 해석하는 능력입니다. 수식이 조금 나오긴 하지만 어렵지 않으니 걱정하지 마세요. 셋째, 다양한 디코딩 전략들을 비교 분석할 수 있게 되고요. 넷째, 확률적 관점에서 언어 모델을 해석하는 통찰력을 키우시게 될 겁니다. 마지막으로 도메인 특화 프롬프트 설계 및 튜닝 방법을 익혀보겠습니다.

자, 그럼 본격적으로 시작해볼까요? LLM이 텍스트를 생성하는 과정은 크게 네 단계로 요약할 수 있습니다.
첫 번째는 토큰화입니다. 우리가 입력한 문장을 AI가 이해할 수 있는 작은 단위로 쪼개는 거죠. 두 번째는 어텐션 단계인데, 이 과정에서 각 토큰들 간의 관계를 파악합니다. 세 번째는 로짓 계산입니다. 모델이 다음에 올 가능성이 있는 모든 단어들에 대해 점수를 매기는 과정이에요. 마지막으로 샘플링 단계에서 실제로 하나의 토큰을 선택하게 됩니다.
이 네 단계가 계속 반복되면서 우리가 보는 긴 문장이 만들어지는 거예요. 마치 레고 블록을 하나씩 쌓아올리듯이 말이죠.

언어 모델링의 핵심은 바로 이 수식으로 표현할 수 있습니다. P xt는 조건부 확률인데요, 쉽게 말해 "지금까지 나온 단어들이 주어졌을 때, 다음 단어가 나올 확률"을 의미합니다.
이걸 자기회귀 방식, 영어로는 Autoregressive라고 부릅니다. 왜 자기회귀냐면, 자기 자신이 이전에 생성한 결과를 다시 입력으로 받아서 다음 단어를 예측하거든요. 마치 체인처럼 연결되는 거죠. 첫 단어를 예측하고, 그 단어를 포함해서 두 번째 단어를 예측하고, 또 그걸 포함해서 세 번째를 예측하는 식입니다.
이 방식의 장점은 모델이 문맥을 계속 유지하면서 일관된 문장을 생성할 수 있다는 점입니다. 문서 요약 및 생성과 기계 번역, 코드 생성에 적용되죠.

토큰화를 좀 더 자세히 살펴볼까요? 예를 들어 "생성형 AI의 원리"라는 문장이 있다고 해봅시다. 
우리 눈에는 그냥 하나의 문장으로 보이지만, AI에게는 이게 여러 개의 토큰으로 쪼개집니다.
'생성', '형', 'AI', '의', '원리' 이런 식으로 나뉘는 거죠. 여기서 중요한 건 단어 단위가 아니라 서브워드 단위라는 점입니다. '생성형'이라는 단어도 '생성'과 '형'으로 나뉘거든요.
이렇게 하는 이유는 효율성 때문입니다. 모든 가능한 단어를 다 외우게 하는 것보다, 작은 단위로 쪼개서 조합하게 하는 게 훨씬 효율적이거든요. 마치 한글의 자음과 모음으로 모든 글자를 만들 수 있는 것처럼요.
토크나이제이션을 설계할 때 주요 고려사항은 첫째, 어휘 집합 크기입니다. 어휘가 너무 크면 모델 파라미터가 급증하고, 너무 작으면 문맥 압축 효율이 떨어집니다. 보통 3만~10만 개 내외로 설정하죠.
둘째는 희귀어 및 OOV 처리입니다. 사전에 없는 단어가 발생 시 [UNK] 처리 대신, 서브워드 단위로 쪼개어 의미를 보존해야 합니다.
마지막으로 한국어의 언어적 특성을 반영하는 겁니다. 교착어인 한국어는 조사와 어미가 발달해 있어, 이를 분리하지 않으면 어휘수가 기하급수적으로 늘어납니다. 형태소 분석 기반 처리가 유리할 수 있습니다.

토크나이저에도 여러 종류가 있습니다. 대표적으로 BPE 방식과 WordPiece, 그리고 Unigram 방식이 있는데요.
BPE는 빈도 기반입니다. 데이터에서 자주 같이 나오는 문자 쌍을 찾아서 하나로 합치는 방식이죠. 빈도수만을 기준으로 하기 때문에, 문맥적 의미보다는 통계적 결합에 의존합니다. GPT 시리즈가 이 방식을 사용합니다.
반면 WordPiece나 Unigram은 확률 기반입니다. 어떤 조합이 통계적으로 가장 합리적인지를 계산해서 선택하는 거예요. 확률 모델을 기반으로 하기때문에 학습과 토큰화 과정에서 계산 비용이 더 들 수 있습니다. Bert나 T-five 같은 모델들이 이 방식을 씁니다.
각 방식마다 장단점이 있지만, 최근에는 BPE가 가장 많이 사용되고 있습니다. 구현이 상대적으로 단순하면서도 성능이 우수하거든요.

토큰화가 끝나면 이제 임베딩 단계로 넘어갑니다. 토큰 ID는 그냥 숫자예요. 예를 들어 '생성'이 1234번, 'AI'가 5678번 이런 식이죠. 그런데 이 숫자 자체로는 의미를 담을 수 없습니다.
그래서 각 토큰을 고차원의 벡터로 변환합니다. 보통 수백에서 수천 차원의 벡터로 표현되는데요. 이 벡터 공간 안에서 비슷한 의미를 가진 단어들은 가까운 위치에 배치됩니다.
예시를 보시죠. "왕 - 남자 + 여자는 여왕"이라는 관계가 벡터 공간에서 실제로 성립합니다. 이게 바로 임베딩의 힘입니다. 단순한 기호를 의미 있는 수학적 표현으로 바꿔주는 거죠.
임베딩의 주요 특징에 살펴봅시다. 먼저 밀집 표현입니다. 수만 개의 차원을 가진 원-핫 벡터와 달리, 수백~수천 차원으로 정보를 압축하여 표현 효율성을 극대화합니다.
다음은 가중치 공유입니다. 입력 임베딩 행렬과 출력 레이어의 가중치를 공유하여, 학습 파라미터를 줄이고 일반화 성능을 높입니다.
마지막은 학습 가능성입니다. 임베딩은 고정된 값이 아니라, 모델 학습 과정을 통해 최적의 표현으로 업데이트됩니다.

이제 본격적으로 트랜스포머 구조를 살펴보겠습니다. GPT는 디코더 온리 구조를 사용하는데요, 이게 무슨 뜻이냐면 인코더 없이 디코더만으로 구성되어 있다는 뜻입니다.
핵심은 마스크드 어텐션입니다. 이 메커니즘을 통해 모델은 미래의 토큰을 볼 수 없게 제한됩니다.
왜 그럴까요? 학습할 때 치팅을 방지하기 위해서입니다. 만약 답을 미리 볼 수 있다면, 모델이 제대로 학습되지 않겠죠.
또 하나의 큰 장점은 병렬 처리입니다. 과거 RNN 방식은 순차적으로 처리해야 했지만, 트랜스포머는 한 번에 모든 위치를 동시에 계산할 수 있습니다. 이게 바로 트랜스포머가 혁신적이었던 이유입니다. 속도가 비교할 수 없을 정도로 빨라졌거든요.

어텐션 메커니즘은 Query, Key, Value가 핵심입니다. 조금 복잡하지만 천천히 설명드리겠습니다.
쿼리는 "내가 찾고 싶은 정보"입니다. 현재 시점의 토큰이 다른 코튼과의 관계를 묻는 벡터입니다.
키는 "각 단어의 식별자"고요. 쿼리가 자신과 관련 있는지 확인하기 위한 식별 태그 벡터죠.
Value는 "실제 내용물"입니다. 실제 추출하여 합성하고자 하는 의미 정보를 담은 벡터로 볼 수 있죠.
이를 도서관에 비유하면, Query는 검색어, Key는 책 제목, Value는 책 내용이라고 생각하시면 됩니다.
어텐션 계산 과정은 이렇습니다. 쿼리와 키의 Dot Product을 통해 유사도를 계산합니다. 그 다음 소프트맥스를 취해서 확률 분포로 만들어요. 
마지막으로 이 확률을 가중치로 사용해 Value들을 가중합산합니다. 결과적으로 현재 위치에서 어떤 단어에 주목해야 하는지 자동으로 학습하게 되는 거죠.

멀티헤드 어텐션을 히트맵으로 한번 살펴보시죠. 우선 셀프 어테션 매커니즘은 각 토큰이 자기 자신을 포함한 문장 내 모든 토큰과의 상관관계를 계산합니다. 색상이 밝을수록 더 강하게 집중하고 있음을 의미합니다.
실제 예시를 보면 더 명확하게 이해가 됩니다. 이 히트맵을 보시면 it이라는 단어가 'animal'을 강하게 가리키고 있는 걸 볼 수 있죠.
이게 바로 어텐션의 힘입니다. 대명사가 무엇을 지칭하는지, 문맥상 어떤 단어와 연결되어야 하는지를 모델이 스스로 파악합니다. 사람이 일일이 규칙을 만들어주지 않아도 데이터로부터 학습하는 거예요.
멀티헤드라는 건 이런 어텐션을 여러 개 병렬로 수행한다는 뜻입니다. 각 헤드마다 다른 관점에서 관계를 포착하게 되죠. 한 헤드는 문법적 관계를, 다른 헤드는 의미적 관계를 학습하는 식으로요.

그런데 여기서 문제가 하나 있습니다. 트랜스포머는 모든 위치를 동시에 처리하기 때문에, 단어의 순서 정보가 사라집니다. "개가 고양이를 쫓는다"와 "고양이가 개를 쫓는다"가 구별이 안 되는 거죠.
이 문제를 해결하기 위해 포지셔널 인코딩을 추가합니다. 사인과 코사인 함수를 사용해서 각 위치마다 고유한 패턴을 만들어내는 거예요. 이 패턴이 임베딩 벡터에 더해지면서, 모델이 순서 정보를 인식할 수 있게 됩니다.
수학적으로 보면 주기가 다른 삼각함수들을 조합하는데요, 이렇게 하면 위치 간의 상대적 거리도 표현할 수 있게 됩니다. 추가적으로 학습 때 보지 못한 더 긴 문장이 들어와도, 주기적 패턴 덕분에 위치 정보를 어느 정도 유추하여 처리할 수 있는 장점이 있습니다.
이렇게 계산된 포지셔널 인코딩 벡터는 입력 임베딩 벡터와 동일한 차원을 가지며, 단순히 더해져서 트랜스포머의 첫 번째 레이어로 전달됩니다.

조금 전에 언급했던 마스크드 어텐션을 좀 더 자세히 볼까요?
이건 인과성, 즉 Causality를 보장하기 위한 장치입니다.
학습할 때 모델은 문장 전체를 한 번에 봅니다. 
하지만 각 위치에서 예측할 때는 그 위치보다 뒤에 있는 토큰들을 봐서는 안 되겠죠. 그래서 미래 토큰들에 대한 어텐션을 마이너스 무한대로 마스킹합니다. 소프트맥스를 거치면 이 값들은 0이 되어버려서, 사실상 미래 정보가 차단되는 거예요.
이렇게 하면 모델이 추론할 때와 동일한 조건에서 학습하게 됩니다. 정직하게 왼쪽에서 오른쪽으로만 정보를 사용하도록 강제하는 거죠.

이제 학습의 핵심 원리인 Next Token Prediction을 살펴보겠습니다. LLM 학습의 본질은 다음 토큰의 등장 확률을 최대화하여 손실을 최소화하는 것입니다.
손실 함수를 보시면 negative log like-li-hood 즉, 크로스엔트로피가 사용되고 있죠.
이게 뭐냐면, 모델이 예측한 확률 분포와 정답 분포 사이의 차이를 측정하는 겁니다. 이 차이가 작을수록, 즉 손실이 작을수록 모델이 잘 학습된 거예요.
수십억 개의 문장으로 학습하면서 이 손실을 계속 줄여나갑니다. 결과적으로 모델은 "이 문맥 다음에는 이런 단어가 올 확률이 높다"는 패턴을 내재화하게 되는 거죠. 이게 바로 GPT가 그럴듯한 문장을 생성할 수 있는 이유입니다.

자, 그럼 이제 모델이 출력하는 출력값인 로짓에 대해 살펴보겠습니다.
로짓에서 실제 확률로 변환하는 과정이 바로 소프트맥스입니다. 그런데 여기서 Temperature라는 중요한 파라미터가 등장합니다.
Temperature를 낮추면 확률 분포가 날카로워집니다. 가장 높은 확률을 가진 토큰이 더 두드러지게 되죠. 반대로 Temperature를 높이면 분포가 평평해집니다. 확률이 고르게 분산되는 거예요.
실무적으로 이게 왜 중요하냐면, 생성 스타일을 조절할 수 있기 때문입니다. 
낮은 Temperature는 보수적이고 예측 가능한 출력을 만들어냅니다. 
높은 Temperature는 창의적이지만 때로는 이상한 결과를 낼 수 있죠. 
상황에 따라 적절히 조절해야 합니다.

확률 분포를 얻었으면 이제 실제로 토큰을 선택해야 합니다. 여기에는 결정적 방식과 확률적 방식 두 가지 방식으로 나뉩니다.
먼저 결정적 방식은 매 단계에서 가장 높은 확률의 토큰 하나를 선택하는 Greedy 방식 또는 상위 N개 경로를 선택하는 Beam Search 방식을 통해 가장 그럴듯한 문장을 완성합니다.
결정적 방식은 예측 결과가 일관적이고, 문법적 정확도가 높습니다. 불확실성을 최소화해서 안정적인 출력을 보장하죠. 하지만 다양성이 부족해서 문장이 단조롭거나 반복될 수 있습니다. 정답이 정해진 번역, 요약, Q&A 태스크에 적합합니다.
확률적 방식은 Sampling 기법들인데요, 매번 다른 결과가 나올 수 있습니다. 확률 분포에서 무작위로 샘플링하기 때문이죠. 
확률적 방식은 사람의 언어 구사처럼 다양하고 창의적인 문장을 생성할 때 적합합니다. 하지만 논리적 오류나 환각, Hallucination 발생 가능성이 존재하죠.
각 방식의 장단점을 이해하고 상황에 맞게 선택하는 게 중요합니다.

확률적 샘플링에도 여러 기법이 있습니다. 대표적인 게 Top-k와 Top-p, Temperature죠.
Top-k는 간단합니다. 확률이 높은 상위 k개 토큰 중에서만 선택하는 거예요. 예를 들어 k가 5면 상위 5개 후보 중 하나를 고르는 겁니다. 문제는 k가 고정되어 있다는 점입니다. 어떤 상황에서는 5개가 너무 많을 수도, 너무 적을 수도 있거든요.
Top-p는 이 문제를 해결합니다. 누적 확률이 p를 넘을 때까지 후보를 추가하는 방식이에요. 만약 p가 0.9라면, 확률을 높은 순서대로 더해가다가 90%가 되는 순간 멈추는 겁니다. 그래서 상황에 따라 후보 개수가 동적으로 변합니다. 확실한 문맥에서는 적게, 애매한 문맥에서는 많이 선택하는 거죠.
마지막으로 Temperature Scaling은 확률 분포의 모양 자체를 바꿉니다. 값이 낮을수록, 1등 단어의 확률이 극대화되고, 값이 높을수록 분포가 평평해져 다양한 단어가 선택될 기회를 줍니다.

실제로 텍스트를 생성하다 보면 같은 말을 계속 반복하는 문제가 생길 수 있습니다. 이를 방지하기 위한 여러 기법들이 있습니다.
N-gram Penalty는 같은 단어 조합이 반복되면 확률을 깎아내립니다. Repetition Penalty는 이미 나온 토큰의 확률을 감소시키고요. Length Penalty는 문장이 너무 길거나 짧아지지 않도록 조절합니다.
금지 토큰 같은 경우는 욕설, 편향적 표현, 개인정보 등 생성을 원치 않는 특정 단어나 구문을 사전에 차단합니다.
마지막으로 길이 제약은 Min-Max Token 설정이나 문장 끝에 토큰 강제 생성을 통해 출력 문장의 길이를 적절히 조절합니다.
이런 제약들을 적절히 조합하면 훨씬 자연스럽고 읽기 좋은 텍스트를 생성할 수 있습니다. 하지만 너무 강하게 적용하면 오히려 부자연스러워질 수 있으니 균형이 중요합니다.

지금까지 배운 모든 내용을 종합해볼까요? 생성 과정은 다섯 단계가 계속 반복됩니다.
입력 토큰화부터 시작해서, 임베딩 변환, 트랜스포머 레이어 통과, 로짓 계산, 그리고 샘플링까지. 샘플링된 토큰은 다시 입력에 추가되고, 이 과정이 끝나는 토큰이 나올 때까지 반복됩니다.
여기서 중요한 최적화 기법이 KV-캐시예요. 매번 처음부터 다시 계산하는 게 아니라, 이전에 계산한 Key와 Value를 저장해두고 재사용하는 거예요.
이렇게 하면 속도가 엄청나게 빨라집니다. 특히 긴 문장을 생성할 때 효과가 크죠.

실제 예시로 이해해봅시다. 
"오늘 날씨가 정말"
이라는 입력이 들어왔다고 가정하겠습니다.
이제 입력을 모델이 이해하는 토큰 단위로 분해합니다. "오늘"은 2,4,0,1이되고, "날씨"는 5,9,3,2가 되겠죠.
그 후 모델은 다음 토큰으로 올 수 있는 후보들의 확률 점수를 계산합니다.
다음 토큰의 후보로는 
"좋네요", "춥네요", "더워요" 등이 있겠죠. 
이러한 토큰에 대해 각각 확률을 계산합니다. 
현재 문맥을 고려했을 때 "좋네요"가 가장 높은 확률을 가진다고 해봅시다.
샘플링 전략에 따라 "좋네요"가 선택될 가능성이 크지만, Top-p 샘플링을 사용하면 다른 후보도 선택될 수 있습니다. 이렇게 한 토큰이 생성되면 "오늘 날씨가 정말 좋네요"가 되고, 이제 이 전체 문맥을 바탕으로 다음 토큰을 예측하는 과정이 반복됩니다.

자, 이제 Part B로 넘어가겠습니다. 지금까지는 메커니즘을 봤다면, 이제는 그 이면의 확률과 최적화 이론을 좀 더 깊이 있게 다룰 예정입니다.
수학적 배경을 이해하면 실무에서 파라미터를 조절할 때 훨씬 더 효과적으로 대응할 수 있습니다.

언어 모델의 역사를 간단히 살펴보면, 초기에는 N-gram 모델이 사용됐습니다. 단순히 단어의 빈도를 세는 방식이었죠. 그 다음 RNN과 LSTM이 등장하면서 순차적 패턴을 학습할 수 있게 됐습니다.
하지만 진짜 혁명은 2017년 트랜스포머의 등장입니다. 병렬 처리가 가능해지면서 대규모 학습이 현실화됐고, 그 결과 GPT 같은 거대 언어 모델이 탄생하게 된 거죠.
이 계보를 이해하는 게 중요한 이유는, 각 세대마다 해결하려 했던 문제와 한계를 알 수 있기 때문입니다.
트랜스포머도 완벽하진 않죠. 여전히 개선의 여지가 많습니다.

트랜스포머 기반 모델에도 여러 유형이 있습니다.
자기회귀 방식의 GPT는 이전 토큰들을 기반으로 다음 토큰을 예측하는 데 특화되어 있습니다. 자연스러운 텍스트 생성 능력과 문맥을 이어가는 창작에 최적화되어 있죠.
Masked LM 방식의 Bert는 양방향으로 문맥을 이해합니다. 문장의 빈칸을 채우는 식으로 학습하기 때문에 이해 능력이 뛰어나죠. 검색이나 분류 같은 태스크에 적합합니다.
Sequence-To-Sequence 방식은 인코더-디코더 구조로, 입력 시쿼스를 압축 한 후 다른 시퀀스로 변환합니다. 번역이나 요약처럼 입력과 출력이 모두 중요한 작업에 유리합니다.
각 아키텍처는 설계 목적이 다르기 때문에, 과제에 맞게 선택하는 게 중요합니다.

이제 수학적 기반을 보겠습니다. 문장의 생성 확률은 연쇄 법칙으로 분해할 수 있습니다. 전체 문장의 확률은 각 단어의 조건부 확률을 곱한 것과 같다는 거죠.

수식으로 보면 1부터 T까지의 확률은 T의 이전 확률들이 주어졌을 때 T의 확률의 곱으로 표현됩니다.
쉽게 보면, 첫 번째 단어가 나올 확률, 그 다음 첫 번째가 주어졌을 때 두 번째가 나올 확률, 이런 식으로 곱해나가는 거예요.
이 원리가 바로 LLM의 수학적 토대입니다. 모델은 이 조건부 확률들을 학습하고, 생성할 때는 이 확률들을 차례로 적용하는 거죠.

조건부 확률을 직관적으로 이해해봅시다.
"오늘 날씨가 정말" 이라는 예시에서 다음 토큰을 예측할 때, 수십억 개의 문장에서 단어 간의 Co-occurrence 빈도를 학습하여 '상식'에 가까운 확률 분포를 형성합니다. 
따라서, 각 후보들의 예측 확률 분포는 "좋네요" 45%, "춥습니다" 30%, 맑아요 15% 등으로 나타납니다.
하지만 모델이 뉴스 데이터로만 학습했다면 '춥습니다'의 확률이 더 높고, 소설 데이터라면 '맑아요' 같은 표현의 확률이 더 높아집니다. 이걸 도메인 분포 이동이라고 합니다. 
학습 데이터의 특성이 모델의 확률 분포를 결정하는 거예요. 그래서 범용 모델을 만들려면 다양한 도메인의 데이터를 균형 있게 학습시켜야 합니다.
사전 확률도 중요합니다. 처음 단어를 생성할 때는 문맥이 없으니까요. 이때는 학습 데이터에서 각 단어가 문장 시작 위치에 나타난 빈도가 사전 확률이 됩니다.

모델의 성능을 어떻게 측정할까요? Perplexity라는 지표를 사용합니다. 한국어로는 혼란도 정도로 번역할 수 있는데요.
수식을 보면 교차 엔트로피의 지수 변환입니다. 직관적으로 설명하면, 모델이 다음 단어를 예측할 때 평균적으로 몇 개의 후보 중에서 고민하는지를 나타냅니다. PPL이 10이라면 평균적으로 10개 정도의 후보를 고려한다는 뜻이죠.
PPL는 낮을수록 좋습니다. PPL이 1에 가까울수록 모델이 다음 단어를 완벽하게 확신한다는 의미이고, 수치가 높을수록 헷갈리는 선택지가 많다는 뜻입니다.

Temperature 파라미터가 생성된 텍스트의 다양성과 정확성에 미치는 영향을 비교해보겠습니다.
다음 예시를 보시죠. 같은 프롬프트에 T값만 바꿔서 생성해봤습니다.
T가 0.2일 때는 매우 보수적인 문장이 나옵니다. "오늘 날씨가 좋습니다. 산책하기 좋은 날입니다." 처럼 무난하고 예측 가능한 표현들이죠. 
T가 1.0에서는 조금 더 자연스러워집니다. "오늘 날씨 정말 좋네요! 밖에 나가고 싶어지는 날이에요." 이런 식으로요.
T를 1.8로 높이면 창의적이지만 때로는 이상한 문장이 나옵니다. "오늘 날씨... 하늘이 춤추는 것 같아요. 구름이 노래하네요"과 같이 말이죠.
실무에서는 보통 0.7~1.0 사이를 많이 씁니다.

실제 서비스에서는 속도가 매우 중요합니다. 여러 최적화 기법들이 개발됐는데요.
KV Cache는 앞에서 언급했죠. 이미 계산한 것을 재사용하는 겁니다. 
Speculative Decoding은 작은 모델로 먼저 예측하고, 큰 모델로 검증하는 방식입니다. 대부분은 작은 모델 예측이 맞으니까 시간을 절약할 수 있죠.
FlashAttention은 어텐션 계산을 메모리 효율적으로 재구성한 겁니다. GPU 메모리 접근 패턴을 최적화해서 속도를 크게 올렸습니다. 이런 기법들을 조합하면 실시간 서비스가 가능해집니다.

AI의 안전성도 중요한 주제입니다. 모델이 유해한 내용을 생성하지 않도록 가드레일을 설치하는데요.
문제는 안전성과 다양성 사이의 트레이드오프입니다. 너무 강하게 제약하면 정상적인 질문에도 답변을 거부하는 경우가 생깁니다. 이걸 거부 편향이라고 하죠.
예를 들어 "폭탄"이라는 단어만 들어가도 무조건 차단하면, "폭탄 발언의 뜻은 뭔가요?" 같은 정상적인 질문도 막히게 됩니다. 그래서 문맥을 고려한 정교한 필터링이 필요합니다.

실무에서 도메인별로 어떻게 적용하면 좋을까요? 몇 가지 팁을 드리겠습니다.
제조업에서는 정형화된 포맷이 중요합니다. Temperature를 낮게 설정하고, 템플릿 기반 생성을 권장합니다. 불량 보고서 같은 건 창의성보다 정확성이 중요하니까요.
교육 분야에서는 Persona 설정이 효과적입니다. "당신은 초등학교 선생님입니다"라고 명시하면 설명 수준이 조절됩니다. 
공공 분야에서는 래그를 적극 활용하세요. 외부 문서를 참조하고 근거를 명시하면 신뢰도가 올라갑니다.
각 도메인의 특성을 이해하고 그에 맞는 전략을 수립하는 게 핵심입니다.

하지만 현재 기술에도 여전히 한계가 있습니다. 
먼저 소프트맥스 병목 문제입니다. 모든 가능한 토큰에 대해 확률을 계산해야 하는데, 어휘가 수만 개니까 계산량이 어마어마하죠. 그래서 고차원의 언어 공간이 저차원 행렬로 표현되면서 표현력이 제한됩니다.
긴 컨텍스트도 문제입니다. 입력 길이가 길어질수록 연산 비용이 제곱으로 증가하며, 정보 손실이 발생합니다.
다음으로 노출 편향입니다. 학습 시와 추론 시의 환경 차이로 인한 오류 누적 문제입니다.
마지막으로 가장 큰 문제인 할루시네이션입니다. 모델이 그럴듯하지만 거짓인 정보를 생성하는 겁니다.
확률 기반이다 보니 "있을 법한" 답을 만들어내는데, 그게 항상 사실은 아니거든요.
이런 문제들을 해결하기 위한 연구가 활발히 진행 중입니다. 완벽하진 않지만 계속 개선되고 있습니다.

마지막으로 최신 기술 동향을 소개하겠습니다.
Mixture-of-Experts, MoE는 여러 전문가 모델을 조합하는 방식입니다. 모든 파라미터를 항상 사용하는 게 아니라, 입력에 따라 필요한 전문가만 활성화합니다. 효율성이 크게 올라가죠.
Multi-Token Prediction은 한 번에 여러 토큰을 예측하는 기법입니다. 기존에는 한 번에 하나씩 생성했는데, 여러 개를 동시에 예측하면 속도가 빨라지죠. 
기능 확장 측면에서 래그 기술은 모델이 학습하지 않은 외부 지식 검색을 가능하게 하고, 도구 사용, Tool Use를 통해 AI가 계산기나 검색 엔진 같은 외부 도구를 활용하게 합니다. 이렇게 하면 할루시네이션을 줄이고 정확도를 높일 수 있습니다.
추가적으로 안정성 강화 측면에서 RLHF 기술도 주로 사용합니다.

이론은 여기까지고, 이제 직접 실습해볼 시간입니다. 과제는 파라미터 튜닝 실험인데요.
같은 프롬프트에 Temperature, 탑-p, 탑-k 값을 바꿔가면서 결과를 비교해보세요. 각 파라미터가 실제로 어떤 영향을 미치는지 체감하는 게 중요합니다.
추천하는 실험 조합은 슬라이드에 나와 있습니다. 시간이 되시면 여러 조합을 시도해보시고, 어떤 설정이 어떤 상황에 적합한지 스스로 판단해보세요. 실습을 통해 이론을 내 것으로 만들 수 있을 겁니다.

오늘 배운 내용을 정리하겠습니다.
생성형 AI의 핵심은 확률 분포에서 샘플링하는 것입니다. LLM의 본질은 다음에 올 단어의 확률을 예측하고 선택하는 것입니다. 정답이 정해진 것이 아니라, 가장 그럴듯한 경로를 탐색하는 과정이죠.
우리는 토크나이저, 어텐션 메커니즘부터 Temperature, 탑-p와 같은 디코딩 파라미터까지 이해하였고, 다양한 도구를 통해 모델의 출력을 제어할 수 있습니다.
실무에서는 안전성, 속도, 품질 사이의 균형을 잡아야 합니다. 완벽한 설정은 없고, 상황에 맞게 조율하는 게 중요합니다.

오늘 교육이 여러분의 생성형 AI의 이해도를 한 단계 높이는 계기가 되었길 바랍니다. 감사합니다.
