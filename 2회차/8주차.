📝 구조화 데이터 분석 - KPI 분석 및 인사이트 생성 실습 강의 스크립트
슬라이드 1: 표지 (1분)
안녕하세요, 여러분. 국립창원대학교 PRU 재직자 교육 과정에 오신 것을 환영합니다. 오늘은 '구조화 데이터 분석'이라는 주제로 KPI 분석부터 인사이트 생성까지 실무에서 바로 활용할 수 있는 내용을 함께 다뤄보려고 해요.

Python을 활용한 데이터 분석, 그리고 대시보드 구축까지 전 과정을 실습 중심으로 진행할 예정입니다. 난이도는 실무 초급에서 중급 수준이니 부담 갖지 마시고 편하게 따라와 주시면 됩니다.

슬라이드 2: 학습 목표와 기대 성과 (1.5분)
오늘 실습을 통해 여러분이 달성하게 될 핵심 역량은 크게 네 가지로 정리할 수 있어요. 첫 번째, 핵심 KPI 정의 및 계산입니다. 매출이나 마진, 재구매율처럼 비즈니스 성장에 꼭 필요한 지표들을 직접 설계하고 계산하는 방법을 배우게 되죠.

두 번째는 데이터 전처리 실습인데요, 결측치나 이상치를 처리하고 데이터 타입을 변환하는 등 분석 가능한 형태로 데이터를 정제하는 노하우를 익히게 됩니다. 세 번째로 EDA, 즉 탐색적 데이터 분석을 통해 숨겨진 패턴을 발견하고 통계 검정으로 가설을 검증하는 과정도 경험하실 거예요. 마지막으로 Streamlit을 활용해서 분석 결과를 실시간으로 공유할 수 있는 대시보드까지 구현해 보겠습니다.

슬라이드 3: 오늘의 아젠다 (1.5분)
자, 그럼 오늘의 로드맵을 한번 살펴볼까요? 데이터 로딩부터 대시보드 구축까지 총 7단계로 구성되어 있습니다. Step 1에서는 실습 데이터와 분석 환경을 소개하고, Step 2에서 데이터 전처리를 진행하죠. 결측치, 이상치, 타입 변환 이런 것들을 다룰 거예요.

Step 3은 EDA 단계입니다. 데이터 패턴을 파악하고 기초 시각화를 해보는 거죠. Step 4에서는 본격적으로 KPI를 설계하고 계산하게 됩니다. 매출, 마진, 재구매율 같은 지표들을 정의하는 과정이에요. Step 5는 통계 분석과 고객 세분화, Step 6에서 Streamlit으로 대시보드를 만들어보고, 마지막으로 실습과 과제를 통해 전체 과정을 복습하게 됩니다.

슬라이드 4: 실습 데이터셋 소개 (2분)
이번 실습에서 사용할 데이터는 가상의 이커머스 주문 데이터입니다. orders.csv 파일인데요, 약 5천 건의 주문 기록이 담겨 있어요. 총 12개의 컬럼으로 구성되어 있는데, 주문 ID부터 시작해서 주문 일자, 고객 ID, 상품명, 카테고리 같은 기본 정보들이 있고요.

가격, 수량, 할인율 같은 거래 정보도 포함되어 있습니다. 또 원가 정보가 있어서 마진 계산이 가능하고, 지역이나 유입 채널 정보도 있어서 세분화 분석이 가능하죠. 주문 상태 컬럼도 있어서 완료된 건인지 환불된 건인지 구분할 수 있어요. 실제로는 일부 결측치와 이상치가 포함되어 있기 때문에, 전처리가 필요한 현실적인 데이터라고 보시면 됩니다.

슬라이드 5: 분석 환경 세팅 (1.5분)
원활한 실습을 위해 필요한 환경을 점검해 볼게요. Python 버전은 3.10 이상을 권장하고요, IDE는 VS Code나 Jupyter 같은 걸 사용하시면 됩니다. 가상환경은 venv나 conda를 사용하시는 게 좋아요. 의존성 충돌을 방지하기 위해서죠.

필수 라이브러리는 pandas, numpy 같은 데이터 처리 라이브러리와 matplotlib, seaborn 같은 시각화 도구들이 필요합니다. scipy는 통계 검정용이고, scikit-learn은 머신러닝용인데 오늘 실습에서는 일부만 사용할 거예요. 그리고 streamlit은 대시보드 구축용입니다. 프로젝트 폴더 구조는 화면에 보시는 것처럼 data 폴더에 원본 데이터를 넣고, out 폴더에 결과를 저장하는 형태로 구성하시면 됩니다.

슬라이드 6: 패키지 설치 및 임포트 (1.5분)
자, 이제 실제로 환경을 구성해 보겠습니다. 먼저 터미널에서 가상환경을 생성하고 활성화하는 명령어를 실행하시면 돼요. Windows는 Scripts\activate, Mac이나 Linux는 bin/activate 이렇게 경로가 다르니 주의하세요.

가상환경이 활성화되면 pip install로 필요한 라이브러리들을 한 번에 설치합니다. 그 다음 Python 코드에서 라이브러리를 임포트하는데요, pandas는 pd, numpy는 np 이렇게 관례적으로 사용하는 약자가 있어요. 시각화 스타일도 미리 설정해 두면 좋은데, seaborn의 테마를 적용하고 한글 폰트도 설정해야 차트에서 한글이 깨지지 않습니다. Windows에서는 Malgun Gothic을 많이 사용하죠.

슬라이드 7: 데이터 로딩 & 최초 점검 (2분)
본격적으로 데이터를 불러와 봅시다. pandas의 read_csv 함수로 데이터를 읽어오면 데이터프레임이라는 테이블 형태의 객체가 만들어져요. 먼저 head 메서드로 상위 3행 정도를 확인해 보면 데이터가 어떻게 생겼는지 감이 오죠.

shape 속성으로 전체 행과 열의 개수를 확인하고, columns.tolist()로 컬럼명 목록도 살펴봅니다. 출력 결과를 보면 5천 행, 12개 컬럼으로 구성되어 있고, 주문 ID부터 시작해서 날짜, 고객 ID, 카테고리 등이 나열되어 있네요. 주문 상태 컬럼의 분포도 확인해 보면 완료된 주문이 4,250건으로 가장 많고, 환불이나 대기 상태도 일부 있습니다. 결측치도 50건 정도 보이네요.

슬라이드 8: 정보 요약과 기술통계 (2분)
데이터의 전반적인 정보를 확인하기 위해 info 메서드를 실행해 볼게요. 이 메서드는 각 컬럼의 데이터 타입과 결측치 개수, 메모리 사용량까지 보여줍니다. 출력 결과를 보면 order_date가 object 타입, 즉 문자열로 되어 있네요. 이건 나중에 datetime으로 변환해야 합니다. category 컬럼은 50개 정도 결측치가 있고요.

describe 메서드는 수치형 변수들의 기초 통계량을 보여줍니다. count, mean, std, min, max 이런 값들이죠. 결과를 보면 price의 최댓값이 5만으로 평균 대비 너무 크고, quantity도 최댓값이 100으로 극단적이에요. 이런 이상치는 나중에 처리해야 할 후보입니다. discount 컬럼은 count가 4,800개로 다른 컬럼보다 적은데, 이것도 결측치가 있다는 뜻이죠.

슬라이드 9: 결측치 탐지와 처리 전략 (2분)
데이터 정제의 첫 단계는 결측치 처리입니다. 결측치가 어느 컬럼에 얼마나 있는지 파악하는 게 우선이에요. isna 메서드로 결측치를 찾고 sum으로 개수를 세면 됩니다. 그 다음 처리 전략을 수립하는데, 결측 비율이 아주 낮으면 해당 행을 삭제하는 것도 방법이에요. 하지만 데이터 손실을 막으려면 대체하는 게 더 나을 수 있죠.

수치형 변수는 평균이나 중앙값으로 채우는데, 중앙값이 이상치 영향을 덜 받아서 더 안전합니다. 범주형 변수는 최빈값을 쓰거나 'Unknown' 같은 새 라벨을 부여하는 방법이 있어요. 코드를 보면 fillna 메서드로 median이나 특정 값을 넣어주는 방식이죠. 주의할 점은, 결측치가 무작위로 발생한 게 아니라 특정 패턴이 있을 수 있다는 거예요. 예를 들어 배송 실패 건만 특정 컬럼이 비어있다면, 단순 대체보다 원인 분석이 먼저일 수 있습니다.

슬라이드 10: 타입 정제: 날짜/범주형 (2분)
데이터 타입 변환도 중요한 전처리 과정입니다. 왜 굳이 타입을 바꿔야 할까요? 날짜 컬럼이 문자열로 되어 있으면 연도나 월을 추출할 수 없고, 기간 계산도 불가능해요. pd.to_datetime 함수로 날짜 타입으로 바꾸면 dt 접근자를 통해 다양한 시계열 기능을 사용할 수 있죠.

범주형 변수도 마찬가지입니다. '배송완료' 같은 문자열이 계속 반복되면 메모리를 많이 차지하는데, category 타입으로 변환하면 내부적으로 정수 ID로 매핑해서 메모리를 최대 90%까지 줄일 수 있어요. 처리 속도도 빨라지고요. 코드를 보면 astype으로 타입을 변경하고, to_period로 월 단위 파생변수를 만드는 모습이 보이네요. 이렇게 하면 월별 트렌드 분석이 훨씬 쉬워집니다.

도구 사용
|
심층 사고
총: 5개의 할 일
남은 할 일 3개
PPT 전체 내용 분석 및 50분 배분 계획 수립
슬라이드 1-10: 도입 및 환경 설정 스크립트 작성 (10분)
슬라이드 11-20: 전처리 및 EDA 스크립트 작성 (12분)
슬라이드 21-30: KPI 분석 및 실습 스크립트 작성 (15분)
슬라이드 31-40: 대시보드 및 마무리 스크립트 작성 (13분)
네, 이어서 슬라이드 11-40까지의 스크립트를 완성하겠습니다.

슬라이드 11: 이상치 탐지(IQR)와 처리 (2분)
이제 이상치를 다뤄볼 차례입니다. IQR, 즉 사분위수 범위를 활용하는 방법인데요, 통계학에서 가장 널리 쓰이는 기법이에요. 개념은 간단합니다. 데이터를 정렬했을 때 25% 지점이 Q1, 75% 지점이 Q3이고, 그 차이가 IQR이죠.

여기서 Q1에서 IQR의 1.5배를 뺀 값이 하한선, Q3에 1.5배를 더한 값이 상한선이 됩니다. 이 범위를 벗어나는 값들이 이상치로 간주되는 거예요. 코드를 보면 quantile 함수로 Q1, Q3를 구하고, clip 메서드로 상한선과 하한선을 적용하는 모습이 보입니다. 이렇게 하면 극단적인 값들이 경계값으로 조정되면서 평균 왜곡을 막을 수 있어요. 실행 결과를 보니 price의 최댓값이 1만 2천에서 182로, quantity는 100에서 6으로 크게 줄어들었네요.

슬라이드 12: 파생변수 생성 (2분)
데이터 정제가 끝났으니 이제 분석에 필요한 핵심 지표들을 계산해 봅시다. 비즈니스 로직에 따라 네 가지 파생변수를 만들 거예요. 첫째는 매출인데, 단가 곱하기 수량에 할인율을 적용한 값입니다. 할인율이 없는 경우도 있으니 fillna로 0을 채워줘야 해요.

둘째는 총 비용으로, 원가에 수량을 곱하면 되죠. 셋째는 마진인데, 매출에서 총 비용을 빼면 됩니다. 넷째는 마진율, 즉 마진을 매출로 나눈 백분율이에요. 여기서 주의할 점은 매출이 0인 경우 나누기 에러가 발생할 수 있다는 거예요. np.where로 조건을 걸어서, 매출이 0보다 클 때만 계산하고 아니면 NaN 처리하는 방식으로 안전하게 처리합니다.

슬라이드 13: EDA: 월별 매출 추이 (2분)
자, 이제 본격적으로 데이터를 탐색해 볼까요? 첫 번째로 볼 건 시계열 패턴입니다. 월별로 매출이 어떻게 변화하는지 라인 차트로 그려보는 거죠. groupby로 order_month 기준으로 묶고, revenue를 합산하면 월별 매출 집계가 완성돼요.

seaborn의 lineplot을 사용하면 깔끔한 선 그래프가 그려지는데, 여기서 우리가 봐야 할 포인트는 세 가지입니다. 첫째, 전반적인 성장 추세가 있는가? 우상향하는지 정체되어 있는지 확인하는 거죠. 둘째, 계절성이 있는가? 특정 월이나 분기에 반복되는 패턴이 보이는지 살펴봐야 해요. 셋째, 급격한 변화 지점, 즉 스파이크가 있다면 그 원인이 뭔지 파악해야 합니다. 프로모션이었을 수도 있고, 시스템 오류일 수도 있거든요.

슬라이드 14: EDA: 카테고리/채널별 성과 (2분)
다음은 카테고리별 분석입니다. 어떤 제품군이 매출에 가장 많이 기여하는지 보는 거죠. groupby로 category 기준 매출을 집계하고, sort_values로 내림차순 정렬한 뒤 상위 10개만 뽑아냅니다. 가로 막대 차트로 그려보면 한눈에 순위가 보이죠.

여기서 파레토 법칙을 확인해 볼 수 있어요. 상위 20% 카테고리가 전체 매출의 80%를 차지하는지 말이죠. 만약 특정 카테고리에 대한 의존도가 너무 높다면 리스크가 될 수 있어요. 반대로 매출은 적지만 성장률이 높은 롱테일 카테고리도 주목할 필요가 있습니다. 잠재력이 있는 틈새시장일 수 있거든요.

슬라이드 15: KPI 설계 프레임 (1.5분)
이제 KPI를 본격적으로 정의해 봅시다. 비즈니스 성장을 측정하는 7가지 핵심 지표가 있어요. 매출은 총 판매 금액이고, 주문수는 결제 완료 건수죠. 고객수는 기간 내 구매한 고유 고객의 수예요.

AOV, 즉 객단가는 주문 1건당 평균 결제액을 의미합니다. ARPU는 고객 1인당 평균 매출이고요. 마진율은 매출 대비 순이익 비율이에요. 마지막으로 재구매율은 기간 내 2회 이상 구매한 고객의 비율로, 고객 충성도를 나타내는 중요한 지표입니다. 명확한 KPI 정의는 분석의 시작이자 액션 가능한 인사이트의 기반이 됩니다.

슬라이드 16: KPI 계산 코드 (2.5분)
실제 코드로 KPI를 계산해 볼게요. 먼저 기본 집계부터 시작합니다. nunique로 고유한 주문 ID와 고객 ID 개수를 세고, sum으로 매출과 마진을 합산하죠. 비율 지표는 나눗셈으로 계산하는데, 분모가 0이 될 수 있으니 max(orders, 1) 이런 식으로 보호장치를 넣어줍니다.

재구매율은 조금 복잡해요. 먼저 고객별로 주문 횟수를 세고, 2회 이상인 고객의 비율을 구하는 방식이죠. 결과를 dictionary로 정리하면 대시보드에 표시하기 좋아요. 출력 결과를 보면 매출 1,254만 원, 주문 4,500건, AOV 2,787원 이런 식으로 나오네요. 마진율은 15.4%, 재구매율은 24.5%입니다. 이 숫자들이 의미하는 바를 해석하는 게 다음 단계죠.

슬라이드 17: 월별 KPI 트렌드 테이블 (2분)
월별로 KPI가 어떻게 변화하는지 테이블로 정리해 봅시다. groupby로 order_month 기준 집계를 하는데, agg 메서드를 쓰면 여러 지표를 한 번에 계산할 수 있어요. 매출은 sum, 주문수와 고객수는 nunique 이런 식으로요.

결과 테이블을 보면 7월부터 12월까지의 추이가 보입니다. 매출은 4,520만 원에서 7,520만 원으로 꾸준히 증가했네요. 주문수와 고객수도 함께 늘었고요. 흥미로운 건 11월, 12월인데, 매출은 크게 증가했지만 마진율은 오히려 소폭 하락했어요. 시즌 특수로 할인 프로모션을 많이 해서 그런 걸로 보입니다. 이런 패턴을 발견하는 게 바로 데이터 분석의 가치죠.

슬라이드 18: KPI 트렌드 시각화 (2분)
테이블만 보면 지루하니까 시각화로 바꿔볼게요. matplotlib의 subplots로 1행 2열 구조를 만들고, 첫 번째 차트에 매출, 두 번째 차트에 AOV를 그립니다. 이렇게 두 지표를 나란히 놓으면 상관관계를 파악하기 쉬워요.

분석 포인트는 이거예요. 매출과 AOV가 함께 상승하면 이상적인 성장이죠. 고객도 늘고 객단가도 올랐다는 뜻이니까요. 반대로 매출은 늘었는데 AOV가 급락했다면, 저가 할인 위주로 판매량만 늘린 거예요. 수익성 관점에서는 좋지 않을 수 있죠. 계절성도 체크해야 하고요. 연말 선물 시즌에만 AOV가 높다면, 그 외 기간에는 다른 전략이 필요할 거예요.

슬라이드 19: 할인과 매출의 관계 (산점도) (2분)
할인이 정말 매출에 도움이 될까요? 산점도로 확인해 봅시다. x축에 할인율, y축에 주문 매출을 놓고 점을 찍는 거예요. 데이터가 너무 많으면 느려지니까 sample로 일부만 추출하고, alpha로 투명도를 줘서 밀도도 확인할 수 있게 합니다.

그래프를 보면 몇 가지 패턴이 보일 거예요. 할인율이 높다고 매출이 정비례하지는 않아요. 오히려 특정 구간, 예를 들어 20% 할인대에 점이 많이 뭉쳐 있을 수 있죠. 이게 주력 판매 구간이에요. 흩어진 점들은 예외 케이스고요. 극단치도 주목해야 합니다. 할인은 없는데 매출이 엄청 높거나, 과도하게 할인했는데 매출이 낮은 경우, 이런 건 별도로 원인을 파악해야 해요.

슬라이드 20: 가설검정: 할인 효과가 있는가? (2.5분)
이제 통계적으로 검증해 봅시다. 할인을 적용한 주문과 그렇지 않은 주문의 평균 매출에 차이가 있는가? 이게 우리의 귀무가설이에요. 독립표본 t-검정을 사용할 거예요. scipy.stats의 ttest_ind 함수로 두 그룹을 비교하죠.

결과를 보면 할인 적용 그룹의 평균 매출이 1,850원, 일반 그룹은 1,200원이네요. t-통계량은 5.234이고 p-value는 0.0000으로 나왔어요. p-value가 0.05보다 훨씬 작으니까 귀무가설을 기각합니다. 즉, 할인이 적용된 주문의 평균 매출액이 통계적으로 유의하게 높다는 거죠. 할인 프로모션이 효과가 있다는 증거가 되는 거예요.

슬라이드 21: 고객 재구매와 코호트 개념 (2분)
이제 고객 분석으로 넘어갑시다. 코호트 분석이라는 걸 해볼 건데, 이건 동일 시점에 유입된 고객 그룹을 추적하는 기법이에요. 예를 들어 1월에 첫 구매한 고객들을 하나의 코호트로 묶고, 이들이 2월, 3월에도 재구매하는지 추적하는 거죠.

이렇게 하면 시간이 지남에 따라 고객 유지율이 어떻게 변하는지 볼 수 있어요. 신규 고객 확보도 중요하지만, 기존 고객을 유지하는 게 비용 측면에서 훨씬 효율적이거든요. 코호트 분석은 이 유지율을 정량적으로 측정하는 강력한 도구입니다.

슬라이드 22: 코호트 분석 코드 (2분)
실제 코드로 구현해 봅시다. 먼저 각 고객의 첫 구매 월을 찾아서 코호트를 정의해요. 그 다음 각 주문이 첫 구매로부터 몇 개월 후인지 계산하고, 코호트별로 월차별 고객 수를 집계하죠. 이걸 피벗 테이블로 만들면 행은 코호트, 열은 경과 개월 수가 되는 매트릭스가 완성돼요.

여기서 각 코호트의 첫 달 고객 수로 나누면 유지율이 나옵니다. 결과를 히트맵으로 그리면 색깔로 유지율의 높고 낮음을 직관적으로 볼 수 있어요. 진한 색은 유지율이 높은 거고, 옅은 색은 이탈이 많다는 뜻이죠.

슬라이드 23: RFM 세분화 코드 (2분)
다음은 RFM 분석입니다. Recency, Frequency, Monetary, 즉 최근성, 빈도, 금액 이 세 가지 차원으로 고객을 평가하는 거예요. 최근성은 마지막 구매일이 얼마나 최근인지, 빈도는 구매 횟수, 금액은 총 구매액을 의미하죠.

코드를 보면 각 지표를 계산한 뒤 qcut으로 4분위 구간으로 나눕니다. 1부터 4까지 점수를 부여하는데, Recency는 작을수록 좋으니까 역순으로 매기고요. 세 점수를 합산하면 RFM 총점이 나와요. 10점 이상이면 최우수 고객, 7~9점은 우수, 4~6점은 보통, 3점 이하는 주의 대상으로 분류할 수 있습니다.

슬라이드 24: RFM 결과 해석 및 활용 전략 (2분)
RFM 세그먼트별로 어떤 전략을 쓸 수 있을까요? 최우수 고객은 충성도가 높으니 VIP 프로그램이나 얼리버드 혜택을 제공하는 게 좋아요. 우수 고객은 한 단계 더 끌어올리기 위해 크로스셀이나 업셀을 시도하면 되고요.

보통 고객은 재참여를 유도해야 합니다. 이메일 마케팅이나 개인화 추천 같은 거죠. 주의 고객은 이탈 위험군이니까 윈백 캠페인, 즉 특별 할인이나 리워드로 되돌려야 해요. 이렇게 세분화해서 접근하면 마케팅 효율이 크게 올라갑니다.

슬라이드 25: 대시보드 구성안 (1.5분)
이제 지금까지 분석한 내용을 대시보드로 만들어 봅시다. Streamlit을 사용할 건데, 웹 기반이라 공유하기 편하고 인터랙티브한 기능도 쉽게 구현할 수 있어요. 대시보드 구성안을 보면 상단에 KPI 카드가 4개 배치되어 있고, 그 아래 필터 영역과 차트 영역이 있죠.

KPI 카드에는 매출, 주문수, AOV, 재구매율 같은 핵심 지표를 크게 표시하고, 필터로 기간이나 카테고리를 선택하면 아래 차트가 동적으로 업데이트되는 구조예요. 사용자 친화적인 레이아웃이 중요합니다.

슬라이드 26: Streamlit KPI 카드 구현 (2분)
Streamlit 코드는 정말 간단해요. st.metric으로 KPI 카드를 만들 수 있는데, 값과 변화량, 심지어 화살표 방향까지 자동으로 표시해 줍니다. columns로 레이아웃을 나누면 가로로 여러 개를 배치할 수 있고요.

예를 들어 매출 카드를 만든다면, 현재 매출과 전월 대비 증감률을 함께 보여줄 수 있어요. 플러스면 녹색 화살표, 마이너스면 빨간 화살표가 뜨는 거죠. 이런 시각적 피드백이 의사결정에 큰 도움이 됩니다.

슬라이드 27: 필터/차트 구현 (2분)
필터 기능도 추가해 봅시다. st.date_input으로 날짜 범위를 선택하거나, st.multiselect로 카테고리를 복수 선택할 수 있어요. 선택된 값으로 데이터를 필터링하고, 그 결과로 차트를 다시 그리면 인터랙티브한 대시보드가 완성되죠.

차트는 st.line_chart나 st.bar_chart 같은 내장 함수를 써도 되고, plotly나 altair 같은 고급 라이브러리를 연동해도 됩니다. plotly는 줌이나 호버 같은 기능까지 지원해서 사용자 경험이 훨씬 좋아요.

슬라이드 28: 결과 저장 및 리포팅 자동화 (1.5분)
분석 결과는 저장해서 공유해야겠죠. pandas의 to_csv나 to_parquet으로 집계 결과를 파일로 저장할 수 있어요. Parquet 포맷은 압축률이 좋고 속도도 빠릅니다. 리포팅 자동화는 스케줄러를 사용하면 되는데, Linux의 cron이나 Windows의 작업 스케줄러로 매일 특정 시간에 스크립트를 실행시킬 수 있어요.

Python에서는 schedule 라이브러리를 쓰면 더 간단하죠. 매일 아침 9시에 리포트를 생성해서 이메일로 보내는 것도 가능합니다.

슬라이드 29: 전처리 체크리스트 (1.5분)
실전에 들어가기 전에 전처리 체크리스트를 점검해 봅시다. 첫째, 데이터 타입이 올바른가? 날짜는 datetime, 범주는 category인가? 둘째, 결측치는 모두 처리했는가? 셋째, 이상치는 클리핑이나 제거로 정리했는가?

넷째, 중복 레코드는 없는가? drop_duplicates로 제거해야 해요. 다섯째, 파생변수는 모두 생성했는가? 이 체크리스트를 통과해야 신뢰할 수 있는 분석이 가능합니다.

슬라이드 30-33: 실습 과제 (4분)
자, 이제 여러분이 직접 해볼 차례입니다. Lab 1은 데이터 정제 실습이에요. orders.csv를 로드하고 결측치, 이상치를 처리한 뒤 파생변수를 만들어 보세요. Lab 2는 KPI 시각화입니다. 월별 매출과 카테고리별 성과를 차트로 그려보는 거죠.

Lab 3은 가설 검정입니다. 채널별 AOV 차이가 통계적으로 유의한지 t-test로 확인해 보세요. Lab 4는 미니 대시보드 제작입니다. Streamlit으로 KPI 카드 4개와 라인 차트 하나를 구현해 보는 거예요. 각 실습은 10~15분 정도 소요될 겁니다.

슬라이드 34: 제조/교육 데이터 적용 사례 (1.5분)
오늘 배운 기법은 이커머스뿐만 아니라 다른 산업에도 적용 가능해요. 제조업에서는 불량률, 가동률, 재고 회전율 같은 KPI를 추적할 수 있고, 교육 분야에서는 수강 완료율, 학습 시간, 성적 향상도 같은 지표를 분석할 수 있죠.

핵심은 도메인에 맞는 KPI를 정의하고, 데이터 정제 과정을 거쳐, 통계적으로 검증하는 프로세스 자체는 동일하다는 거예요. 오늘 익힌 프레임워크를 여러분의 업무 데이터에 바로 적용해 보시길 바랍니다.

슬라이드 35: 성능 및 스케일링 전략 (1.5분)
데이터가 커지면 성능 이슈가 생깁니다. 몇 가지 최적화 전략을 소개할게요. 첫째, 캐싱입니다. Streamlit의 @st.cache_data 데코레이터를 쓰면 반복 계산을 줄일 수 있어요. 둘째, 다운캐스팅입니다. float64를 float32로 바꾸면 메모리가 절반으로 줄어들죠.

셋째, 청크 단위 처리입니다. 수백만 행 데이터는 한 번에 읽지 말고 chunksize 옵션으로 나눠서 처리하세요. 넷째, 병렬 처리입니다. multiprocessing이나 dask 같은 라이브러리를 쓰면 멀티코어를 활용할 수 있어요.

슬라이드 36: 품질 보증과 로그 (1.5분)
데이터 파이프라인의 품질을 보증하려면 로깅이 필수예요. Python의 logging 모듈로 각 단계마다 로그를 남기면, 문제가 생겼을 때 어디서 실패했는지 추적할 수 있죠. ETL 작업 시작, 종료 시간, 처리 건수, 에러 메시지 이런 것들을 기록해야 해요.

또 데이터 무결성 검증도 중요합니다. assert 문으로 매출이 음수가 아닌지, 날짜 범위가 유효한지 체크하는 거죠. 이런 방어적 프로그래밍이 프로덕션 환경에서는 생명줄입니다.

슬라이드 37: 보안 및 윤리 고려사항 (1.5분)
개인정보가 포함된 데이터를 다룰 때는 보안과 윤리를 꼭 고려해야 해요. 고객 ID나 이메일 같은 민감 정보는 해싱이나 마스킹 처리를 하세요. hashlib으로 SHA256 해시를 만들거나, 이메일 앞자리만 남기고 별표 처리하는 방식이죠.

또 데이터 접근 권한도 관리해야 합니다. 분석용 계정과 운영 계정을 분리하고, 최소 권한 원칙을 적용하세요. GDPR이나 개인정보보호법 같은 법규도 준수해야 하고요. 윤리적으로도 데이터를 왜곡하거나 악용하지 않도록 주의가 필요합니다.

슬라이드 38: 요약 (Takeaways) (2분)
자, 오늘 배운 내용을 정리해 볼게요. 첫째, 데이터 전처리는 분석의 80%입니다. 결측치, 이상치, 타입 변환을 꼼꼼히 해야 해요. 둘째, KPI는 비즈니스 목표와 정렬되어야 합니다. 의미 없는 지표를 추적하는 건 시간 낭비죠.

셋째, 시각화는 인사이트 발견의 지름길이에요. 숫자만 보지 말고 차트로 패턴을 찾으세요. 넷째, 통계 검정으로 직관을 검증하세요. p-value가 당신의 주장을 뒷받침해 줄 겁니다. 다섯째, 대시보드는 소통의 도구입니다. 이해관계자가 쉽게 이해할 수 있게 설계하세요.

슬라이드 39: 다음 단계 (Roadmap) (1.5분)
오늘은 기초를 다졌으니, 다음 단계로 나아갈 수 있어요. 첫째, 예측 모델링입니다. 머신러닝으로 매출을 예측하거나 이탈 고객을 미리 파악할 수 있죠. 둘째, 실시간 분석입니다. Apache Kafka나 Spark Streaming으로 데이터를 실시간 처리하는 거예요.

셋째, 클라우드 배포입니다. AWS나 GCP에 대시보드를 올리면 언제 어디서나 접근 가능하죠. 넷째, A/B 테스트 프레임워크를 구축해 보세요. 가설을 실험하고 데이터로 검증하는 문화가 만들어집니다. 계속 배우고 실험하면서 성장하시길 바랍니다.

슬라이드 40: Q&A (1분)
자, 오늘 강의는 여기까지입니다. 질문 있으신 분 계신가요? 실습 과정에서 막히는 부분이 있거나, 더 깊이 다뤘으면 하는 주제가 있다면 자유롭게 말씀해 주세요. 오늘 배운 내용을 실제 업무에 적용해 보시고, 궁금한 점이 생기면 언제든 연락 주시기 바랍니다.

데이터 분석은 도구가 아니라 사고방식입니다. 숫자 뒤에 숨은 스토리를 찾아내는 탐정이 되어 보세요. 오늘 수고 많으셨습니다!
