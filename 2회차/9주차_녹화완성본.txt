이겸
안녕하세요, 국립창원대학교 PRU 재직자 역량 강화 교육에 오신 것을 환영합니다. 오늘은 AI와 컴퓨터 비전 분야의 최신 기술인 Vision-Language Model을 다뤄보겠습니다.
이미지와 언어를 결합하여 불량 이미지에 대한 설명을 자동으로 생성하는 시스템을 함께 구축해볼 예정인데요. 제조 현장에서 바로 적용 가능한 실무 중심의 내용으로 준비했습니다.

자, 본격적으로 시작하기 전에 Vision-Language 모델의 개념부터 살펴보겠습니다. 멀티모달 학습이 왜 필요한지, 이미지와 텍스트를 결합하는 핵심 아이디어는 무엇인지 함께 알아보죠.
기존 컴퓨터 비전의 한계를 생각해보시면 이해가 쉬울 겁니다. 단순히 "양품", "불량" 이렇게 분류하는 것만으로는 부족하죠.
Vision-Language 모델은 세 가지 혁신적인 가치를 제공합니다. 첫째, 활용성 극대화인데요. "우측 하단 미세 스크래치"처럼 이미지의 상태를 자연어로 풍부하게 표현할 수 있어요.
둘째는 데이터 효율성입니다. 방대한 라벨링 데이터 없이도 제로샷 학습으로 새로운 불량 유형을 즉시 탐지할 수 있죠. 라벨링 비용이 획기적으로 절감되는 겁니다.
셋째, 신뢰성 강화입니다. AI가 왜 이 판단을 내렸는지 텍스트로 설명해주니까 작업자 입장에서 의사결정이 훨씬 수월해집니다. 블랙박스 모델의 불투명성을 해소하는 설명 가능한 AI가 되는 거죠.

멀티모달 학습의 아키텍처는 크게 세 부분으로 구성됩니다. 화면에 보시는 것처럼 이미지 인코더, 텍스트 인코더, 그리고 공통 임베딩 공간이죠.
먼저 이미지 인코더는 입력 이미지를 고차원 벡터로 변환하는데요. 레즈넷 같은 CNN이나 최근에는 Vision Transformer를 주로 사용합니다.
텍스트 인코더는 자연어 설명을 토큰화해서 의미 벡터로 바꿔줍니다. Bert나 GPT 계열의 Transformer 모델이 여기에 활용되고요.
가장 핵심은 세 번째, 공통 임베딩 공간입니다. 이미지 벡터와 텍스트 벡터를 같은 차원의 공간에 투영해서 서로 유사한 쌍은 가깝게, 다른 쌍은 멀게 배치하는 거예요. Clip은 이걸 대조학습 방식으로, Blip은 생성 방식으로 학습합니다.

Clip 모델의 핵심 원리를 자세히 살펴볼까요? Clip은 Contrastive Language, Image Pre-training의 약자입니다.
학습 원리는 이렇습니다. 인터넷에서 수집한 4억 개의 이미지-텍스트 쌍-데이터를 활용해서 공동 임베딩 공간을 학습하는데요. 의미적으로 연관된 쌍들은 가까이 위치시키는 정렬 과정을 거치죠.
손실 함수는 Info-N-CE Loss를 사용합니다. N개의 배치에서 N by N 유사도 행렬을 만들고, 대각 성분인 정답 쌍은 최대화하고 나머지 오답-쌍은 최소화하는 방식이에요.
이렇게 학습된 모델의 활용 효과가 정말 강력한데요. 학습하지 않은 클래스도 텍스트 이름만으로 분류 가능한 제로샷 분류가 가능하고, "긁힌 자국이 있는 표면" 같은 자연어 문장으로 이미지를 검색할 수 있습니다. 도메인 변경이나 노이즈에도 강인한 특징이 있어요.

그렇다면 제조 현장에서는 어떻게 활용할 수 있을까요? 크게 세 가지 관점으로 접근할 수 있습니다.
첫째, 비전 검사 고도화입니다. 결함 유형을 제로샷으로 분류하고 자연어로 검색할 수 있어요. 과거 유사 사례를 텍스트 쿼리로 신속히 조회하는 것도 가능하죠.
둘째는 작업자 의사결정 지원인데요. 단순히 OK, NG 판정을 넘어서 "왜 불량인가"에 대한 구체적 근거를 제시합니다. 비숙련 작업자에게 판단 가이드를 제공해서 검사 신뢰성과 속도를 향상시킬 수 있어요.
셋째, 데이터 적합성도 고려해야 합니다. 제조 데이터와 일반 이미지 데이터의 차이를 인식하고, 공장 내 조명 변화나 노이즈에 강건해야 하죠. 한국어 제조 용어 처리를 위한 파인튜닝도 필요합니다.

자, 이제 주요 V-LM 모델들을 구체적으로 살펴보겠습니다. Clip, Blip, 그리고 Lava를 비교해보고 프로젝트 목적에 맞는 모델 선택 전략을 함께 고민해봅시다.

먼저 Clip부터 볼까요. 화면의 다이어그램처럼 N개의 이미지와 텍스트를 대조 학습하는 구조입니다.
Clip의 가장 큰 강점은 가볍고 빠른 제로샷 능력이에요. 별도의 파인튜닝 없이도 텍스트 프롬프트만으로 이미지를 분류할 수 있죠. 추론 속도가 빨라서 대량의 데이터를 필터링하는 데 적합합니다.
하지만 약점도 명확해요. 이미지와 텍스트 사이의 유사도점수만 계산할 뿐, 구체적인 문장을 생성하지는 못합니다. 생성 능력이 부재한 거죠.
그래서 실무에서는 불량 후보 탐색 및 필터링 용도로 활용합니다. "손상된 부품", "깨끗한 표면" 같은 텍스트와 이미지 간 유사도를 측정해서 1차로 불량 의심 이미지를 걸러내는 스마트 필터 역할을 하는 겁니다.

다음은 생성 능력을 갖춘 Blip 모델입니다.
Blip-v-one은 이미지-텍스트 이해와 생성 능력을 동시에 학습합니다. Vision Transformer와 MED 인코더-디코더 구조를 갖추고 있는데요. 웹에서 수집한 노이즈 데이터를 부트스트래핑하면서 학습해서 기본 캡션 생성과 이미지 검색에 강력한 성능을 보입니다.
Blip-two는 한 단계 더 진화했어요. Q-Former라는 경량 모듈을 통해 시각 정보를 거대 언어 모델이 이해할 수 있는 프롬프트로 변환합니다. Vision Transformer는 프리즈하고, LLM도 프리즈한 상태에서 Q-Former만 학습시키는 효율적인 구조죠.
불량 설명 자동화의 핵심이 바로 여기에 있습니다. Blip-two는 LLM의 추론 능력을 시각 정보와 결합해서 훨씬 상세하고 지능적인 설명을 생성해요. 
단순히, "불량 있음"을 넘어서 "우측 하단에 2밀리미터 길이의 긁힘이 발생함" 같은, 사람이 작성한 듯한 품질 리포트를 자동으로 만들어낼 수 있습니다.

Lava는 대화형 멀티모달 모델입니다. Large Language and Vision Assistant의 약자죠.
대화형 추론 능력이 정말 뛰어난데요. Visual Instruction Tuning을 통해 이미지를 보고 자연어 지시를 수행할 수 있어요. 단순 인식을 넘어 상황 맥락과 인과관계까지 파악하고, 멀티턴 대화로 이전 내용을 기억하면서 연속적인 질의응답이 가능합니다.
산업 현장에서는 복잡한 원인 설명이나 조치 사항 제안에 활용할 수 있어요. "왜 불량인가"에 대한 구체적인 근거를 제시하고, 검사 결과를 사람이 읽기 쉬운 비정형 리포트로 생성하죠.
다만 비용과 리소스 측면에서 고려할 점이 있습니다. 7B, 13B 이상의 대형 LLM 기반이라 고성능 GPU가 필수이고, 추론 속도도 상대적으로 느려요. 실시간 검사보다는 정밀 분석에 적합한 모델입니다.

그렇다면 어떤 모델을 선택해야 할까요? 프로젝트의 목적과 제약 조건에 따라 결정해야 합니다.
먼저 속도와 품질의 트레이드오프를 고려하세요. Clip은 가볍고 빠른 추론이 가능해서 제로샷 분류와 필터링에 적합하고, Blip-2나 Lava는 고품질 설명 생성과 복잡한 추론이 가능하지만 연산 비용이 높습니다.
리소스와 비용도 중요해요. Clip은 4 giga-byte 미만, Blip-two는 약 8 giga-byte, Lava는 12 giga-byte 이상의 GPU 메모리가 필요합니다. 제조 라인의 Takt Time, 즉 사이클 타임을 준수할 수 있는지도 검증해야 하고요.
한글 지원 전략도 빼놓을 수 없습니다. 대부분 모델이 영문 위주로 학습되어서 한글 직접 입력 시 성능이 저하돼요. 번역 파이프라인을 구축하거나 Lora 같은 방법으로 산업 특화 한국어 용어를 파인튜닝해야 합니다.
권장하는 전략은 하이브리드 접근입니다. 1차로 Clip으로 필터링하고, 2차로 Blip으로 상세 분석하는 방식이죠. 이렇게 하면 속도와 품질의 균형이 가장 좋습니다.

이제 실습을 위한 환경 설정을 시작하겠습니다. 필요한 리소스와 패키지를 설치하고, 프로젝트 폴더 구조를 정의해보죠.

원활한 실습을 위해 권장되는 사양을 말씀드릴게요.
하드웨어는 GPU를 권장하는데요. Nvidia V-ram이 8 giga-byte 이상, 가능하면 16 giga-byte가 좋습니다. 
시스템 Ram은 최소 16 기가바이트고, 권장은 32 giga-byte 이상이에요. CPU 전용 모드도 가능하지만 속도 제약이 있습니다.
소프트웨어 스택은 Python은 3.10 이상 버전을 사용하시고요. PyTorch는 2.0 대의 안정 버전을, Cuda는 11.8이나 12.1에 맞춰 설치하셔야 해요. Transformers, Data-sets, Timm, Pillow 같은 핵심 라이브러리도 필요하죠.
개발 툴은 VS Code를 권장하고, Jupyter Lab도 괜찮습니다. Git으로 버전 관리를 하시고, 가상환경은 v-env나 conda로 반드시 분리해서 사용하세요.

화면에 보이는 설치 스크립트를 함께 살펴보겠습니다.
먼저, 가상환경을 생성하고 활성화합니다.
다음으로, PyTorch를 설치하는데 Cuda 12.4 버전 호환으로 설치합니다. GPU 가속이 필수이니 반드시 Cuda 버전을 확인하세요. 
터미널에서 nvidia-smi 명령으로 확인 가능합니다. 
그 후 핵심 라이브러리들을 설치하시고, 모델 최적화를 위해 Accelerate, PEFT, Bits-And-Bytes를 설치합니다. 
다만 윈도우 사용자는 주의하세요. BitsAndBytes가 공식 지원되지 않을 수 있어서 WSL2 환경을 사용하거나 비공식 빌드를 찾아야 할 수도 있습니다.

실습용 프로젝트 구조를 미리 구성해뒀어요. 화면을 보시면서 설명드릴게요.
루트 폴더 아래에 data, defects는 불량 유형별 이미지 데이터가 저장되는 곳입니다. 클래스별 하위 폴더로 구분되어 있고요.
prompts 폴더에는 Clip 분류와 Blip 캡셔닝에 사용할 텍스트 프롬프트 템플릿이 한글, 영문으로 준비되어 있습니다.
notebooks 폴더에는 단계별 실습용 Jupyter Notebook 파일들이 있어요. 코드를 실행하고 시각화 결과를 바로 확인할 수 있죠.
src 폴더는 핵심 Python 모듈들입니다. dataset.py는 PyTorch Data-set 클래스 정의, pipeline.py는 Clip 필터링과 Blip 생성을 연결하는 전체 추론 파이프라인이에요.
outputs 폴더에는 모델이 생성한 캡션 json이나 CSV, 최종 검사 리포트 파일이 저장됩니다.

자, 이제 본격적인 실습에 들어갑니다. 첫 번째 실습은 Clip 모델을 활용한 제로샷 분류와 유사도 검색입니다. Python 코드를 직접 작성하면서 불량 후보를 필터링해보죠.

Clip 모델을 로드하고 전처리하는 코드를 함께 보시죠.
먼저 필요한 라이브러리를 임포트합니다. Torch, PIL, 그리고 Transformers에서 ClipProcessor와 ClipModel을 가져오고요.
다음은 모델 로딩 부분입니다. openai의 Clip-vit-base-patch32라는 모델 ID를 사용하는데요. 이게 가장 널리 쓰이는 베이스라인 모델이에요. ViT-B/32는 이미지를 32 by 32 패치로 나눠서 처리하며 속도와 정확도의 균형이 좋습니다.
ClipModel은 이미지 인코더인 Vision Transformer와 텍스트 인코더 Transformer를 모두 포함하고 있어요. ClipProcessor는 이미지 리사이징, 정규화, 텍스트 토크나이징을 한 번에 처리하는 유틸리티죠.
2번째 코드 부분은 이미지와 텍스트를 준비하는 부분입니다. "스크래치", "찍힘", "양품" 이렇게 세 가지 라벨을 리스트로 만들고요.
그리고 processor 함수에 텍스트와 이미지를 넣고 return-tensors는, "pt"로 PyTorch 텐서 형식으로 받아요. 이렇게 생성된 inputs는 모델에 바로 전달할 수 있습니다.
팁 하나 드리자면, GPU를 사용하려면 model.to("cuda")를 호출하고 inputs의 텐서들도 .to("cuda")로 이동시켜야 GPU 가속을 받을 수 있습니다.

Clip 모델의 인식률을 높이는 핵심은 프롬프트 설계입니다.
구체적 묘사 템플릿이 중요한데요. 단순히 "스크래치"보다는, "스크래치가 있는 금속 표면" 또는 문장형 구조로 작성하는게 훨씬 효과적입니다.
도메인 용어 확장도 필요해요. 제조 현장의 전문 용어를 시각적 속성으로 변환하는 겁니다. "찍힘"을 "dent"로, "이물질"을 "foreign object"로 매핑하고요. "shiny", "metallic", "rusty", "dark spot" 같은 형용사를 결합해서 모델이 이해하기 쉽게 만들어야 해요.
한국어-영어 매핑 전략도 중요합니다. Clip은 영어 데이터 위주로 학습되어서 정확도를 위해 핵심 키워드는 영어 사용을 권장해요. 사용자 입력을 한글로 받아서 번역 API로 영문화한 뒤 모델에 입력하는 파이프라인을 구축하거나, 현장 용어 사전을 미리 정의해두면 좋습니다.

이제 실제 추론 과정을 보시죠.
이-번 라인, torch.no-grad()로 gradient 계산을 비활성화합니다. 학습이 아닌 추론 단계니까 메모리를 절약하는 거죠.
삼-번에서 model(**inputs)를 호출하면 내부적으로 이미지와 텍스트를 각각의 인코더에 통과시켜 임베딩 벡터를 얻습니다. 그다음 이미지 벡터와 텍스트 벡터의 행렬 곱, 즉 Dot Product를 수행해서 유사도 행렬을 생성해요.
칠번의 logits-per-image가 핵심입니다. 이건 이미지 임베딩과 각 텍스트 임베딩 간의 유사도 점수예요. 값이 클수록 해당 텍스트 설명과 이미지가 잘 매칭된다는 뜻이죠.
십번에서 소프트맥스로 확률 변환을 합니다. Logit 값 자체는 범위가 정해져 있지 않거든요. 소프트맥스를 적용하면 모든 라벨 점수의 합이 1이 되도록 정규화되어서 이걸 확률로 해석할 수 있어요.
십칠번부터 이십번은 출력 예시인데요. "스크래치" 98.5%, "찍힘" 1.2%, "양품" 0.3% 이런 식으로 나옵니다.
주의할 점은, Clip은 주어진 보기 중에서 가장 적합한 걸 고르기 때문에 "기타"나 "알 수 없음" 같은 라벨이 없으면 엉뚱한 이미지도 강제로 특정 불량으로 분류될 수 있어요. 임계값 설정이 필수입니다.

Clip 기반 분류의 정확도를 높이는 테크닉을 몇 가지 소개할게요.
프롬프트 앙상블은 다중 템플릿을 활용하는 건데요. "a photo of a...", "a picture of a..." 등 여러 문장 패턴으로 각각 임베딩을 구하고 평균을 내면 노이즈가 감소하고 일관성이 확보됩니다.
이미지 전처리 최적화도 중요해요. 클라헤, 즉 대비 제한 적응형 히스토그램 균일화를 적용하면 미세 결함 특징이 강조되고요. 중앙 크롭으로 불필요한 배경 노이즈를 제거하면 핵심 영역에 집중할 수 있어요.
하드 네거티브 활용도 효과적입니다. 실제 결함과 혼동하기 쉬운 "먼지", "얼룩" 같은 유사 클래스를 라벨에 포함시키면 모델의 구분력이 강화되고 오탐이 감소합니다. 신뢰도 임계값 설정도 잊지 마세요.

두 번째 실습은 Blip를 이용한 이미지 캡셔닝입니다. 불량 이미지에 자연어 설명을 자동으로 생성해보죠.

Blip 모델로 캡션을 생성하는 코드를 보겠습니다.
일-이번 라인은 임포트 부분이고요. 오번부터 칠번에서 Sales-force의 Blip-image-captioning-base 모델을 로드합니다. CoCo 데이터셋으로 학습된 베이스 모델인데 약 2억 2천만 개의 파라미터를 갖고 있어요.
BlipForConditionalGeneration 클래스가 핵심입니다. 이미지를 입력받아 텍스트를 생성하는 디코더 구조를 포함하고 있어요. 이미지 인코더가 시각적 특징을 추출하면 텍스트 디코더가 이를 바탕으로 단어를 순차적으로 예측하는 거죠.
십번, 십일번에서 이미지를 준비하고 프로세서로 전처리합니다.
십사번의 model.generate()가 캡션 생성 부분이에요. 빔 서치 같은 알고리즘을 사용해서 토큰 ID 시퀀스를 생성하고요.
십오번의 processor.decode는 이 숫자 ID들을 사람이 읽을 수 있는 텍스트로 변환합니다.
추가 팁은, inputs에 text는 "a photography of" 같은 시작 문구를 넣어주면 해당 문구에 이어지는 형태로 캡션을 유도할 수 있어요. 조건부 생성이 가능한 겁니다.

불량 설명의 길이, 정확성, 창의성을 결정하는 하이퍼파라미터를 살펴보겠습니다.
길이와 품질 제어는 max-new-tokens로 합니다. 생성할 텍스트의 최대 길이를 제한하는 건데 50에서 100 토큰 정도가 적당해요. 너무 짧으면 설명이 잘리고, 너무 길면 불필요한 내용이 반복됩니다.
num-beams는 빔 서치의 빔 개수예요. 값이 클수록 여러 후보를 탐색해서 품질이 높아지지만 속도는 느려집니다. 보통 3에서 5 정도 사용하죠.
사실성과 창의성의 균형은 temperature로 조절합니다. 확률 분포를 조절하는 건데 0.1에서 1.0 사이 값을 쓰고요. 불량 진단은 낮은 값으로 0.1에서 0.3을 사용해서 환각을 줄이고 사실적 묘사를 유도하는 게 좋아요.
프롬프트 전략도 중요한데요. 목적을 명시해서 결함 클래스를 명시하거나, 구체적 지시로 요구사항을 상세히 전달하면 됩니다.

한글 출력을 위한 번역 파이프라인도 구축해야 합니다. Marian-MT 모델을 사용하는데요. Helsinki-NLP에서 제공하는 영-한 번역 모델입니다.
먼저 Blip으로 영문 캡션을 생성하고요. 그걸 Marian-MT 번역기에 넣어서 한글로 변환합니다. model.generate() 후에 tokenizer.decode()로 텍스트를 얻는 구조죠.
용어 사전 매핑도 중요합니다. "scratch"를 "스크래치"로, "dent"를 "찍힘"으로 치환하는 사전을 미리 정의해두면 번역 품질이 향상돼요.

자, 이제 더 높은 품질의 설명을 만들어보겠습니다. Blip-two를 활용해볼게요.
Blip-two의 핵심 혁신은 바로 'Q-Former'라는 구조에 있습니다. 이미지 인코더는 사진을 벡터로 변환하죠. 
그런데 이 벡터를 GPT 같은 언어 모델이 바로 이해하기는 어렵습니다. 언어와 이미지는 다른 '언어'로 말하는 것과 같거든요.
Q-Former는 바로 이 둘 사이의 '다리' 역할을 합니다. 이미지의 시각적 특징을 언어 모델이 이해할 수 있는 '언어적 특징'으로 변환해주는 거죠.
코드를 보시면 육-칠번 줄에서 Blip-two-ForConditionalGeneration을 로드하고 있죠? 이 안에 Q-Former와 OPT-2.7B라는 큰 언어 모델이 함께 들어있습니다.
Blip-two는 훨씬 강력하지만 모델 크기가 크기 때문에 최적화가 필요합니다.
구번 라인에 load-In-8bit는 True 옵션이 있어요. 8비트 양자화를 활성화해서 메모리 사용량을 절반으로 줄이는 겁니다.
십번의 device_map은 "auto"로, 모델의 레이어를 사용 가능한 GPU와 CPU에 자동으로 분산 배치해서, 대형 모델을 여러 디바이스에 나눠서 로드할 수 있죠.
십사번 줄에서 이미지를 불러오고, 십오번 줄에서 전처리합니다. to("cuda", torch.float16)은 GPU로 데이터를 옮기고 16비트 정밀도로 변환하는 부분이에요.
십팔번 줄이 핵심입니다. model.generate()로 캡션을 생성하죠. max-new-tokens는 50으로, 최대 50개 단어까지 생성하라는 뜻입니다.
Blip-two의 장점은 L-LM의 지식을 활용한다는 점입니다. 단순히 시각 정보만 설명하는 게 아니라 맥락을 파악하고 추론한 결과를 제공하죠. 

세 번째 실습은 불량 이미지 데이터셋을 구축하는 겁니다. 실제 제조 현장의 데이터를 체계적으로 관리하는 방법을 배워보죠.

데이터셋 구축의 첫 단계는 불량 카테고리를 명확히 정의하는 Taxonomy 수립입니다.
네 가지 주요 카테고리가 보이시죠? 스크래치는 표면에 긁힌 자국이 있는 경우고, 길이와 깊이로 특징을 정의합니다.
오염은 이물질이나 얼룩이 부착된 경우예요. 크기와 분포로 특징을 나타내고요.
찍힘은 외부 충격으로 인한 움푹 들어간 자국입니다. 깊이와 면적이 주요 특징이죠.
균열은 재료 내부의 갈라진 틈인데, 길이와 방향성으로 특징을 정의합니다.
각 카테고리마다 명확한 특징을 정의해두면 일관된 라벨링이 가능하고, 모델 학습 시 분류 성능도 향상됩니다. 실제 현장에서는 이보다 훨씬 세분화된 카테고리를 사용하실 수도 있어요.

데이터 주석은 COCO Format을 따르는 게 표준입니다.
왼쪽 아래에 json 구조를 보시면 image id로 이미지를 식별하고, category id로 불량 유형을 지정해요. bbox는 결함 위치를 좌표로 표현한 건데 x, y, width, height 형식입니다.
메타데이터 관리도 중요한데요. timestamp로 촬영 시각을 기록하고, inspector로 검사자를 추적하며, line_id로 생산 라인을 식별합니다. confidence는 라벨링 신뢰도를 0에서 1 사이 값으로 표현하고요. 이렇게 체계적으로 관리하면 나중에 데이터 필터링이나 품질 관리가 훨씬 수월해집니다.

다음은 전처리 및 데이터 증강 파이프라인입니다. 기존 Raw Image에 Resize, Crop, Normalize와 같은 Pre-processing 기법들이 있고, Rotation, Horizontal-Flip으로 좌우 반전, Color-Jitter로 밝기와 대비를 조정해요. 데이터의 다양성을 확보하는 겁니다.
데이터 불균형 해결 전략도 있습니다. 소수 클래스는 오버샘플링으로 증강 비율을 높이고, 다수 클래스는 언더샘플링으로 일부만 사용합니다.
왼쪽 하단의 파이프라인 코드를 보시면 다양한 데이터 전처리와 증강 기법이 적용되어 있어요.
먼저, 256 by 256로 리사이즈하고, RandomCrop을 통해 224 by 224로 이미지를 자릅니다. 그 후 RandomHorizontalFlip으로 좌우 반전하고, 텐서로 변환한 뒤, ImageNet 통계로 정규화합니다. Clip이나 Blip 같은 사전 학습 모델들이 이 크기와 정규화 값을 사용하거든요.

네 번째 실습은 불량 설명 자동 생성 파이프라인을 구축하는 겁니다. Clip과 Blip을 연결해서 엔드-투-엔드 시스템을 만들어보죠.

전체 워크플로우를 단계별로 살펴보겠습니다.
먼저, 이미지 폴더에 watch-dog으로 모니터링하면서 이미지가 입력되면 Clip 필터링을 시작합니다. 입력 이미지들을 Clip 모델에 넣어서 "불량", "양품" 등의 라벨과 유사도를 계산하고, 임계값 이상인 것만 불량 후보로 선별하는 거죠.
다음으로 Blip 캡셔닝입니다. 앞 단에서 필터링된 불량 후보 이미지들을 Blip에 입력해서 상세한 자연어 설명을 생성합니다. "금속 표면 우측에 10cm 길이의 스크래치" 이런 식으로요.
그 다음은 후처리입니다. 생성된 텍스트를 정제하고 용어 사전으로 표준화하며, 필요하면 한글로 번역합니다.
마지막 단계는 리포트 생성입니다. 구조화된 포맷으로 검사 결과를 취합해서 json이나 PDF 형태의 품질 리포트를 자동 생성하는 거예요.
이 파이프라인의 핵심 장점은 Clip의 빠른 필터링과 Blip의 고품질 생성을 결합해서 속도와 품질을 모두 확보한다는 점입니다.

필터링 코드를 자세히 보시죠.
이번부터 사번 라인이 추론 부분입니다. Clip 모델에 이미지와 "defect", "normal" 라벨을 넣어서 유사도를 계산하고 소프트맥스로 확률을 구해요.
구번 라인에서 임계값 필터링을 합니다. 불량 확률이 0.6 이상인 것만 defect_candidates 리스트에 추가하는 거죠. 이 임계값은 오탐률과 재현율의 균형을 고려해서 조정하셔야 해요.
결과적으로 defect_candidates에는 불량 의심 이미지들만 남게 되고, 이걸 다음 단계인 Blip으로 넘기는 겁니다.

자, 이제 Clip으로 걸러낸 불량 이미지를 Blip이 설명하는 단계입니다. 왼쪽 다이어그램을 보시죠.
불량 이미지 하나가 들어오면, 두 가지 경로로 정보를 추출합니다. 파란색 박스가 Blip 모델이고, 보라색이 메타데이터/컴퓨터 비전 분석입니다.
이 두 정보를 결합하는 게 핵심입니다. Blip은 불량이 '무엇인지'를 알려주고, 컴퓨터 비전은 불량이 '어디에 얼마나'를 알려주죠. 실무에서는 OpenCV의 컨투어 분석이나 YOLO 같은 객체 탐지 모델을 병행합니다."
이제 취합된 정보들을 템플릿에 끼워 넣습니다. Blip이 가끔 없는 것을 지어내는 '환각' 현상이 있어요. 따라서 이렇게 템플릿 슬롯을 미리 정해두면 이 위험을 줄일 수 있습니다.

생성된 설명의 품질을 관리하는 프로세스도 중요합니다.
규칙 기반 필터링은 최소길이 체크를 하는 건데요. 다섯 단어 미만의 너무 짧은 설명은 불충분하다고 판단해서 재생성하거나 사람이 검토하도록 넘깁니다. 금지 키워드 제거도 하고요. "maybe", "possibly" 같은 불확실한 표현을 걸러냅니다.
L-LM 기반 표준화는 GPT와 같은 생성형 모델을 활용해서 용어를 통일하는 겁니다. "긁힌 자국", "기스", "흠집" 이런 다양한 표현을 "스크래치"로 통일하는 거죠.
시각화 결합도 효과적이에요. Grad CAM으로 모델이 주목한 영역을 히트맵으로 표시하면 설명의 근거가 시각적으로 드러나죠. 바운딩 박스로 결함 위치를 표시하는 것도 좋고요.
이런 품질관리를 통해 자동 생성된 설명의 신뢰성을 높일 수 있습니다.

오늘 교육 내용을 정리하면서 마무리하겠습니다.
핵심 요약을 다시 한번 짚어볼게요. Vision-Language 모델은 이미지와 텍스트를 결합해서 설명 가능한 AI를 구현하는 혁신적인 기술입니다.
Clip은 제로샷 분류로 빠른 필터링에 활용하고, Blip과 Blip-two는 상세한 자연어 설명 생성에 사용하죠. 하이브리드 접근이 실무에서 가장 효과적입니다.
실무 적용 시에는 데이터 품질 관리와 파이프라인 최적화, 그리고 작업자 UI 설계가 성공의 핵심이에요.
향후 학습 가이드도 말씀드리면, Lora 파인튜닝으로 산업 도메인 특화 모델을 만들어보시고요. Tensor-RT 같은 도구로 추론 속도를 최적화하는 것도 도전해보세요. 
오늘 긴 시간 집중해주셔서 감사합니다.



