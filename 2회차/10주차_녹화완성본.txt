이겸
안녕하세요, 여러분. 오늘 국립창원대학교 PRU 재직자 역량 강화 교육에 참여해주셔서 감사합니다.
오늘 우리가 함께 배울 주제는 '래그 작동원리 및 실습'입니다. 래그는 Retrieval-Augmented Generation의 약자로, 검색 증강 생성 기술을 의미하죠.
이 기술은 최근 가장 주목받고 있는 AI 응용 기술 중 하나입니다. 오늘 강의는 임베딩과 벡터 검색 구조를 이해하고, 사내 문서 검색 자동화 파이프라인을 직접 구축해보는 실습으로 진행됩니다.

먼저 오늘 교육의 목표를 명확히 하고 시작하겠습니다. 우리는 래그 기술의 이론적 이해부터 사내 문서 검색 자동화 시스템 구축 실습까지 전 과정을 다루게 됩니다.
왼쪽에 보시는 핵심 학습 목표를 살펴볼까요? 첫째, 래그 개념을 완벽하게 이해하는 거예요. LLM의 한계를 어떻게 극복하는지 그 원리를 파악하게 됩니다.
둘째, Python 코드 중심 실습이에요. LangChain, ChromaDB 같은 최신 라이브러리를 직접 다뤄보실 겁니다.
셋째는 임베딩과 벡터 검색인데요, 텍스트를 벡터로 변환하고 유사도를 계산하는 원리를 이해하게 되죠.
넷째, 사내 문서 최적화 방법을 배웁니다. PDF나 Word 같은 비정형 데이터를 어떻게 처리하는지 알게 되는 거예요.

전체 커리큘럼을 살펴보겠습니다. 총 6단계로 구성되어 있어요.
첫 번째는 래그 개념 이해입니다. 래그가 무엇이고 왜 필요한지, 전통적 검색과의 차이점을 배우게 됩니다.
두 번째는 임베딩과 벡터 검색이에요. 텍스트 임베딩 모델의 원리를 이해하고 코사인 유사도 기반 검색을 실습합니다.
세 번째 단계는 오늘의 핵심이라고 할 수 있습니다. 사내 문서 검색 자동화를 위한 엔드-투-엔드 파이프라인을 구축해보는 거죠. 문서 파싱부터 청크 분할, 벡터 저장, LLM 연동까지 전 과정을 다룹니다.
네 번째는 실습 프로젝트예요. Python과 LangChain을 활용해서 미니 래그 애플리케이션을 직접 구현해보실 겁니다.
다섯 번째는 평가 및 최적화인데요, Recall이나 MRR 같은 성능 평가 지표를 배우고 실전 최적화 전략을 익히게 됩니다.
마지막으로 핵심 내용을 요약하고 현업 적용을 위한 로드맵 제안으로 마무리하겠습니다.

자, 이제 본격적으로 첫 번째 섹션으로 들어가볼까요. 래그 개념과 필요성에 대해 알아보겠습니다.
왜 단순 L-LM으로는 부족한 걸까요? 래그가 필요한 배경과 핵심 아이디어를 함께 이해해보시죠.

래그란 정확히 무엇일까요? Retrieval-Augmented Generation, 즉 검색을 통해 LLM의 지식을 확장하는 기술입니다.
기술 정의를 보시면, '검색 증강 생성'이라고 되어 있어요. 대규모 언어 모델, 즉 LLM에 외부 지식 베이스를 연결하는 아키텍처를 말하죠.
쉽게 비유하자면 '오픈북 방식'이라고 생각하시면 됩니다. 기억에만 의존하지 않고 참고자료를 보면서 답안을 작성하는 시험과 비슷한 거예요.
핵심 3대 요소를 살펴볼까요? 첫째는 인덱싱입니다. 문서를 쪼개고 벡터로 변환해서 저장하는 과정이죠.
둘째는 검색, 즉 Retrieval이에요. 사용자 질문과 의미적으로 가장 유사한 문맥을 탐색하는 겁니다.
셋째가 생성, Generation인데요, 검색된 문서를 프롬프트에 포함시켜서 LLM이 최종 답변을 만들어내는 단계예요.
주요 기대 효과도 중요합니다. 첫째, 최신성을 확보할 수 있어요. 실시간 사내 문서나 뉴스 데이터를 즉시 답변에 반영할 수 있거든요.
둘째는 환각 감소입니다. AI가 근거 없이 거짓 답변을 하는 걸 줄이고, 사실기반의 응답을 유도할 수 있죠.
셋째, 모델 재학습 없이도 새로운 정보를 즉시 반영할 수 있다는 점이에요. 마지막으로 답변의 출처를 명시할 수 있어서 신뢰도가 높아집니다.

전통적인 키워드 검색의 한계를 먼저 이해해야 합니다. 먼저 키워드 검색 방식부터 살펴보겠습니다.
키워드 검색은 대표적으로 BM25나 TF-IDF 같은 기법을 사용하는데요, 단순히 검색어와 문서 내 단어가 일치하는지만 확인하는 거예요.
예를 들어 "자동차"라고 검색했는데, 문서에 "승용차"라고 쓰여 있으면 못 찾는 거죠.
동의어나 문맥을 이해하지 못하기 때문에 사용자가 정확한 용어를 모르면 정보를 찾기 어렵습니다. "차"라고 검색했을 때 Car를 말하는지 Tea를 말하는지 구분조차 못해요.
반면에, 의미 기반 검색은 텍스트를 고차원 벡터로 변환해서 의미적 유사도를 계산합니다.
단어가 달라도 문맥상 같은 의미라면, "자동차"와 "승용차"와 같이 높은 유사도로 검색되는 거예요. 벡터 공간에서 두 단어가 가까운 위치에 있기 때문이죠.
래그의 핵심 가치가 바로 여기에 있습니다. 의미 기반 검색을 통해 LLM에게 질문과 가장 관련성 높은 참고 자료를 제공함으로써, AI가 팩트에 기반한 정확한 답변을 생성하도록 유도하는 거예요.

이제 래그의 전체 데이터 흐름을 살펴보겠습니다. 크게 두 단계로 나뉘어요.
위쪽의 Knowledge Ingestion, 즉 지식 적재 단계는 오프라인에서 미리 처리됩니다. Documents, 즉 PDF나 Wiki, Docx 같은 문서들을 수집하죠.
그 다음 Chunking 단계에서 텍스트를 토큰 단위로 쪼갭니다. 그리고 Embedding 단계에서 텍스트를 벡터로 변환하고요, 마지막으로 Vector DB에 인덱싱해서 저장하는 겁니다.
아래쪽은 Query & Generation, 즉 질의와 생성 단계예요. 이건 온라인에서 실시간으로 일어나죠.
사용자가 자연어로 질문을 하면, Top-K Search를 통해 유사도를 체크합니다. 가장 관련성 높은 상위 K개의 문서를 찾는 거예요.
그 다음 Prompting 단계에서 질문과 검색된 문맥을 함께, L-LM에게 전달하고요, 최종적으로 LLM Response, 즉 출처가 포함된 답변을 받게 되는 겁니다.
이 흐름을 머릿속에 확실히 그려두시면 이후 실습이 훨씬 수월해지실 거예요.

두 번째 섹션으로 넘어가보겠습니다. 임베딩과 벡터 검색 구조에 대해 깊이 있게 다뤄볼게요.
임베딩 모델과 유사도검색의 핵심을 파악하고, 비정형 데이터를 수치화해서 의미적으로 검색하는 원리를 이해하게 되실 겁니다.

임베딩 설계에서 가장 중요한 결정 요소들을 살펴보겠습니다. 첫째, 모델 선택 전략이에요.
한국어 성능을 고려해야 합니다. MTEB 리더보드 상위권의 bge-M-three나 코-s-roberta 같은 검증된 모델을 사용하는 게 좋죠.
문서 길이에 따라서도 선택이 달라져요. 512토큰까지만 처리하는 모델은 문장 단위에 적합하고, 8192토큰까지 가능한 모델은 긴 문서에 유리합니다.
제조나 기술 용어가 많은 경우라면 도메인 특화 사전학습 모델을 고려해보세요.
둘째는 차원 설계입니다. 성능과 효율성 사이의 균형이 중요해요. 예를 들어, 1024 이상의 고차원은 정밀하지만 느리고, 384와 같은 저차원은 빠르지만 정보 손실이 있을 수 있죠.
일반적인 래그 구축에서는 768에서 1024 차원이 균형 잡힌 선택입니다. 최신 기법인 Matryoshka Embedding은 필요에 따라 앞부분 차원만 잘라 써도 성능이 유지되는 장점이 있어요.
셋째, 거리 척도와 정규화입니다. 코사인 Similarity는 벡터 간의 각도, 즉 방향 유사도를 측정하는데 텍스트 검색에 가장 널리 사용돼요. 
L, two Normalization을 적용하면 모든 벡터의 길이를 1로 만들어서, Dot-product 계산만으로도 코사인 유사도를 대체할 수 있습니다. Euclidean Distance는 벡터 간 직선 거리를 측정하는데, 정규화되지 않은 데이터에서는 크기 차이에 민감하죠.

자, 이제 실제 코드로 들어가볼까요? Sentence-Transformers 라이브러리를 활용해서 자연어 문장을 고차원 벡터로 변환하는 과정을 실습합니다.
먼저 라이브러리를 임포트하고요, 한국어 임베딩 모델을 로드합니다. 여기서는 코-Sim-CSE-roberta-multitask 모델을 사용해요.
SentenceTransformer는 Bert 기반 모델을 쉽게 사용할 수 있게 해주는 Wrapper예요. 이 모델은 한국어 문장 유사도성능이 우수하죠.
두 번째 단계에서는 변환할 텍스트 데이터를 리스트 형태로 준비합니다. "래그는 검색 증강 생성 기술입니다", "임베딩은 텍스트를 벡터로 변환합니다" 같은 문장들이죠.
세 번째 단계가 핵심입니다. model.encode 메서드를 호출하는데요, normalize_embeddings는  True 옵션을 주목하세요. 이걸 켜면 생성된 벡터의 길이를 1로 만들어줍니다.
이렇게 하면 나중에 벡터 간 유사도를 계산할 때 복잡한 공식 없이 단순 Dot-product 만으로 코사인 유사도를 구할 수 있어요. convert-to-tensor-True는 결과를 PyTorch 텐서로 받겠다는 의미죠.
결과값을 출력하면 Shape가 [3 콤마 768]로 나옵니다. 3개의 문장이 각각 768차원의 실수 벡터로 변환되었다는 뜻이에요.
팁 하나 드리자면, 대량의 문서를 처리할 때는 device 'cuda' 옵션을 추가해서 GPU를 사용하면 처리 속도가 수십 배 빨라집니다.

다음으로 벡터 데이터베이스의 구조 설계를 이해해봅시다. 세 가지 핵심 개념이 있어요.
첫째, 컬렉션입니다. 관계형 데이터베이스의 테이블과 비슷한 개념이죠. 데이터의 성격, 예를 들어 사내 규정, 기술 문서, 프로젝트 이력에 따라 논리적으로 분리해서 관리해요. 인덱스 설정의 기본 단위가 됩니다.
둘째, 벡터와 페이로드예요. 각 문서는 고유한 ID를 가지고 있고요, Vector 필드에는 고차원 숫자 배열인 임베딩 벡터가 저장됩니다. 예를들면, [0.12, -0.54, 0.88...] 이런 식입니다.
그리고 Document, 즉 페이로드 필드에는 원본 텍스트 내용이 들어갑니다. "2024년도 인사 평가 가이드라인에 따르면..." 이런 실제 내용입니다. 검색할 때는 벡터로 유사도를 계산하고, 결과로는 텍스트를 반환하는 거예요.
셋째, 메타데이터 활용입니다. json 형태로 부서, 작성일, 보안 등급 같은 정보를 저장할 수 있어요. 이걸 활용하면 "HR 부서 문서 중에서 휴가 관련 내용 검색" 같은 하이브리드 검색이 가능해지죠.
마지막으로 팁 하나 드리겠습니다. 메타데이터 필터링, 즉 Pre-filtering을 하면 불필요한 벡터 연산을 줄여서 검색 속도와 정확도를 획기적으로 높일 수 있습니다.

벡터 공간에서 질문과 문서 사이의 거리를 계산해서 가장 가까운 문서를 찾아내는 과정을 실습해볼게요.
먼저 검색 질의, 즉 Query를 임베딩으로 변환합니다. "래그의 핵심 원리가 무엇인가요?"라는 질문을 벡터로 만드는 거죠.
두 번째 단계에서 코사인 유사도를 계산해요. util-cos-sim 함수는 두 벡터 사이의 각도 코사인 값을 계산합니다. 값이 1에 가까울수록 두 벡터가 같은 방향, 즉 높은 유사도를 가리키는 거예요. 0은 직교, 즉 관계없음을 의미하고, -1은 반대 방향이죠.
여기서 중요한 건 행렬 연산의 효율성입니다. 반복문을 사용하지 않고 행렬 곱셈을 통해 수천, 수만 개의 문서와의 유사도를 한 번에 계산해요. 이게 GPU 병렬 처리에 최적화된 방식이죠.
세 번째 단계는 상위 K개 결과 추출입니다. torch.top-k를 사용해서 계산된 유사도점수 중 가장 높은 K개의 값과 해당 문서의 인덱스를 추출하는 거예요. 이게 바로 검색 엔진의 핵심인 '랭킹' 과정입니다.
결과를 출력하면 "Doc 0 (Score: 0.8512): 래그는 검색 증강 생성 기술입니다" 이런 식으로 나오죠. 점수가 높을수록 질문과 관련성이 높다는 의미예요.
추가 팁인데요, 임베딩 벡터가 이미 정규화되어 있다면 단순 Dot-product의 결과는 코사인 유사도와 동일합니다. 정규화된 Dot-product는 계산 비용이 더 저렴하죠.

LangChain 라이브러리와 Faiss를 활용한 전체 과정을 실습해보겠습니다. 텍스트 데이터를 벡터화해서 저장하고, 질문과 가장 유사한 문서를 검색하는 거예요.
먼저 문서와 임베딩 모델을 준비합니다. 간단한 텍스트 리스트와 OpenAI 임베딩 모델을 사용하죠.
두 번째 단계가 벡터 인덱스 생성입니다. Faiss-.from_texts() 메서드는 텍스트 리스트를 받아서 지정된 임베딩 모델로 벡터화한 후, 효율적으로 검색할 수 있는 인덱스 구조로 변환해요. 이건 메모리 상에 존재하죠.
db.save_local()을 통해 생성된 인덱스를 로컬 디스크에 파일 형태로 저장할 수 있습니다. 서버 재시작 후에도 데이터를 유지할 수 있고, load_local()로 다시 불러와서 사용하면 되는 거예요.
세 번째 단계에서 인덱스를 로드하고 검색을 실행합니다. similarity_search() 메서드가 핵심인데요, 입력된 쿼리를 벡터로 변환하고, 저장된 벡터들과의 거리를 계산해서 가장 가까운 k개의 문서를 반환해요.
"래그가 무엇인가요?"라고 물으면 "래그는 검색 증강 생성입니다"라는 가장 유사한 문서를 찾아주는 거죠.
Faiss는 대규모 데이터의 고속 검색에 최적화된 인메모리 라이브러리고, Chroma는 메타데이터 필터링 같은 데이터베이스 기능이 강화된 솔루션입니다.

검색 정확도와 재현율을 극대화하기 위한 핵심 튜닝 포인트를 알아보겠습니다. 크게 세 가지 영역이에요.
첫째, 청크 전략입니다. 최적의 크기를 설정하는 게 중요해요. 너무 작으면 문맥이 손실되고, 너무 크면 노이즈가 증가합니다. 보통 300에서 500 토큰이 권장되죠.
의미 단위로 분할하는 것도 중요한데요, RecursiveCharacterTextSplitter 같은 도구를 활용해서 문단이나 섹션 단위로 절단하면 좋아요. 그리고 오버랩, 즉 중복 구간을 10에서 20% 정도 설정해야 문장 간 연결성이 유지됩니다.
둘째, 재랭킹 기법이에요. Cross-Encoder를 도입하면 벡터 검색 결과의 정확도를 높일 수 있습니다. 1차로 벡터를 검색해서 상위 50개를 뽑고, 재랭킹해서 최종 3에서 5개를 선정하는 식이죠.
하이브리드 검색도 효과적입니다. BM25 같은 키워드 검색과 벡터 검색의 의미 기반 결과를 결합하면 상호 보완할 수 있어요.
셋째, 메타데이터 필터링입니다. Pre-filtering을 적용해서 검색 전에 부서, 날짜, 문서 유형으로 범위를 좁히면 연산량이 감소하죠. 접근 권한 제어에도 활용할 수 있어요. 사용자 권한에 맞는 문서만 검색되도록 메타데이터를 설정하는 겁니다.

세 번째 섹션으로 넘어가겠습니다. 사내 문서 검색 자동화 구축이에요.
엔드-투-엔드 파이프라인 구현, 즉 문서 수집부터 임베딩, 검색, LLM 응답 생성까지의 전 과정을 실습하게 됩니다.

사내 문서 수집부터 검색 서비스 제공까지의 엔드-투-엔드 파이프라인을 살펴보겠습니다. 크게 두 부분으로 나뉘어요.
위쪽은 Data Ingestion, 즉 데이터 적재 단계입니다. Source Connector에서 나스, Confluence, Slack 같은 다양한 소스에서 데이터를 수집하죠.
Parser 단계에서는 PDF, Docx, HWP 같은 파일을 읽고 필요하면 OCR도 적용합니다. Chunker에서 텍스트를 분할하고, Embedding 단계에서 밀집 벡터로 변환하는 거예요.
아래쪽은 Service & Consumption, 즉 서비스 활용 단계입니다. Vector DB에 Chroma나 faiss를 사용해서 데이터를 저장하고요, Search API는 FastAPI나 LangChain으로 구현해요.
최종적으로 UI나 Bot, 예를 들어 Slack Bot이나 Web 인터페이스를 통해 사용자에게 서비스를 제공하는 거죠.
이 구조를 이해하면 실제 프로덕션 환경에서도 바로 적용할 수 있습니다.

이제 다양한 포맷의 비정형 데이터를 래그 파이프라인에서 처리 가능한 표준화된, Document 객체로 변환하는 과정을 살펴보시죠.
먼저 LangChain의 document-loaders를 임포트합니다. 파이-PDF-로더, Docx-to-txt-Loader, DirectoryLoader 같은 전용 로더들이죠.
단일 파일 로더를 보시면, PyPDFLoader는 페이지 단위로 PDF를 읽어옵니다. Docx-to-txt-Loader는 워드 문서의 텍스트와 일부 구조를 추출하고요. LangChain은 CSV, HTML, Markdown 등 거의 모든 포맷의 로더를 제공해요.
두 번째는 폴더 내 모든 파일을 재귀적으로 로딩하는 방법입니다. DirectoryLoader를 사용하는 거죠. 수백 개의 문서를 일일이 지정할 수 없으니까요.
glob 패턴을 통해 하위 폴더까지 재귀적으로 탐색해서 특정 확장자 파일만 일괄 로딩할 수 있습니다. show_progress=True 옵션을 켜면 진행 상황도 볼 수 있어요.
로딩 결과를 확인하면 page_content에는 본문이, metadata에는 출처나 페이지 번호 같은 정보가 담겨 있습니다. 이 메타데이터는 나중에 답변의 근거를 사용자에게 제시할 때 필수적이에요.
고급 팁인데요, 표나 이미지가 포함된 복잡한 문서는 UnstructuredLoader를 사용하거나 OCR 기능이 포함된 파서를 연동해야 데이터 유실을 최소화할 수 있습니다.

문맥을 유지하면서 검색 정확도를 높이기 위한 RecursiveCharacterTextSplitter 설정을 알아보겠습니다.
먼저 분할기를 초기화하는데요, 한국어 문서 구조를 고려해야 해요. chunk_size 500은 검색에 적절한 토큰 길이입니다. 너무 짧으면 문맥이 손실되죠.
chunk_overlap 50은 문장 간 연결성을 위한 중복 구간이에요. 이전 청크의 마지막 부분을 현재 청크의 앞부분에 포함시켜서 검색 시 문맥이 잘리는 걸 방지하는 겁니다.
separators 파라미터를 주목하세요. "더블 슬래쉬-N", "슬래쉬-N", "공백", "빈칸" 순서로 설정되어 있죠. 이게 Recursive 분할의 핵심입니다. 단락, 줄, 단어 순으로 분할을 시도하기 때문에 의미적 완결성을 더 잘 유지해요.
두 번째 단계에서 실제 문서 파일을 로드하고 분할합니다. text_splitter.create_documents() 메서드를 호출하면 Raw-text가 여러 개의 청크로 나뉘어지죠.
결과를 확인하면 총 생성된 청크 수와 각 청크의 내용을 볼 수 있어요. Chunk Size 설정 팁을 드리자면, 임베딩 모델의 최대 입력 토큰 수를 넘지 않아야 합니다. 보통 300에서 500자 정도가 단일 질문에 대한 답변 근거로 적절해요.

사내 문서를 벡터화하는 핵심 단계입니다. 고성능 다국어 모델인 BGE-M, Three를 활용해볼게요.
먼저 라이브러리를 임포트하고 모델을 로드합니다. BA-AI의 bge-m-three 모델은 최신 래그 구축에 널리 쓰이는 Sota급 모델이에요. 한국어를 포함한 다국어를 지원하고, 최대 8192 토큰까지 처리 가능해서 긴 문서 이해력이 뛰어나죠.
디바이스-Cuda 설정을 보세요. GPU를 사용할 수 있으면 자동으로 Cuda를 활용합니다. 문서 양이 많을 경우 CPU만으로는 처리가 매우 느려요. GPU V-ram을 활용하고 batch_size를 조절해서 메모리 효율성을 극대화해야 하죠.
두 번째 단계에서 문서 청크 데이터를 준비합니다. "출장비 규정: KTX 특실은 임원급 이상만 가능합니다", "복지 포인트: 매년 1월 1일 100만원이 지급됩니다" 같은 실제 사내 문서 내용들이에요.
세 번째 단계가 임베딩 벡터 생성입니다. model.encode() 메서드를 호출하는데요, batch_size는 32로 설정해서 GPU 배치 처리를 최적화해요. normalize_embeddings=True는 앞서 배운 대로 코사인 유사도 계산을 간단하게 만들어주죠.
show-progress-bar=True 옵션을 켜면 진행 상황을 볼 수 있어요. bge-Mthree 모델은 1024차원의 밀집 벡터를 생성합니다. 이후 벡터 데이터베이스 컬렉션을 생성할 때, 차원 수를 정확히 맞춰줘야 해요.
실무 팁인데요, 문서가 수정되지 않았다면 매번 임베딩을 다시 생성할 필요가 없습니다.
파일의 해시값을 키로 사용해서 로컬 디스크에 임베딩 결과를 캐싱해두면, 재실행 속도가 획기적으로 빨라져요.

생성된 임베딩 벡터와 원본 텍스트를 검색 가능한 형태의 데이터베이스로 저장하고, 디스크에 영구 보관하는 방법을 알아보겠습니다.
먼저 ChromaDB에 저장하는 방법이에요. Chroma.from_documents() 메서드를 사용하는데요, persist_directory를 지정하면 메모리가 아닌 로컬 디스크에 데이터가 저장됩니다.
SQLite 기반으로 작동하고요, 서버를 재시작해도 인덱스가 유지되기 때문에 사내 지식 베이스 구축에 필수적이죠. collection_name으로 데이터셋을 구분할 수 있어요. 예를 들어 "company_docs", "tech_manuals" 이런 식으로요.
두 번째는 Faiss 인덱스 생성 및 저장입니다. faiss는 기본적으로 인메모리 방식인데, save_local() 메서드를 통해 인덱스 파일과 메타데이터 파일로 직렬화해서 저장할 수 있어요.
검색 속도가 매우 빠르지만, 메타데이터 관리는 Chroma보다 다소 복잡할 수 있습니다. load_local()로 저장된 인덱스를 다시 불러와서 사용하면 되죠.
메타데이터 활용 전략도 중요합니다. 부서, 작성 날짜, 문서 유형, 보안 등급 같은 정보를 메타데이터로 저장하면 필터링 검색이 가능해요. "2024년 이후 작성된 HR 부서 문서만 검색" 이런 식으로 세밀하게 제어할 수 있죠.

사용자 질문을 처리하는 검색 로직을 구현해봅시다. "2025년 연차 휴가 이월 규정은?" 이라는 질문을 예시로 보겠습니다.
질문이 입력되면, 먼저 질문을 임베딩으로 변환합니다. embed_query 메서드가 질문 텍스트를 벡터로 바꿔줘요.
다음으로 메타데이터 필터를 정의합니다. 예를 들어 "인사팀 문서만 검색"하고 싶을 때, Department는 "HR"로 추가하면 돼요. 보안등급 필터링도 같은 방식으로 구현할 수 있습니다.
이제 쿼리 벡터로 similarity_search_by_vector를 호출하면 유사한 문서들과 점수가 반환됩니다.
k = 3은 상위 3개 문서를 가져오라는 뜻이죠. filter 파라미터를 보세요, 앞에서 정의한 메타데이터 조건을 적용합니다.
마지막으로 검색 결과를 포맷팅하는 부분을 보면 문서 내용, 출처를 함께 출력하고 있어요.

다음은 검색 정확도를 한 단계 더 높이는 재랭킹 기법입니다. Cross-Encoder 모델을 사용할 거예요.
코드를 보면, 먼저 CrossEncoder 모델로, BA-AI의 bge-re-ranker-large를 로드합니다.
다음으로 1차 검색 결과 준비입니다. "재택근무 신청 절차는 어떻게 되나요?"와 같은 쿼리에 대해 추출할 문서들, 규정, 가이드라인, 운영지침이나 양식 등을 준비합니다.
이제 predict 메서드에 쿼리-문서 쌍 리스트를 전달하면 재랭킹 점수가 나옵니다. 그 점수로 다시 정렬해서 득점 임계값을 설정하면 신뢰할 수 있는 정보를 가진 문서 추출이 가능합니다.

검색된 문서를 LLM과 연결하는 실습입니다. LangChain으로 Ollama를 사용할 거예요.
먼저 ChatOllama 인스턴스를 생성합니다. model은 로컬에 설치된 lama-three-ko 모델을 지정하면 돼요. 추가적으로 temperature 파라미터를 사실기반 응답을 위해 0.1로 낮게 설정합니다.
다음으로 프롬프트 템플릿을 정의하겠습니다. "아래의 Context를 바탕으로 질문에 답변해주세요."라는 시스템 지시어로 시작하죠. context 변수에 검색된 문서들이 들어가고, question에 사용자 질문이 들어갑니다.
체인을 구성하는 부분을 보면 프롬프트와 LLM, 그리고 출력 파서를 파이프로 연결했습니다. 이제 Invoke 메서드로 질문을 입력하면 전체 래그 파이프라인이 실행되면서 response에 답변이 저장됩니다.

최종 답변을 생성하고 출처를 인용하는 로직입니다. format_docs_with_source 함수를 보세요.
각 문서의 내용과 메타데이터에서 파일명, 페이지를 추출해서 "[출처: xxx.pdf, p.5]" 형식으로 만듭니다. 이렇게 포맷된 컨텍스트를 LLM에게 전달하면 답변에 출처가 자연스럽게 포함돼요.
하이라이트 기능도 구현할 수 있습니다. 답변에서 문서 원문을 인용한 부분을 찾아 HTML로 강조 표시하는 거죠.
토큰 길이 제어도 중요해요. 컨텍스트가 너무 길면 LLM의 입력 한계를 초과할 수 있으니, tiktoken 라이브러리로 토큰을 세고 필요하면 잘라내야 합니다.
보통 최대 컨텍스트 길이를 3000토큰 정도로 제한하고, 그 안에서 가장 관련성 높은 문서들만 포함시키죠. 답변 품질을 유지하면서도 비용을 절감하는 방법입니다.

래그 시스템의 성능을 어떻게 측정할까요? 두 가지 측면으로 나눠서 평가합니다.
첫째, 검색 정확도예요. Recall-K는 정답 문서가 상위 K개 안에 포함되었는지 봅니다. MRR은 정답 문서가 몇 번째에 등장했는지 순위를 평가하죠.
둘째, 답변 품질입니다. 정확성은 답변이 사실적으로 맞는지, 근거성은 검색된 문서에 기반했는지, 유창성은 문장이 자연스러운지를 평가해요.

실제 운영 환경에서 필요한 최적화 전략들을 정리해드릴게요. 첫째, 증분 인덱싱 자동화입니다.
매일 밤 신규 문서만 크롤링해서 벡터 DB에 추가하는 스케줄러를 구성하세요. Apache Airflow나 Cron으로 구현할 수 있어요.
둘째, 접근제어 시스템을 통합해야 합니다. 사용자의 부서와 권한에 따라 검색 결과를 필터링하죠. 메타데이터에 security_level을 저장하고 쿼리 시 체크하면 됩니다.
셋째, 모니터링이 중요해요. 검색 지연 시간, 오류율, 임베딩 드리프트를 대시보드로 추적하세요. 
추가적으로 비용 관리인데, 임베딩 모델을 로컬에서 돌리면 API 비용을 아낄 수 있습니다. 클라우드 LLM 사용량도 캐싱으로 줄일 수 있죠.

여기까지가 실습 파트였고요. 이제 오늘 배운 내용을 정리하고 마무리하겠습니다.

오늘 교육의 핵심 포인트를 다시 짚어보겠습니다. 래그 아키텍처는 인덱싱, 검색, 생성의 3단계 구조예요.
임베딩은 텍스트를 벡터로 변환하고, 벡터 DB는 의미 기반 검색을 가능하게 하며, LLM은 검색된 문서를 바탕으로 답변을 생성하죠. 실습 포인트는 청크 크기 조정, 재랭킹 적용, 메타데이터 필터링이었습니다.
운영 팁으로는 증분 인덱싱 자동화, 접근제어 통합, 모니터링 대시보드 구축을 강조했어요. 가장 중요한 건 사용자 피드백을 지속적으로 수집해서 시스템을 개선하는 거예요.
래그는 완성형이 아니라 계속 진화하는 시스템입니다. 여러분의 도메인 데이터로 파인튜닝하고, A/B 테스트로 검증하면서 발전시켜 나가세요.

추가 학습을 위한 리소스들을 안내드릴게요. 핵심 라이브러리 문서로는 LangChain 공식 문서, Faiss GitHub, Chroma 튜토리얼을 참고하세요. HuggingFace Transformers 문서에는 다양한 한국어 임베딩 모델이 소개되어 있습니다.

오늘 교육에 참여해주셔서 감사합니다. 배우신 내용을 실무에 바로 적용해보시길 바랍니다!

